{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reader():\n",
    "    def __init__(self,dataset_path= \"Data1.xlsx\"):\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.features = 0\n",
    "        self.taret = 0\n",
    "        \n",
    "    def load_data(self,dataset_path):\n",
    "        data = pd.read_excel(dataset_path, sheetname=None)\n",
    "        print(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read = reader()\n",
    "#print(read.load_data(\"Data1.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cancer_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed Sample ID</th>\n",
       "      <th>Cancer type</th>\n",
       "      <th>Clinical diagnosis</th>\n",
       "      <th>Inflammatory</th>\n",
       "      <th>Risk Factor: None</th>\n",
       "      <th>Risk Factor: Smoking History</th>\n",
       "      <th>Risk Factor: Family History</th>\n",
       "      <th>Risk Factor: Medical Condition</th>\n",
       "      <th>Risk Factor: Other</th>\n",
       "      <th>Risk Factor: Unknown</th>\n",
       "      <th>...</th>\n",
       "      <th>GNAS.2</th>\n",
       "      <th>U2AF1.1</th>\n",
       "      <th>KDM6A.2</th>\n",
       "      <th>ARID1A.1</th>\n",
       "      <th>GATA3.4</th>\n",
       "      <th>SUM</th>\n",
       "      <th>TP53.82</th>\n",
       "      <th>APC.20</th>\n",
       "      <th>CTNNB1.31</th>\n",
       "      <th>Sum tertiary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.9</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Processed Sample ID Cancer type  Clinical diagnosis  Inflammatory  \\\n",
       "0                  2.9       Daisy                   1             0   \n",
       "1                  2.1       Daisy                   1             0   \n",
       "2                  5.9       Daisy                   1             0   \n",
       "3                  5.1       Daisy                   1             0   \n",
       "4                  5.1       Daisy                   1             0   \n",
       "\n",
       "   Risk Factor: None  Risk Factor: Smoking History  \\\n",
       "0                  0                             1   \n",
       "1                  0                             0   \n",
       "2                  0                             1   \n",
       "3                  0                             0   \n",
       "4                  0                             0   \n",
       "\n",
       "   Risk Factor: Family History  Risk Factor: Medical Condition  \\\n",
       "0                            1                               1   \n",
       "1                            1                               0   \n",
       "2                            0                               1   \n",
       "3                            1                               1   \n",
       "4                            1                               1   \n",
       "\n",
       "   Risk Factor: Other  Risk Factor: Unknown      ...       GNAS.2  U2AF1.1  \\\n",
       "0                   0                     0      ...            0        0   \n",
       "1                   0                     0      ...            1        0   \n",
       "2                   0                     0      ...            0        1   \n",
       "3                   0                     0      ...            0        0   \n",
       "4                   1                     0      ...            0        0   \n",
       "\n",
       "   KDM6A.2  ARID1A.1  GATA3.4  SUM  TP53.82  APC.20  CTNNB1.31  Sum tertiary  \n",
       "0        1         0        0    1       10       7          3            20  \n",
       "1        0         0        0    3        6       4          1            11  \n",
       "2        3         0        0    3       14       9          5            28  \n",
       "3        0         0        0    2        9       3          6            18  \n",
       "4        0         0        0    1       10       3          0            13  \n",
       "\n",
       "[5 rows x 297 columns]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Processed Sample ID'] = data['Processed Sample ID'].str.replace('-', \".\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed Sample ID</th>\n",
       "      <th>Clinical diagnosis</th>\n",
       "      <th>Inflammatory</th>\n",
       "      <th>Risk Factor: None</th>\n",
       "      <th>Risk Factor: Smoking History</th>\n",
       "      <th>Risk Factor: Family History</th>\n",
       "      <th>Risk Factor: Medical Condition</th>\n",
       "      <th>Risk Factor: Other</th>\n",
       "      <th>Risk Factor: Unknown</th>\n",
       "      <th>Comorbidities: None</th>\n",
       "      <th>...</th>\n",
       "      <th>APC.20</th>\n",
       "      <th>CTNNB1.31</th>\n",
       "      <th>Sum tertiary</th>\n",
       "      <th>Gender_M</th>\n",
       "      <th>Ancestry_BLACK OR AFRICAN AMERICAN</th>\n",
       "      <th>Ancestry_OTHER</th>\n",
       "      <th>Ancestry_WHITE</th>\n",
       "      <th>Batch number_Batch4</th>\n",
       "      <th>Batch number_Batch5</th>\n",
       "      <th>Batch number_Batch6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Processed Sample ID  Clinical diagnosis  Inflammatory  Risk Factor: None  \\\n",
       "0                  2.9                   1             0                  0   \n",
       "\n",
       "   Risk Factor: Smoking History  Risk Factor: Family History  \\\n",
       "0                             1                            1   \n",
       "\n",
       "   Risk Factor: Medical Condition  Risk Factor: Other  Risk Factor: Unknown  \\\n",
       "0                               1                   0                     0   \n",
       "\n",
       "   Comorbidities: None         ...           APC.20  CTNNB1.31  Sum tertiary  \\\n",
       "0                    0         ...                7          3            20   \n",
       "\n",
       "   Gender_M  Ancestry_BLACK OR AFRICAN AMERICAN  Ancestry_OTHER  \\\n",
       "0         0                                   0               0   \n",
       "\n",
       "   Ancestry_WHITE  Batch number_Batch4  Batch number_Batch5  \\\n",
       "0               1                    0                    1   \n",
       "\n",
       "   Batch number_Batch6  \n",
       "0                    0  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummy = data\n",
    "\n",
    "def dummify_dataset(df, column):       \n",
    "    df = pd.concat([df, pd.get_dummies(df[column], prefix=column, drop_first=True)],axis=1)\n",
    "    df = df.drop([column], axis=1)\n",
    "    return df\n",
    "\n",
    "columns_to_dummify = ['Gender', 'Ancestry', 'Batch number', 'Cancer type']\n",
    "for column in columns_to_dummify:\n",
    "    data_dummy = dummify_dataset(data_dummy, column)\n",
    "    \n",
    "data_dummy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, column = data.shape\n",
    "y = data_dummy['Clinical diagnosis']\n",
    "X = data_dummy.drop(['Clinical diagnosis'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(600,100,2), max_iter=5000, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001, batch_size=50, learning_rate_init=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.80206451\n",
      "Iteration 2, loss = 0.80169164\n",
      "Iteration 3, loss = 0.80113393\n",
      "Iteration 4, loss = 0.80047947\n",
      "Iteration 5, loss = 0.79974689\n",
      "Iteration 6, loss = 0.79899247\n",
      "Iteration 7, loss = 0.79819640\n",
      "Iteration 8, loss = 0.79740807\n",
      "Iteration 9, loss = 0.79658634\n",
      "Iteration 10, loss = 0.79575551\n",
      "Iteration 11, loss = 0.79493358\n",
      "Iteration 12, loss = 0.79412303\n",
      "Iteration 13, loss = 0.79329533\n",
      "Iteration 14, loss = 0.79248002\n",
      "Iteration 15, loss = 0.79166951\n",
      "Iteration 16, loss = 0.79084520\n",
      "Iteration 17, loss = 0.79001883\n",
      "Iteration 18, loss = 0.78921453\n",
      "Iteration 19, loss = 0.78840704\n",
      "Iteration 20, loss = 0.78758644\n",
      "Iteration 21, loss = 0.78676133\n",
      "Iteration 22, loss = 0.78594609\n",
      "Iteration 23, loss = 0.78515760\n",
      "Iteration 24, loss = 0.78436820\n",
      "Iteration 25, loss = 0.78355714\n",
      "Iteration 26, loss = 0.78272496\n",
      "Iteration 27, loss = 0.78193740\n",
      "Iteration 28, loss = 0.78113357\n",
      "Iteration 29, loss = 0.78033757\n",
      "Iteration 30, loss = 0.77955632\n",
      "Iteration 31, loss = 0.77874703\n",
      "Iteration 32, loss = 0.77797280\n",
      "Iteration 33, loss = 0.77720747\n",
      "Iteration 34, loss = 0.77638952\n",
      "Iteration 35, loss = 0.77563844\n",
      "Iteration 36, loss = 0.77483336\n",
      "Iteration 37, loss = 0.77405044\n",
      "Iteration 38, loss = 0.77327935\n",
      "Iteration 39, loss = 0.77248576\n",
      "Iteration 40, loss = 0.77171648\n",
      "Iteration 41, loss = 0.77093054\n",
      "Iteration 42, loss = 0.77015663\n",
      "Iteration 43, loss = 0.76937124\n",
      "Iteration 44, loss = 0.76859373\n",
      "Iteration 45, loss = 0.76783891\n",
      "Iteration 46, loss = 0.76706718\n",
      "Iteration 47, loss = 0.76632894\n",
      "Iteration 48, loss = 0.76555812\n",
      "Iteration 49, loss = 0.76480544\n",
      "Iteration 50, loss = 0.76406755\n",
      "Iteration 51, loss = 0.76330039\n",
      "Iteration 52, loss = 0.76254629\n",
      "Iteration 53, loss = 0.76182100\n",
      "Iteration 54, loss = 0.76108231\n",
      "Iteration 55, loss = 0.76033251\n",
      "Iteration 56, loss = 0.75961993\n",
      "Iteration 57, loss = 0.75888889\n",
      "Iteration 58, loss = 0.75816431\n",
      "Iteration 59, loss = 0.75744920\n",
      "Iteration 60, loss = 0.75670729\n",
      "Iteration 61, loss = 0.75599975\n",
      "Iteration 62, loss = 0.75528624\n",
      "Iteration 63, loss = 0.75457366\n",
      "Iteration 64, loss = 0.75388934\n",
      "Iteration 65, loss = 0.75317043\n",
      "Iteration 66, loss = 0.75248781\n",
      "Iteration 67, loss = 0.75175649\n",
      "Iteration 68, loss = 0.75106909\n",
      "Iteration 69, loss = 0.75033676\n",
      "Iteration 70, loss = 0.74964497\n",
      "Iteration 71, loss = 0.74891151\n",
      "Iteration 72, loss = 0.74822450\n",
      "Iteration 73, loss = 0.74749703\n",
      "Iteration 74, loss = 0.74678568\n",
      "Iteration 75, loss = 0.74605716\n",
      "Iteration 76, loss = 0.74533741\n",
      "Iteration 77, loss = 0.74463735\n",
      "Iteration 78, loss = 0.74389551\n",
      "Iteration 79, loss = 0.74317171\n",
      "Iteration 80, loss = 0.74248301\n",
      "Iteration 81, loss = 0.74176282\n",
      "Iteration 82, loss = 0.74106090\n",
      "Iteration 83, loss = 0.74034411\n",
      "Iteration 84, loss = 0.73962885\n",
      "Iteration 85, loss = 0.73895300\n",
      "Iteration 86, loss = 0.73827106\n",
      "Iteration 87, loss = 0.73759924\n",
      "Iteration 88, loss = 0.73688843\n",
      "Iteration 89, loss = 0.73621824\n",
      "Iteration 90, loss = 0.73553346\n",
      "Iteration 91, loss = 0.73485705\n",
      "Iteration 92, loss = 0.73418216\n",
      "Iteration 93, loss = 0.73350952\n",
      "Iteration 94, loss = 0.73282222\n",
      "Iteration 95, loss = 0.73217262\n",
      "Iteration 96, loss = 0.73149214\n",
      "Iteration 97, loss = 0.73082976\n",
      "Iteration 98, loss = 0.73015904\n",
      "Iteration 99, loss = 0.72948672\n",
      "Iteration 100, loss = 0.72884174\n",
      "Iteration 101, loss = 0.72815764\n",
      "Iteration 102, loss = 0.72752155\n",
      "Iteration 103, loss = 0.72686047\n",
      "Iteration 104, loss = 0.72618649\n",
      "Iteration 105, loss = 0.72553472\n",
      "Iteration 106, loss = 0.72487606\n",
      "Iteration 107, loss = 0.72422088\n",
      "Iteration 108, loss = 0.72358325\n",
      "Iteration 109, loss = 0.72294285\n",
      "Iteration 110, loss = 0.72232099\n",
      "Iteration 111, loss = 0.72166089\n",
      "Iteration 112, loss = 0.72103160\n",
      "Iteration 113, loss = 0.72039526\n",
      "Iteration 114, loss = 0.71976781\n",
      "Iteration 115, loss = 0.71914889\n",
      "Iteration 116, loss = 0.71853294\n",
      "Iteration 117, loss = 0.71791506\n",
      "Iteration 118, loss = 0.71729512\n",
      "Iteration 119, loss = 0.71665753\n",
      "Iteration 120, loss = 0.71601571\n",
      "Iteration 121, loss = 0.71536630\n",
      "Iteration 122, loss = 0.71476882\n",
      "Iteration 123, loss = 0.71414550\n",
      "Iteration 124, loss = 0.71354328\n",
      "Iteration 125, loss = 0.71291586\n",
      "Iteration 126, loss = 0.71229936\n",
      "Iteration 127, loss = 0.71166931\n",
      "Iteration 128, loss = 0.71107388\n",
      "Iteration 129, loss = 0.71043774\n",
      "Iteration 130, loss = 0.70982657\n",
      "Iteration 131, loss = 0.70920043\n",
      "Iteration 132, loss = 0.70859389\n",
      "Iteration 133, loss = 0.70797507\n",
      "Iteration 134, loss = 0.70735304\n",
      "Iteration 135, loss = 0.70674631\n",
      "Iteration 136, loss = 0.70612874\n",
      "Iteration 137, loss = 0.70550296\n",
      "Iteration 138, loss = 0.70490632\n",
      "Iteration 139, loss = 0.70428723\n",
      "Iteration 140, loss = 0.70368526\n",
      "Iteration 141, loss = 0.70307346\n",
      "Iteration 142, loss = 0.70247069\n",
      "Iteration 143, loss = 0.70188393\n",
      "Iteration 144, loss = 0.70128038\n",
      "Iteration 145, loss = 0.70069691\n",
      "Iteration 146, loss = 0.70011063\n",
      "Iteration 147, loss = 0.69951854\n",
      "Iteration 148, loss = 0.69891317\n",
      "Iteration 149, loss = 0.69831669\n",
      "Iteration 150, loss = 0.69773303\n",
      "Iteration 151, loss = 0.69713883\n",
      "Iteration 152, loss = 0.69656854\n",
      "Iteration 153, loss = 0.69596535\n",
      "Iteration 154, loss = 0.69539778\n",
      "Iteration 155, loss = 0.69483107\n",
      "Iteration 156, loss = 0.69425611\n",
      "Iteration 157, loss = 0.69370339\n",
      "Iteration 158, loss = 0.69310903\n",
      "Iteration 159, loss = 0.69256095\n",
      "Iteration 160, loss = 0.69199849\n",
      "Iteration 161, loss = 0.69141289\n",
      "Iteration 162, loss = 0.69085790\n",
      "Iteration 163, loss = 0.69032814\n",
      "Iteration 164, loss = 0.68973730\n",
      "Iteration 165, loss = 0.68918061\n",
      "Iteration 166, loss = 0.68863941\n",
      "Iteration 167, loss = 0.68807328\n",
      "Iteration 168, loss = 0.68753026\n",
      "Iteration 169, loss = 0.68696637\n",
      "Iteration 170, loss = 0.68641564\n",
      "Iteration 171, loss = 0.68585721\n",
      "Iteration 172, loss = 0.68530529\n",
      "Iteration 173, loss = 0.68474065\n",
      "Iteration 174, loss = 0.68417462\n",
      "Iteration 175, loss = 0.68361177\n",
      "Iteration 176, loss = 0.68304104\n",
      "Iteration 177, loss = 0.68249675\n",
      "Iteration 178, loss = 0.68194222\n",
      "Iteration 179, loss = 0.68137130\n",
      "Iteration 180, loss = 0.68081706\n",
      "Iteration 181, loss = 0.68027968\n",
      "Iteration 182, loss = 0.67971469\n",
      "Iteration 183, loss = 0.67916495\n",
      "Iteration 184, loss = 0.67863575\n",
      "Iteration 185, loss = 0.67807016\n",
      "Iteration 186, loss = 0.67752925\n",
      "Iteration 187, loss = 0.67698990\n",
      "Iteration 188, loss = 0.67643319\n",
      "Iteration 189, loss = 0.67589462\n",
      "Iteration 190, loss = 0.67534750\n",
      "Iteration 191, loss = 0.67483057\n",
      "Iteration 192, loss = 0.67430654\n",
      "Iteration 193, loss = 0.67379015\n",
      "Iteration 194, loss = 0.67328000\n",
      "Iteration 195, loss = 0.67275581\n",
      "Iteration 196, loss = 0.67221996\n",
      "Iteration 197, loss = 0.67168946\n",
      "Iteration 198, loss = 0.67118237\n",
      "Iteration 199, loss = 0.67065320\n",
      "Iteration 200, loss = 0.67010726\n",
      "Iteration 201, loss = 0.66956166\n",
      "Iteration 202, loss = 0.66905183\n",
      "Iteration 203, loss = 0.66852343\n",
      "Iteration 204, loss = 0.66800593\n",
      "Iteration 205, loss = 0.66750400\n",
      "Iteration 206, loss = 0.66699665\n",
      "Iteration 207, loss = 0.66649346\n",
      "Iteration 208, loss = 0.66598857\n",
      "Iteration 209, loss = 0.66547768\n",
      "Iteration 210, loss = 0.66497384\n",
      "Iteration 211, loss = 0.66447380\n",
      "Iteration 212, loss = 0.66397513\n",
      "Iteration 213, loss = 0.66346725\n",
      "Iteration 214, loss = 0.66295940\n",
      "Iteration 215, loss = 0.66246079\n",
      "Iteration 216, loss = 0.66194650\n",
      "Iteration 217, loss = 0.66146015\n",
      "Iteration 218, loss = 0.66096776\n",
      "Iteration 219, loss = 0.66046691\n",
      "Iteration 220, loss = 0.65999401\n",
      "Iteration 221, loss = 0.65949304\n",
      "Iteration 222, loss = 0.65899149\n",
      "Iteration 223, loss = 0.65849093\n",
      "Iteration 224, loss = 0.65801060\n",
      "Iteration 225, loss = 0.65749716\n",
      "Iteration 226, loss = 0.65699945\n",
      "Iteration 227, loss = 0.65650154\n",
      "Iteration 228, loss = 0.65602974\n",
      "Iteration 229, loss = 0.65554441\n",
      "Iteration 230, loss = 0.65506881\n",
      "Iteration 231, loss = 0.65460010\n",
      "Iteration 232, loss = 0.65411860\n",
      "Iteration 233, loss = 0.65363598\n",
      "Iteration 234, loss = 0.65314308\n",
      "Iteration 235, loss = 0.65267581\n",
      "Iteration 236, loss = 0.65218746\n",
      "Iteration 237, loss = 0.65170007\n",
      "Iteration 238, loss = 0.65122754\n",
      "Iteration 239, loss = 0.65076001\n",
      "Iteration 240, loss = 0.65027600\n",
      "Iteration 241, loss = 0.64981366\n",
      "Iteration 242, loss = 0.64933511\n",
      "Iteration 243, loss = 0.64888281\n",
      "Iteration 244, loss = 0.64842017\n",
      "Iteration 245, loss = 0.64796552\n",
      "Iteration 246, loss = 0.64751166\n",
      "Iteration 247, loss = 0.64705095\n",
      "Iteration 248, loss = 0.64659138\n",
      "Iteration 249, loss = 0.64612495\n",
      "Iteration 250, loss = 0.64567173\n",
      "Iteration 251, loss = 0.64520601\n",
      "Iteration 252, loss = 0.64475839\n",
      "Iteration 253, loss = 0.64431312\n",
      "Iteration 254, loss = 0.64384249\n",
      "Iteration 255, loss = 0.64340856\n",
      "Iteration 256, loss = 0.64296174\n",
      "Iteration 257, loss = 0.64250949\n",
      "Iteration 258, loss = 0.64208108\n",
      "Iteration 259, loss = 0.64161368\n",
      "Iteration 260, loss = 0.64118651\n",
      "Iteration 261, loss = 0.64073292\n",
      "Iteration 262, loss = 0.64028944\n",
      "Iteration 263, loss = 0.63983513\n",
      "Iteration 264, loss = 0.63939893\n",
      "Iteration 265, loss = 0.63896203\n",
      "Iteration 266, loss = 0.63853208\n",
      "Iteration 267, loss = 0.63811061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 268, loss = 0.63767137\n",
      "Iteration 269, loss = 0.63724497\n",
      "Iteration 270, loss = 0.63683206\n",
      "Iteration 271, loss = 0.63637843\n",
      "Iteration 272, loss = 0.63596027\n",
      "Iteration 273, loss = 0.63554163\n",
      "Iteration 274, loss = 0.63509953\n",
      "Iteration 275, loss = 0.63468427\n",
      "Iteration 276, loss = 0.63423587\n",
      "Iteration 277, loss = 0.63381156\n",
      "Iteration 278, loss = 0.63339439\n",
      "Iteration 279, loss = 0.63297046\n",
      "Iteration 280, loss = 0.63255998\n",
      "Iteration 281, loss = 0.63213539\n",
      "Iteration 282, loss = 0.63172896\n",
      "Iteration 283, loss = 0.63132203\n",
      "Iteration 284, loss = 0.63089714\n",
      "Iteration 285, loss = 0.63048063\n",
      "Iteration 286, loss = 0.63007863\n",
      "Iteration 287, loss = 0.62965344\n",
      "Iteration 288, loss = 0.62925194\n",
      "Iteration 289, loss = 0.62882370\n",
      "Iteration 290, loss = 0.62842665\n",
      "Iteration 291, loss = 0.62802133\n",
      "Iteration 292, loss = 0.62761514\n",
      "Iteration 293, loss = 0.62721926\n",
      "Iteration 294, loss = 0.62680615\n",
      "Iteration 295, loss = 0.62639402\n",
      "Iteration 296, loss = 0.62601206\n",
      "Iteration 297, loss = 0.62560373\n",
      "Iteration 298, loss = 0.62519555\n",
      "Iteration 299, loss = 0.62479827\n",
      "Iteration 300, loss = 0.62437834\n",
      "Iteration 301, loss = 0.62397051\n",
      "Iteration 302, loss = 0.62355236\n",
      "Iteration 303, loss = 0.62315424\n",
      "Iteration 304, loss = 0.62275614\n",
      "Iteration 305, loss = 0.62235041\n",
      "Iteration 306, loss = 0.62196476\n",
      "Iteration 307, loss = 0.62155735\n",
      "Iteration 308, loss = 0.62116403\n",
      "Iteration 309, loss = 0.62076858\n",
      "Iteration 310, loss = 0.62038220\n",
      "Iteration 311, loss = 0.61997806\n",
      "Iteration 312, loss = 0.61958296\n",
      "Iteration 313, loss = 0.61919024\n",
      "Iteration 314, loss = 0.61879713\n",
      "Iteration 315, loss = 0.61840106\n",
      "Iteration 316, loss = 0.61800810\n",
      "Iteration 317, loss = 0.61763044\n",
      "Iteration 318, loss = 0.61723267\n",
      "Iteration 319, loss = 0.61686560\n",
      "Iteration 320, loss = 0.61648578\n",
      "Iteration 321, loss = 0.61609560\n",
      "Iteration 322, loss = 0.61570622\n",
      "Iteration 323, loss = 0.61533942\n",
      "Iteration 324, loss = 0.61495027\n",
      "Iteration 325, loss = 0.61458601\n",
      "Iteration 326, loss = 0.61420337\n",
      "Iteration 327, loss = 0.61382653\n",
      "Iteration 328, loss = 0.61344912\n",
      "Iteration 329, loss = 0.61307978\n",
      "Iteration 330, loss = 0.61270774\n",
      "Iteration 331, loss = 0.61234636\n",
      "Iteration 332, loss = 0.61195987\n",
      "Iteration 333, loss = 0.61160505\n",
      "Iteration 334, loss = 0.61123258\n",
      "Iteration 335, loss = 0.61086762\n",
      "Iteration 336, loss = 0.61048594\n",
      "Iteration 337, loss = 0.61012530\n",
      "Iteration 338, loss = 0.60976390\n",
      "Iteration 339, loss = 0.60938771\n",
      "Iteration 340, loss = 0.60902352\n",
      "Iteration 341, loss = 0.60866788\n",
      "Iteration 342, loss = 0.60830763\n",
      "Iteration 343, loss = 0.60793238\n",
      "Iteration 344, loss = 0.60756825\n",
      "Iteration 345, loss = 0.60720493\n",
      "Iteration 346, loss = 0.60684618\n",
      "Iteration 347, loss = 0.60645922\n",
      "Iteration 348, loss = 0.60611402\n",
      "Iteration 349, loss = 0.60574306\n",
      "Iteration 350, loss = 0.60538731\n",
      "Iteration 351, loss = 0.60502106\n",
      "Iteration 352, loss = 0.60466048\n",
      "Iteration 353, loss = 0.60430624\n",
      "Iteration 354, loss = 0.60394843\n",
      "Iteration 355, loss = 0.60361170\n",
      "Iteration 356, loss = 0.60325166\n",
      "Iteration 357, loss = 0.60291581\n",
      "Iteration 358, loss = 0.60255654\n",
      "Iteration 359, loss = 0.60221441\n",
      "Iteration 360, loss = 0.60187980\n",
      "Iteration 361, loss = 0.60152308\n",
      "Iteration 362, loss = 0.60119697\n",
      "Iteration 363, loss = 0.60085570\n",
      "Iteration 364, loss = 0.60051764\n",
      "Iteration 365, loss = 0.60015285\n",
      "Iteration 366, loss = 0.59982072\n",
      "Iteration 367, loss = 0.59947754\n",
      "Iteration 368, loss = 0.59913103\n",
      "Iteration 369, loss = 0.59879801\n",
      "Iteration 370, loss = 0.59846393\n",
      "Iteration 371, loss = 0.59812341\n",
      "Iteration 372, loss = 0.59778688\n",
      "Iteration 373, loss = 0.59745169\n",
      "Iteration 374, loss = 0.59710391\n",
      "Iteration 375, loss = 0.59675137\n",
      "Iteration 376, loss = 0.59640386\n",
      "Iteration 377, loss = 0.59605243\n",
      "Iteration 378, loss = 0.59571633\n",
      "Iteration 379, loss = 0.59537378\n",
      "Iteration 380, loss = 0.59504800\n",
      "Iteration 381, loss = 0.59472010\n",
      "Iteration 382, loss = 0.59439652\n",
      "Iteration 383, loss = 0.59405600\n",
      "Iteration 384, loss = 0.59372995\n",
      "Iteration 385, loss = 0.59340197\n",
      "Iteration 386, loss = 0.59307274\n",
      "Iteration 387, loss = 0.59276416\n",
      "Iteration 388, loss = 0.59242126\n",
      "Iteration 389, loss = 0.59209965\n",
      "Iteration 390, loss = 0.59177690\n",
      "Iteration 391, loss = 0.59146629\n",
      "Iteration 392, loss = 0.59113049\n",
      "Iteration 393, loss = 0.59082119\n",
      "Iteration 394, loss = 0.59050048\n",
      "Iteration 395, loss = 0.59019179\n",
      "Iteration 396, loss = 0.58987929\n",
      "Iteration 397, loss = 0.58955265\n",
      "Iteration 398, loss = 0.58923601\n",
      "Iteration 399, loss = 0.58891123\n",
      "Iteration 400, loss = 0.58859690\n",
      "Iteration 401, loss = 0.58828539\n",
      "Iteration 402, loss = 0.58797039\n",
      "Iteration 403, loss = 0.58765427\n",
      "Iteration 404, loss = 0.58733700\n",
      "Iteration 405, loss = 0.58703951\n",
      "Iteration 406, loss = 0.58671445\n",
      "Iteration 407, loss = 0.58642129\n",
      "Iteration 408, loss = 0.58609786\n",
      "Iteration 409, loss = 0.58580608\n",
      "Iteration 410, loss = 0.58548568\n",
      "Iteration 411, loss = 0.58519189\n",
      "Iteration 412, loss = 0.58489359\n",
      "Iteration 413, loss = 0.58457815\n",
      "Iteration 414, loss = 0.58427440\n",
      "Iteration 415, loss = 0.58398396\n",
      "Iteration 416, loss = 0.58369105\n",
      "Iteration 417, loss = 0.58338662\n",
      "Iteration 418, loss = 0.58308433\n",
      "Iteration 419, loss = 0.58278415\n",
      "Iteration 420, loss = 0.58248024\n",
      "Iteration 421, loss = 0.58218802\n",
      "Iteration 422, loss = 0.58189380\n",
      "Iteration 423, loss = 0.58158731\n",
      "Iteration 424, loss = 0.58129147\n",
      "Iteration 425, loss = 0.58099820\n",
      "Iteration 426, loss = 0.58069375\n",
      "Iteration 427, loss = 0.58040619\n",
      "Iteration 428, loss = 0.58008620\n",
      "Iteration 429, loss = 0.57980837\n",
      "Iteration 430, loss = 0.57950282\n",
      "Iteration 431, loss = 0.57919425\n",
      "Iteration 432, loss = 0.57890383\n",
      "Iteration 433, loss = 0.57860738\n",
      "Iteration 434, loss = 0.57830341\n",
      "Iteration 435, loss = 0.57801518\n",
      "Iteration 436, loss = 0.57775253\n",
      "Iteration 437, loss = 0.57745464\n",
      "Iteration 438, loss = 0.57716953\n",
      "Iteration 439, loss = 0.57690032\n",
      "Iteration 440, loss = 0.57661835\n",
      "Iteration 441, loss = 0.57634790\n",
      "Iteration 442, loss = 0.57606881\n",
      "Iteration 443, loss = 0.57577390\n",
      "Iteration 444, loss = 0.57550873\n",
      "Iteration 445, loss = 0.57523775\n",
      "Iteration 446, loss = 0.57496212\n",
      "Iteration 447, loss = 0.57469567\n",
      "Iteration 448, loss = 0.57442353\n",
      "Iteration 449, loss = 0.57412839\n",
      "Iteration 450, loss = 0.57387461\n",
      "Iteration 451, loss = 0.57358912\n",
      "Iteration 452, loss = 0.57331969\n",
      "Iteration 453, loss = 0.57304842\n",
      "Iteration 454, loss = 0.57277097\n",
      "Iteration 455, loss = 0.57250311\n",
      "Iteration 456, loss = 0.57222601\n",
      "Iteration 457, loss = 0.57194572\n",
      "Iteration 458, loss = 0.57166914\n",
      "Iteration 459, loss = 0.57139527\n",
      "Iteration 460, loss = 0.57112740\n",
      "Iteration 461, loss = 0.57084825\n",
      "Iteration 462, loss = 0.57058909\n",
      "Iteration 463, loss = 0.57032189\n",
      "Iteration 464, loss = 0.57003888\n",
      "Iteration 465, loss = 0.56978294\n",
      "Iteration 466, loss = 0.56949440\n",
      "Iteration 467, loss = 0.56922960\n",
      "Iteration 468, loss = 0.56895498\n",
      "Iteration 469, loss = 0.56868773\n",
      "Iteration 470, loss = 0.56840960\n",
      "Iteration 471, loss = 0.56814392\n",
      "Iteration 472, loss = 0.56787590\n",
      "Iteration 473, loss = 0.56760754\n",
      "Iteration 474, loss = 0.56734843\n",
      "Iteration 475, loss = 0.56709966\n",
      "Iteration 476, loss = 0.56683608\n",
      "Iteration 477, loss = 0.56657665\n",
      "Iteration 478, loss = 0.56632576\n",
      "Iteration 479, loss = 0.56606030\n",
      "Iteration 480, loss = 0.56581258\n",
      "Iteration 481, loss = 0.56555113\n",
      "Iteration 482, loss = 0.56529127\n",
      "Iteration 483, loss = 0.56503954\n",
      "Iteration 484, loss = 0.56479762\n",
      "Iteration 485, loss = 0.56453653\n",
      "Iteration 486, loss = 0.56427608\n",
      "Iteration 487, loss = 0.56402830\n",
      "Iteration 488, loss = 0.56376077\n",
      "Iteration 489, loss = 0.56351825\n",
      "Iteration 490, loss = 0.56326535\n",
      "Iteration 491, loss = 0.56301389\n",
      "Iteration 492, loss = 0.56275963\n",
      "Iteration 493, loss = 0.56251094\n",
      "Iteration 494, loss = 0.56226862\n",
      "Iteration 495, loss = 0.56202443\n",
      "Iteration 496, loss = 0.56177760\n",
      "Iteration 497, loss = 0.56152832\n",
      "Iteration 498, loss = 0.56129736\n",
      "Iteration 499, loss = 0.56105012\n",
      "Iteration 500, loss = 0.56081704\n",
      "Iteration 501, loss = 0.56058638\n",
      "Iteration 502, loss = 0.56034506\n",
      "Iteration 503, loss = 0.56009782\n",
      "Iteration 504, loss = 0.55984943\n",
      "Iteration 505, loss = 0.55961590\n",
      "Iteration 506, loss = 0.55936568\n",
      "Iteration 507, loss = 0.55912901\n",
      "Iteration 508, loss = 0.55888778\n",
      "Iteration 509, loss = 0.55865244\n",
      "Iteration 510, loss = 0.55841042\n",
      "Iteration 511, loss = 0.55818052\n",
      "Iteration 512, loss = 0.55794474\n",
      "Iteration 513, loss = 0.55771567\n",
      "Iteration 514, loss = 0.55747817\n",
      "Iteration 515, loss = 0.55725070\n",
      "Iteration 516, loss = 0.55702768\n",
      "Iteration 517, loss = 0.55678509\n",
      "Iteration 518, loss = 0.55655990\n",
      "Iteration 519, loss = 0.55632828\n",
      "Iteration 520, loss = 0.55609261\n",
      "Iteration 521, loss = 0.55585412\n",
      "Iteration 522, loss = 0.55562140\n",
      "Iteration 523, loss = 0.55539129\n",
      "Iteration 524, loss = 0.55515132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 525, loss = 0.55493333\n",
      "Iteration 526, loss = 0.55470708\n",
      "Iteration 527, loss = 0.55447239\n",
      "Iteration 528, loss = 0.55425609\n",
      "Iteration 529, loss = 0.55401502\n",
      "Iteration 530, loss = 0.55379717\n",
      "Iteration 531, loss = 0.55355939\n",
      "Iteration 532, loss = 0.55333940\n",
      "Iteration 533, loss = 0.55311626\n",
      "Iteration 534, loss = 0.55288689\n",
      "Iteration 535, loss = 0.55268170\n",
      "Iteration 536, loss = 0.55246206\n",
      "Iteration 537, loss = 0.55222423\n",
      "Iteration 538, loss = 0.55199760\n",
      "Iteration 539, loss = 0.55177985\n",
      "Iteration 540, loss = 0.55154257\n",
      "Iteration 541, loss = 0.55131848\n",
      "Iteration 542, loss = 0.55109072\n",
      "Iteration 543, loss = 0.55084983\n",
      "Iteration 544, loss = 0.55062282\n",
      "Iteration 545, loss = 0.55039786\n",
      "Iteration 546, loss = 0.55016750\n",
      "Iteration 547, loss = 0.54995656\n",
      "Iteration 548, loss = 0.54972252\n",
      "Iteration 549, loss = 0.54950941\n",
      "Iteration 550, loss = 0.54928744\n",
      "Iteration 551, loss = 0.54906735\n",
      "Iteration 552, loss = 0.54883912\n",
      "Iteration 553, loss = 0.54863189\n",
      "Iteration 554, loss = 0.54842574\n",
      "Iteration 555, loss = 0.54820907\n",
      "Iteration 556, loss = 0.54800725\n",
      "Iteration 557, loss = 0.54779776\n",
      "Iteration 558, loss = 0.54758490\n",
      "Iteration 559, loss = 0.54738630\n",
      "Iteration 560, loss = 0.54716333\n",
      "Iteration 561, loss = 0.54695323\n",
      "Iteration 562, loss = 0.54672678\n",
      "Iteration 563, loss = 0.54652299\n",
      "Iteration 564, loss = 0.54629414\n",
      "Iteration 565, loss = 0.54609340\n",
      "Iteration 566, loss = 0.54585669\n",
      "Iteration 567, loss = 0.54565021\n",
      "Iteration 568, loss = 0.54542915\n",
      "Iteration 569, loss = 0.54520849\n",
      "Iteration 570, loss = 0.54500661\n",
      "Iteration 571, loss = 0.54479706\n",
      "Iteration 572, loss = 0.54459033\n",
      "Iteration 573, loss = 0.54438569\n",
      "Iteration 574, loss = 0.54416375\n",
      "Iteration 575, loss = 0.54396607\n",
      "Iteration 576, loss = 0.54375136\n",
      "Iteration 577, loss = 0.54354416\n",
      "Iteration 578, loss = 0.54333045\n",
      "Iteration 579, loss = 0.54312508\n",
      "Iteration 580, loss = 0.54291946\n",
      "Iteration 581, loss = 0.54270448\n",
      "Iteration 582, loss = 0.54249678\n",
      "Iteration 583, loss = 0.54229817\n",
      "Iteration 584, loss = 0.54207612\n",
      "Iteration 585, loss = 0.54186667\n",
      "Iteration 586, loss = 0.54165541\n",
      "Iteration 587, loss = 0.54143840\n",
      "Iteration 588, loss = 0.54124614\n",
      "Iteration 589, loss = 0.54103748\n",
      "Iteration 590, loss = 0.54083448\n",
      "Iteration 591, loss = 0.54063626\n",
      "Iteration 592, loss = 0.54043875\n",
      "Iteration 593, loss = 0.54024416\n",
      "Iteration 594, loss = 0.54003873\n",
      "Iteration 595, loss = 0.53984989\n",
      "Iteration 596, loss = 0.53964852\n",
      "Iteration 597, loss = 0.53944844\n",
      "Iteration 598, loss = 0.53925110\n",
      "Iteration 599, loss = 0.53904062\n",
      "Iteration 600, loss = 0.53883754\n",
      "Iteration 601, loss = 0.53865007\n",
      "Iteration 602, loss = 0.53844462\n",
      "Iteration 603, loss = 0.53824168\n",
      "Iteration 604, loss = 0.53804532\n",
      "Iteration 605, loss = 0.53785542\n",
      "Iteration 606, loss = 0.53765664\n",
      "Iteration 607, loss = 0.53745910\n",
      "Iteration 608, loss = 0.53727042\n",
      "Iteration 609, loss = 0.53706147\n",
      "Iteration 610, loss = 0.53688276\n",
      "Iteration 611, loss = 0.53667097\n",
      "Iteration 612, loss = 0.53647836\n",
      "Iteration 613, loss = 0.53628930\n",
      "Iteration 614, loss = 0.53608267\n",
      "Iteration 615, loss = 0.53588420\n",
      "Iteration 616, loss = 0.53570471\n",
      "Iteration 617, loss = 0.53551879\n",
      "Iteration 618, loss = 0.53532538\n",
      "Iteration 619, loss = 0.53513367\n",
      "Iteration 620, loss = 0.53494770\n",
      "Iteration 621, loss = 0.53476558\n",
      "Iteration 622, loss = 0.53457646\n",
      "Iteration 623, loss = 0.53439366\n",
      "Iteration 624, loss = 0.53421937\n",
      "Iteration 625, loss = 0.53402301\n",
      "Iteration 626, loss = 0.53384018\n",
      "Iteration 627, loss = 0.53365491\n",
      "Iteration 628, loss = 0.53347416\n",
      "Iteration 629, loss = 0.53328114\n",
      "Iteration 630, loss = 0.53309528\n",
      "Iteration 631, loss = 0.53292004\n",
      "Iteration 632, loss = 0.53272311\n",
      "Iteration 633, loss = 0.53254413\n",
      "Iteration 634, loss = 0.53235371\n",
      "Iteration 635, loss = 0.53216962\n",
      "Iteration 636, loss = 0.53198651\n",
      "Iteration 637, loss = 0.53180237\n",
      "Iteration 638, loss = 0.53161483\n",
      "Iteration 639, loss = 0.53144473\n",
      "Iteration 640, loss = 0.53125973\n",
      "Iteration 641, loss = 0.53107416\n",
      "Iteration 642, loss = 0.53090329\n",
      "Iteration 643, loss = 0.53071853\n",
      "Iteration 644, loss = 0.53053359\n",
      "Iteration 645, loss = 0.53036509\n",
      "Iteration 646, loss = 0.53017820\n",
      "Iteration 647, loss = 0.53001387\n",
      "Iteration 648, loss = 0.52984274\n",
      "Iteration 649, loss = 0.52966142\n",
      "Iteration 650, loss = 0.52948863\n",
      "Iteration 651, loss = 0.52932390\n",
      "Iteration 652, loss = 0.52914931\n",
      "Iteration 653, loss = 0.52896958\n",
      "Iteration 654, loss = 0.52879261\n",
      "Iteration 655, loss = 0.52861808\n",
      "Iteration 656, loss = 0.52846015\n",
      "Iteration 657, loss = 0.52827601\n",
      "Iteration 658, loss = 0.52811091\n",
      "Iteration 659, loss = 0.52793489\n",
      "Iteration 660, loss = 0.52777509\n",
      "Iteration 661, loss = 0.52759814\n",
      "Iteration 662, loss = 0.52743314\n",
      "Iteration 663, loss = 0.52725352\n",
      "Iteration 664, loss = 0.52708306\n",
      "Iteration 665, loss = 0.52691276\n",
      "Iteration 666, loss = 0.52673005\n",
      "Iteration 667, loss = 0.52656129\n",
      "Iteration 668, loss = 0.52638727\n",
      "Iteration 669, loss = 0.52620993\n",
      "Iteration 670, loss = 0.52604494\n",
      "Iteration 671, loss = 0.52586962\n",
      "Iteration 672, loss = 0.52570655\n",
      "Iteration 673, loss = 0.52553323\n",
      "Iteration 674, loss = 0.52537070\n",
      "Iteration 675, loss = 0.52519919\n",
      "Iteration 676, loss = 0.52503333\n",
      "Iteration 677, loss = 0.52486549\n",
      "Iteration 678, loss = 0.52470930\n",
      "Iteration 679, loss = 0.52454098\n",
      "Iteration 680, loss = 0.52438136\n",
      "Iteration 681, loss = 0.52421066\n",
      "Iteration 682, loss = 0.52405469\n",
      "Iteration 683, loss = 0.52389874\n",
      "Iteration 684, loss = 0.52373031\n",
      "Iteration 685, loss = 0.52357237\n",
      "Iteration 686, loss = 0.52341436\n",
      "Iteration 687, loss = 0.52326127\n",
      "Iteration 688, loss = 0.52310273\n",
      "Iteration 689, loss = 0.52295114\n",
      "Iteration 690, loss = 0.52279569\n",
      "Iteration 691, loss = 0.52264894\n",
      "Iteration 692, loss = 0.52248349\n",
      "Iteration 693, loss = 0.52233594\n",
      "Iteration 694, loss = 0.52217330\n",
      "Iteration 695, loss = 0.52202447\n",
      "Iteration 696, loss = 0.52186351\n",
      "Iteration 697, loss = 0.52170442\n",
      "Iteration 698, loss = 0.52154875\n",
      "Iteration 699, loss = 0.52139365\n",
      "Iteration 700, loss = 0.52125084\n",
      "Iteration 701, loss = 0.52108623\n",
      "Iteration 702, loss = 0.52093994\n",
      "Iteration 703, loss = 0.52078308\n",
      "Iteration 704, loss = 0.52062864\n",
      "Iteration 705, loss = 0.52047022\n",
      "Iteration 706, loss = 0.52032366\n",
      "Iteration 707, loss = 0.52017411\n",
      "Iteration 708, loss = 0.52002079\n",
      "Iteration 709, loss = 0.51986206\n",
      "Iteration 710, loss = 0.51970988\n",
      "Iteration 711, loss = 0.51956367\n",
      "Iteration 712, loss = 0.51940998\n",
      "Iteration 713, loss = 0.51926886\n",
      "Iteration 714, loss = 0.51913399\n",
      "Iteration 715, loss = 0.51898192\n",
      "Iteration 716, loss = 0.51883545\n",
      "Iteration 717, loss = 0.51869107\n",
      "Iteration 718, loss = 0.51853832\n",
      "Iteration 719, loss = 0.51838440\n",
      "Iteration 720, loss = 0.51823816\n",
      "Iteration 721, loss = 0.51808310\n",
      "Iteration 722, loss = 0.51793486\n",
      "Iteration 723, loss = 0.51778168\n",
      "Iteration 724, loss = 0.51763298\n",
      "Iteration 725, loss = 0.51749117\n",
      "Iteration 726, loss = 0.51733842\n",
      "Iteration 727, loss = 0.51718817\n",
      "Iteration 728, loss = 0.51704090\n",
      "Iteration 729, loss = 0.51690105\n",
      "Iteration 730, loss = 0.51675341\n",
      "Iteration 731, loss = 0.51660993\n",
      "Iteration 732, loss = 0.51645779\n",
      "Iteration 733, loss = 0.51631108\n",
      "Iteration 734, loss = 0.51616880\n",
      "Iteration 735, loss = 0.51602945\n",
      "Iteration 736, loss = 0.51588224\n",
      "Iteration 737, loss = 0.51573244\n",
      "Iteration 738, loss = 0.51558753\n",
      "Iteration 739, loss = 0.51544164\n",
      "Iteration 740, loss = 0.51530775\n",
      "Iteration 741, loss = 0.51516240\n",
      "Iteration 742, loss = 0.51503740\n",
      "Iteration 743, loss = 0.51489115\n",
      "Iteration 744, loss = 0.51475169\n",
      "Iteration 745, loss = 0.51460440\n",
      "Iteration 746, loss = 0.51445975\n",
      "Iteration 747, loss = 0.51432184\n",
      "Iteration 748, loss = 0.51417353\n",
      "Iteration 749, loss = 0.51402397\n",
      "Iteration 750, loss = 0.51387715\n",
      "Iteration 751, loss = 0.51372779\n",
      "Iteration 752, loss = 0.51359425\n",
      "Iteration 753, loss = 0.51345292\n",
      "Iteration 754, loss = 0.51331397\n",
      "Iteration 755, loss = 0.51317734\n",
      "Iteration 756, loss = 0.51304195\n",
      "Iteration 757, loss = 0.51292535\n",
      "Iteration 758, loss = 0.51278229\n",
      "Iteration 759, loss = 0.51265644\n",
      "Iteration 760, loss = 0.51251417\n",
      "Iteration 761, loss = 0.51238233\n",
      "Iteration 762, loss = 0.51224875\n",
      "Iteration 763, loss = 0.51211481\n",
      "Iteration 764, loss = 0.51197357\n",
      "Iteration 765, loss = 0.51183118\n",
      "Iteration 766, loss = 0.51169847\n",
      "Iteration 767, loss = 0.51155890\n",
      "Iteration 768, loss = 0.51142411\n",
      "Iteration 769, loss = 0.51129926\n",
      "Iteration 770, loss = 0.51115154\n",
      "Iteration 771, loss = 0.51101903\n",
      "Iteration 772, loss = 0.51088235\n",
      "Iteration 773, loss = 0.51074426\n",
      "Iteration 774, loss = 0.51061090\n",
      "Iteration 775, loss = 0.51047569\n",
      "Iteration 776, loss = 0.51033892\n",
      "Iteration 777, loss = 0.51021427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 778, loss = 0.51008766\n",
      "Iteration 779, loss = 0.50995892\n",
      "Iteration 780, loss = 0.50983262\n",
      "Iteration 781, loss = 0.50969632\n",
      "Iteration 782, loss = 0.50957297\n",
      "Iteration 783, loss = 0.50944404\n",
      "Iteration 784, loss = 0.50931761\n",
      "Iteration 785, loss = 0.50918914\n",
      "Iteration 786, loss = 0.50907584\n",
      "Iteration 787, loss = 0.50894543\n",
      "Iteration 788, loss = 0.50882805\n",
      "Iteration 789, loss = 0.50869697\n",
      "Iteration 790, loss = 0.50857050\n",
      "Iteration 791, loss = 0.50844580\n",
      "Iteration 792, loss = 0.50832765\n",
      "Iteration 793, loss = 0.50819399\n",
      "Iteration 794, loss = 0.50807795\n",
      "Iteration 795, loss = 0.50795876\n",
      "Iteration 796, loss = 0.50784118\n",
      "Iteration 797, loss = 0.50772207\n",
      "Iteration 798, loss = 0.50760231\n",
      "Iteration 799, loss = 0.50748465\n",
      "Iteration 800, loss = 0.50735724\n",
      "Iteration 801, loss = 0.50724227\n",
      "Iteration 802, loss = 0.50712229\n",
      "Iteration 803, loss = 0.50700068\n",
      "Iteration 804, loss = 0.50687277\n",
      "Iteration 805, loss = 0.50674734\n",
      "Iteration 806, loss = 0.50662039\n",
      "Iteration 807, loss = 0.50650528\n",
      "Iteration 808, loss = 0.50638325\n",
      "Iteration 809, loss = 0.50624880\n",
      "Iteration 810, loss = 0.50612911\n",
      "Iteration 811, loss = 0.50600759\n",
      "Iteration 812, loss = 0.50587441\n",
      "Iteration 813, loss = 0.50575327\n",
      "Iteration 814, loss = 0.50562680\n",
      "Iteration 815, loss = 0.50548930\n",
      "Iteration 816, loss = 0.50536412\n",
      "Iteration 817, loss = 0.50524405\n",
      "Iteration 818, loss = 0.50511642\n",
      "Iteration 819, loss = 0.50497748\n",
      "Iteration 820, loss = 0.50485642\n",
      "Iteration 821, loss = 0.50473287\n",
      "Iteration 822, loss = 0.50460314\n",
      "Iteration 823, loss = 0.50448708\n",
      "Iteration 824, loss = 0.50436655\n",
      "Iteration 825, loss = 0.50424247\n",
      "Iteration 826, loss = 0.50411566\n",
      "Iteration 827, loss = 0.50398373\n",
      "Iteration 828, loss = 0.50386159\n",
      "Iteration 829, loss = 0.50373911\n",
      "Iteration 830, loss = 0.50361691\n",
      "Iteration 831, loss = 0.50349016\n",
      "Iteration 832, loss = 0.50337492\n",
      "Iteration 833, loss = 0.50325570\n",
      "Iteration 834, loss = 0.50313930\n",
      "Iteration 835, loss = 0.50301939\n",
      "Iteration 836, loss = 0.50291574\n",
      "Iteration 837, loss = 0.50280162\n",
      "Iteration 838, loss = 0.50267681\n",
      "Iteration 839, loss = 0.50257192\n",
      "Iteration 840, loss = 0.50245182\n",
      "Iteration 841, loss = 0.50233870\n",
      "Iteration 842, loss = 0.50222751\n",
      "Iteration 843, loss = 0.50211504\n",
      "Iteration 844, loss = 0.50200259\n",
      "Iteration 845, loss = 0.50188879\n",
      "Iteration 846, loss = 0.50177753\n",
      "Iteration 847, loss = 0.50166109\n",
      "Iteration 848, loss = 0.50154423\n",
      "Iteration 849, loss = 0.50143945\n",
      "Iteration 850, loss = 0.50132320\n",
      "Iteration 851, loss = 0.50121166\n",
      "Iteration 852, loss = 0.50110050\n",
      "Iteration 853, loss = 0.50099504\n",
      "Iteration 854, loss = 0.50088492\n",
      "Iteration 855, loss = 0.50078518\n",
      "Iteration 856, loss = 0.50067580\n",
      "Iteration 857, loss = 0.50055589\n",
      "Iteration 858, loss = 0.50044742\n",
      "Iteration 859, loss = 0.50033195\n",
      "Iteration 860, loss = 0.50022274\n",
      "Iteration 861, loss = 0.50010770\n",
      "Iteration 862, loss = 0.50000048\n",
      "Iteration 863, loss = 0.49988352\n",
      "Iteration 864, loss = 0.49977133\n",
      "Iteration 865, loss = 0.49966011\n",
      "Iteration 866, loss = 0.49955773\n",
      "Iteration 867, loss = 0.49944610\n",
      "Iteration 868, loss = 0.49934076\n",
      "Iteration 869, loss = 0.49922677\n",
      "Iteration 870, loss = 0.49912497\n",
      "Iteration 871, loss = 0.49900702\n",
      "Iteration 872, loss = 0.49890450\n",
      "Iteration 873, loss = 0.49879605\n",
      "Iteration 874, loss = 0.49869397\n",
      "Iteration 875, loss = 0.49859964\n",
      "Iteration 876, loss = 0.49849328\n",
      "Iteration 877, loss = 0.49838524\n",
      "Iteration 878, loss = 0.49828444\n",
      "Iteration 879, loss = 0.49816895\n",
      "Iteration 880, loss = 0.49807479\n",
      "Iteration 881, loss = 0.49796799\n",
      "Iteration 882, loss = 0.49786192\n",
      "Iteration 883, loss = 0.49776125\n",
      "Iteration 884, loss = 0.49764622\n",
      "Iteration 885, loss = 0.49754007\n",
      "Iteration 886, loss = 0.49743931\n",
      "Iteration 887, loss = 0.49733334\n",
      "Iteration 888, loss = 0.49723706\n",
      "Iteration 889, loss = 0.49714109\n",
      "Iteration 890, loss = 0.49704195\n",
      "Iteration 891, loss = 0.49694469\n",
      "Iteration 892, loss = 0.49685230\n",
      "Iteration 893, loss = 0.49675735\n",
      "Iteration 894, loss = 0.49665941\n",
      "Iteration 895, loss = 0.49656033\n",
      "Iteration 896, loss = 0.49647911\n",
      "Iteration 897, loss = 0.49638395\n",
      "Iteration 898, loss = 0.49628834\n",
      "Iteration 899, loss = 0.49619879\n",
      "Iteration 900, loss = 0.49610373\n",
      "Iteration 901, loss = 0.49600676\n",
      "Iteration 902, loss = 0.49590741\n",
      "Iteration 903, loss = 0.49582331\n",
      "Iteration 904, loss = 0.49572351\n",
      "Iteration 905, loss = 0.49562993\n",
      "Iteration 906, loss = 0.49554293\n",
      "Iteration 907, loss = 0.49544429\n",
      "Iteration 908, loss = 0.49533998\n",
      "Iteration 909, loss = 0.49524297\n",
      "Iteration 910, loss = 0.49513811\n",
      "Iteration 911, loss = 0.49503239\n",
      "Iteration 912, loss = 0.49493496\n",
      "Iteration 913, loss = 0.49483050\n",
      "Iteration 914, loss = 0.49473202\n",
      "Iteration 915, loss = 0.49462445\n",
      "Iteration 916, loss = 0.49452783\n",
      "Iteration 917, loss = 0.49443658\n",
      "Iteration 918, loss = 0.49433454\n",
      "Iteration 919, loss = 0.49424477\n",
      "Iteration 920, loss = 0.49414856\n",
      "Iteration 921, loss = 0.49405902\n",
      "Iteration 922, loss = 0.49396819\n",
      "Iteration 923, loss = 0.49387376\n",
      "Iteration 924, loss = 0.49377927\n",
      "Iteration 925, loss = 0.49367999\n",
      "Iteration 926, loss = 0.49358961\n",
      "Iteration 927, loss = 0.49348168\n",
      "Iteration 928, loss = 0.49339460\n",
      "Iteration 929, loss = 0.49329163\n",
      "Iteration 930, loss = 0.49320287\n",
      "Iteration 931, loss = 0.49309890\n",
      "Iteration 932, loss = 0.49300678\n",
      "Iteration 933, loss = 0.49291469\n",
      "Iteration 934, loss = 0.49281902\n",
      "Iteration 935, loss = 0.49272049\n",
      "Iteration 936, loss = 0.49262801\n",
      "Iteration 937, loss = 0.49253199\n",
      "Iteration 938, loss = 0.49243784\n",
      "Iteration 939, loss = 0.49234508\n",
      "Iteration 940, loss = 0.49224874\n",
      "Iteration 941, loss = 0.49214528\n",
      "Iteration 942, loss = 0.49204915\n",
      "Iteration 943, loss = 0.49195376\n",
      "Iteration 944, loss = 0.49186011\n",
      "Iteration 945, loss = 0.49177264\n",
      "Iteration 946, loss = 0.49167325\n",
      "Iteration 947, loss = 0.49158561\n",
      "Iteration 948, loss = 0.49148943\n",
      "Iteration 949, loss = 0.49140071\n",
      "Iteration 950, loss = 0.49130899\n",
      "Iteration 951, loss = 0.49121795\n",
      "Iteration 952, loss = 0.49112027\n",
      "Iteration 953, loss = 0.49103041\n",
      "Iteration 954, loss = 0.49093692\n",
      "Iteration 955, loss = 0.49085356\n",
      "Iteration 956, loss = 0.49075476\n",
      "Iteration 957, loss = 0.49065687\n",
      "Iteration 958, loss = 0.49055814\n",
      "Iteration 959, loss = 0.49047171\n",
      "Iteration 960, loss = 0.49037769\n",
      "Iteration 961, loss = 0.49028884\n",
      "Iteration 962, loss = 0.49019347\n",
      "Iteration 963, loss = 0.49010604\n",
      "Iteration 964, loss = 0.49002026\n",
      "Iteration 965, loss = 0.48992948\n",
      "Iteration 966, loss = 0.48984318\n",
      "Iteration 967, loss = 0.48975241\n",
      "Iteration 968, loss = 0.48966706\n",
      "Iteration 969, loss = 0.48958062\n",
      "Iteration 970, loss = 0.48949301\n",
      "Iteration 971, loss = 0.48940909\n",
      "Iteration 972, loss = 0.48931945\n",
      "Iteration 973, loss = 0.48923793\n",
      "Iteration 974, loss = 0.48915582\n",
      "Iteration 975, loss = 0.48906683\n",
      "Iteration 976, loss = 0.48897087\n",
      "Iteration 977, loss = 0.48889329\n",
      "Iteration 978, loss = 0.48880185\n",
      "Iteration 979, loss = 0.48871324\n",
      "Iteration 980, loss = 0.48862352\n",
      "Iteration 981, loss = 0.48853457\n",
      "Iteration 982, loss = 0.48844776\n",
      "Iteration 983, loss = 0.48837009\n",
      "Iteration 984, loss = 0.48827574\n",
      "Iteration 985, loss = 0.48819943\n",
      "Iteration 986, loss = 0.48810378\n",
      "Iteration 987, loss = 0.48801870\n",
      "Iteration 988, loss = 0.48793176\n",
      "Iteration 989, loss = 0.48784393\n",
      "Iteration 990, loss = 0.48775636\n",
      "Iteration 991, loss = 0.48767833\n",
      "Iteration 992, loss = 0.48759028\n",
      "Iteration 993, loss = 0.48751130\n",
      "Iteration 994, loss = 0.48743307\n",
      "Iteration 995, loss = 0.48735432\n",
      "Iteration 996, loss = 0.48726674\n",
      "Iteration 997, loss = 0.48718896\n",
      "Iteration 998, loss = 0.48710073\n",
      "Iteration 999, loss = 0.48702008\n",
      "Iteration 1000, loss = 0.48693786\n",
      "Iteration 1001, loss = 0.48685965\n",
      "Iteration 1002, loss = 0.48677352\n",
      "Iteration 1003, loss = 0.48669948\n",
      "Iteration 1004, loss = 0.48661894\n",
      "Iteration 1005, loss = 0.48654851\n",
      "Iteration 1006, loss = 0.48647599\n",
      "Iteration 1007, loss = 0.48639599\n",
      "Iteration 1008, loss = 0.48631566\n",
      "Iteration 1009, loss = 0.48623781\n",
      "Iteration 1010, loss = 0.48616468\n",
      "Iteration 1011, loss = 0.48608621\n",
      "Iteration 1012, loss = 0.48601055\n",
      "Iteration 1013, loss = 0.48592445\n",
      "Iteration 1014, loss = 0.48584421\n",
      "Iteration 1015, loss = 0.48577063\n",
      "Iteration 1016, loss = 0.48568668\n",
      "Iteration 1017, loss = 0.48560461\n",
      "Iteration 1018, loss = 0.48553300\n",
      "Iteration 1019, loss = 0.48545569\n",
      "Iteration 1020, loss = 0.48538047\n",
      "Iteration 1021, loss = 0.48530557\n",
      "Iteration 1022, loss = 0.48522829\n",
      "Iteration 1023, loss = 0.48515677\n",
      "Iteration 1024, loss = 0.48508017\n",
      "Iteration 1025, loss = 0.48499856\n",
      "Iteration 1026, loss = 0.48492608\n",
      "Iteration 1027, loss = 0.48484802\n",
      "Iteration 1028, loss = 0.48477673\n",
      "Iteration 1029, loss = 0.48469571\n",
      "Iteration 1030, loss = 0.48461517\n",
      "Iteration 1031, loss = 0.48453765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1032, loss = 0.48446580\n",
      "Iteration 1033, loss = 0.48438991\n",
      "Iteration 1034, loss = 0.48431742\n",
      "Iteration 1035, loss = 0.48424263\n",
      "Iteration 1036, loss = 0.48416397\n",
      "Iteration 1037, loss = 0.48409320\n",
      "Iteration 1038, loss = 0.48402161\n",
      "Iteration 1039, loss = 0.48394534\n",
      "Iteration 1040, loss = 0.48387089\n",
      "Iteration 1041, loss = 0.48378934\n",
      "Iteration 1042, loss = 0.48372216\n",
      "Iteration 1043, loss = 0.48363665\n",
      "Iteration 1044, loss = 0.48356719\n",
      "Iteration 1045, loss = 0.48349127\n",
      "Iteration 1046, loss = 0.48341829\n",
      "Iteration 1047, loss = 0.48334997\n",
      "Iteration 1048, loss = 0.48326774\n",
      "Iteration 1049, loss = 0.48320210\n",
      "Iteration 1050, loss = 0.48312359\n",
      "Iteration 1051, loss = 0.48305056\n",
      "Iteration 1052, loss = 0.48297788\n",
      "Iteration 1053, loss = 0.48290397\n",
      "Iteration 1054, loss = 0.48282681\n",
      "Iteration 1055, loss = 0.48275075\n",
      "Iteration 1056, loss = 0.48268047\n",
      "Iteration 1057, loss = 0.48260232\n",
      "Iteration 1058, loss = 0.48252208\n",
      "Iteration 1059, loss = 0.48244931\n",
      "Iteration 1060, loss = 0.48237320\n",
      "Iteration 1061, loss = 0.48229809\n",
      "Iteration 1062, loss = 0.48221955\n",
      "Iteration 1063, loss = 0.48214036\n",
      "Iteration 1064, loss = 0.48207430\n",
      "Iteration 1065, loss = 0.48200085\n",
      "Iteration 1066, loss = 0.48192397\n",
      "Iteration 1067, loss = 0.48185402\n",
      "Iteration 1068, loss = 0.48178031\n",
      "Iteration 1069, loss = 0.48170795\n",
      "Iteration 1070, loss = 0.48163275\n",
      "Iteration 1071, loss = 0.48155765\n",
      "Iteration 1072, loss = 0.48148079\n",
      "Iteration 1073, loss = 0.48141485\n",
      "Iteration 1074, loss = 0.48134148\n",
      "Iteration 1075, loss = 0.48127431\n",
      "Iteration 1076, loss = 0.48121042\n",
      "Iteration 1077, loss = 0.48115002\n",
      "Iteration 1078, loss = 0.48107629\n",
      "Iteration 1079, loss = 0.48101604\n",
      "Iteration 1080, loss = 0.48094691\n",
      "Iteration 1081, loss = 0.48088040\n",
      "Iteration 1082, loss = 0.48081101\n",
      "Iteration 1083, loss = 0.48074067\n",
      "Iteration 1084, loss = 0.48067798\n",
      "Iteration 1085, loss = 0.48059984\n",
      "Iteration 1086, loss = 0.48052944\n",
      "Iteration 1087, loss = 0.48046301\n",
      "Iteration 1088, loss = 0.48039030\n",
      "Iteration 1089, loss = 0.48032212\n",
      "Iteration 1090, loss = 0.48025386\n",
      "Iteration 1091, loss = 0.48018323\n",
      "Iteration 1092, loss = 0.48012165\n",
      "Iteration 1093, loss = 0.48005350\n",
      "Iteration 1094, loss = 0.47999716\n",
      "Iteration 1095, loss = 0.47993144\n",
      "Iteration 1096, loss = 0.47986718\n",
      "Iteration 1097, loss = 0.47980092\n",
      "Iteration 1098, loss = 0.47973246\n",
      "Iteration 1099, loss = 0.47966684\n",
      "Iteration 1100, loss = 0.47959863\n",
      "Iteration 1101, loss = 0.47953063\n",
      "Iteration 1102, loss = 0.47945557\n",
      "Iteration 1103, loss = 0.47938879\n",
      "Iteration 1104, loss = 0.47932422\n",
      "Iteration 1105, loss = 0.47924126\n",
      "Iteration 1106, loss = 0.47917748\n",
      "Iteration 1107, loss = 0.47911193\n",
      "Iteration 1108, loss = 0.47904952\n",
      "Iteration 1109, loss = 0.47898447\n",
      "Iteration 1110, loss = 0.47892827\n",
      "Iteration 1111, loss = 0.47886566\n",
      "Iteration 1112, loss = 0.47880254\n",
      "Iteration 1113, loss = 0.47873987\n",
      "Iteration 1114, loss = 0.47868034\n",
      "Iteration 1115, loss = 0.47861873\n",
      "Iteration 1116, loss = 0.47855452\n",
      "Iteration 1117, loss = 0.47849554\n",
      "Iteration 1118, loss = 0.47843240\n",
      "Iteration 1119, loss = 0.47837656\n",
      "Iteration 1120, loss = 0.47831713\n",
      "Iteration 1121, loss = 0.47825311\n",
      "Iteration 1122, loss = 0.47819816\n",
      "Iteration 1123, loss = 0.47813095\n",
      "Iteration 1124, loss = 0.47806935\n",
      "Iteration 1125, loss = 0.47799902\n",
      "Iteration 1126, loss = 0.47793724\n",
      "Iteration 1127, loss = 0.47787433\n",
      "Iteration 1128, loss = 0.47781334\n",
      "Iteration 1129, loss = 0.47774898\n",
      "Iteration 1130, loss = 0.47768325\n",
      "Iteration 1131, loss = 0.47761528\n",
      "Iteration 1132, loss = 0.47754694\n",
      "Iteration 1133, loss = 0.47748767\n",
      "Iteration 1134, loss = 0.47741801\n",
      "Iteration 1135, loss = 0.47736433\n",
      "Iteration 1136, loss = 0.47730349\n",
      "Iteration 1137, loss = 0.47723504\n",
      "Iteration 1138, loss = 0.47717793\n",
      "Iteration 1139, loss = 0.47712063\n",
      "Iteration 1140, loss = 0.47705491\n",
      "Iteration 1141, loss = 0.47699332\n",
      "Iteration 1142, loss = 0.47692958\n",
      "Iteration 1143, loss = 0.47687759\n",
      "Iteration 1144, loss = 0.47681086\n",
      "Iteration 1145, loss = 0.47675019\n",
      "Iteration 1146, loss = 0.47669417\n",
      "Iteration 1147, loss = 0.47662630\n",
      "Iteration 1148, loss = 0.47657120\n",
      "Iteration 1149, loss = 0.47650492\n",
      "Iteration 1150, loss = 0.47643906\n",
      "Iteration 1151, loss = 0.47638126\n",
      "Iteration 1152, loss = 0.47631488\n",
      "Iteration 1153, loss = 0.47625952\n",
      "Iteration 1154, loss = 0.47620246\n",
      "Iteration 1155, loss = 0.47614003\n",
      "Iteration 1156, loss = 0.47608728\n",
      "Iteration 1157, loss = 0.47602589\n",
      "Iteration 1158, loss = 0.47596523\n",
      "Iteration 1159, loss = 0.47590354\n",
      "Iteration 1160, loss = 0.47584769\n",
      "Iteration 1161, loss = 0.47578024\n",
      "Iteration 1162, loss = 0.47572471\n",
      "Iteration 1163, loss = 0.47566218\n",
      "Iteration 1164, loss = 0.47560424\n",
      "Iteration 1165, loss = 0.47554828\n",
      "Iteration 1166, loss = 0.47548889\n",
      "Iteration 1167, loss = 0.47543097\n",
      "Iteration 1168, loss = 0.47536463\n",
      "Iteration 1169, loss = 0.47530893\n",
      "Iteration 1170, loss = 0.47524969\n",
      "Iteration 1171, loss = 0.47518833\n",
      "Iteration 1172, loss = 0.47513549\n",
      "Iteration 1173, loss = 0.47507840\n",
      "Iteration 1174, loss = 0.47502813\n",
      "Iteration 1175, loss = 0.47497018\n",
      "Iteration 1176, loss = 0.47490946\n",
      "Iteration 1177, loss = 0.47484921\n",
      "Iteration 1178, loss = 0.47479539\n",
      "Iteration 1179, loss = 0.47472974\n",
      "Iteration 1180, loss = 0.47467706\n",
      "Iteration 1181, loss = 0.47461020\n",
      "Iteration 1182, loss = 0.47455190\n",
      "Iteration 1183, loss = 0.47448601\n",
      "Iteration 1184, loss = 0.47442718\n",
      "Iteration 1185, loss = 0.47437223\n",
      "Iteration 1186, loss = 0.47431131\n",
      "Iteration 1187, loss = 0.47425865\n",
      "Iteration 1188, loss = 0.47420377\n",
      "Iteration 1189, loss = 0.47415660\n",
      "Iteration 1190, loss = 0.47410060\n",
      "Iteration 1191, loss = 0.47404100\n",
      "Iteration 1192, loss = 0.47399068\n",
      "Iteration 1193, loss = 0.47393459\n",
      "Iteration 1194, loss = 0.47387787\n",
      "Iteration 1195, loss = 0.47382374\n",
      "Iteration 1196, loss = 0.47377011\n",
      "Iteration 1197, loss = 0.47371647\n",
      "Iteration 1198, loss = 0.47366437\n",
      "Iteration 1199, loss = 0.47361535\n",
      "Iteration 1200, loss = 0.47355561\n",
      "Iteration 1201, loss = 0.47350607\n",
      "Iteration 1202, loss = 0.47345393\n",
      "Iteration 1203, loss = 0.47339736\n",
      "Iteration 1204, loss = 0.47333922\n",
      "Iteration 1205, loss = 0.47329136\n",
      "Iteration 1206, loss = 0.47323432\n",
      "Iteration 1207, loss = 0.47318020\n",
      "Iteration 1208, loss = 0.47313183\n",
      "Iteration 1209, loss = 0.47307727\n",
      "Iteration 1210, loss = 0.47302310\n",
      "Iteration 1211, loss = 0.47297336\n",
      "Iteration 1212, loss = 0.47292023\n",
      "Iteration 1213, loss = 0.47285965\n",
      "Iteration 1214, loss = 0.47280447\n",
      "Iteration 1215, loss = 0.47274759\n",
      "Iteration 1216, loss = 0.47269470\n",
      "Iteration 1217, loss = 0.47263723\n",
      "Iteration 1218, loss = 0.47258783\n",
      "Iteration 1219, loss = 0.47253145\n",
      "Iteration 1220, loss = 0.47248048\n",
      "Iteration 1221, loss = 0.47242908\n",
      "Iteration 1222, loss = 0.47238297\n",
      "Iteration 1223, loss = 0.47233030\n",
      "Iteration 1224, loss = 0.47227339\n",
      "Iteration 1225, loss = 0.47222312\n",
      "Iteration 1226, loss = 0.47216983\n",
      "Iteration 1227, loss = 0.47211896\n",
      "Iteration 1228, loss = 0.47207353\n",
      "Iteration 1229, loss = 0.47202535\n",
      "Iteration 1230, loss = 0.47197698\n",
      "Iteration 1231, loss = 0.47192972\n",
      "Iteration 1232, loss = 0.47188520\n",
      "Iteration 1233, loss = 0.47183412\n",
      "Iteration 1234, loss = 0.47179230\n",
      "Iteration 1235, loss = 0.47174331\n",
      "Iteration 1236, loss = 0.47170580\n",
      "Iteration 1237, loss = 0.47165181\n",
      "Iteration 1238, loss = 0.47160312\n",
      "Iteration 1239, loss = 0.47155254\n",
      "Iteration 1240, loss = 0.47150743\n",
      "Iteration 1241, loss = 0.47145274\n",
      "Iteration 1242, loss = 0.47140679\n",
      "Iteration 1243, loss = 0.47135536\n",
      "Iteration 1244, loss = 0.47130574\n",
      "Iteration 1245, loss = 0.47125557\n",
      "Iteration 1246, loss = 0.47120295\n",
      "Iteration 1247, loss = 0.47116098\n",
      "Iteration 1248, loss = 0.47110606\n",
      "Iteration 1249, loss = 0.47105858\n",
      "Iteration 1250, loss = 0.47101150\n",
      "Iteration 1251, loss = 0.47096611\n",
      "Iteration 1252, loss = 0.47091238\n",
      "Iteration 1253, loss = 0.47086791\n",
      "Iteration 1254, loss = 0.47081879\n",
      "Iteration 1255, loss = 0.47077689\n",
      "Iteration 1256, loss = 0.47072374\n",
      "Iteration 1257, loss = 0.47067677\n",
      "Iteration 1258, loss = 0.47062668\n",
      "Iteration 1259, loss = 0.47057510\n",
      "Iteration 1260, loss = 0.47052987\n",
      "Iteration 1261, loss = 0.47048510\n",
      "Iteration 1262, loss = 0.47043753\n",
      "Iteration 1263, loss = 0.47038660\n",
      "Iteration 1264, loss = 0.47034606\n",
      "Iteration 1265, loss = 0.47030088\n",
      "Iteration 1266, loss = 0.47025287\n",
      "Iteration 1267, loss = 0.47020464\n",
      "Iteration 1268, loss = 0.47015486\n",
      "Iteration 1269, loss = 0.47010447\n",
      "Iteration 1270, loss = 0.47005981\n",
      "Iteration 1271, loss = 0.47001310\n",
      "Iteration 1272, loss = 0.46996194\n",
      "Iteration 1273, loss = 0.46991668\n",
      "Iteration 1274, loss = 0.46987200\n",
      "Iteration 1275, loss = 0.46981701\n",
      "Iteration 1276, loss = 0.46977500\n",
      "Iteration 1277, loss = 0.46972729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1278, loss = 0.46968231\n",
      "Iteration 1279, loss = 0.46963916\n",
      "Iteration 1280, loss = 0.46958672\n",
      "Iteration 1281, loss = 0.46954419\n",
      "Iteration 1282, loss = 0.46950016\n",
      "Iteration 1283, loss = 0.46945645\n",
      "Iteration 1284, loss = 0.46940983\n",
      "Iteration 1285, loss = 0.46936624\n",
      "Iteration 1286, loss = 0.46932057\n",
      "Iteration 1287, loss = 0.46927439\n",
      "Iteration 1288, loss = 0.46922923\n",
      "Iteration 1289, loss = 0.46918638\n",
      "Iteration 1290, loss = 0.46914350\n",
      "Iteration 1291, loss = 0.46909553\n",
      "Iteration 1292, loss = 0.46904836\n",
      "Iteration 1293, loss = 0.46900397\n",
      "Iteration 1294, loss = 0.46895509\n",
      "Iteration 1295, loss = 0.46891073\n",
      "Iteration 1296, loss = 0.46886571\n",
      "Iteration 1297, loss = 0.46882226\n",
      "Iteration 1298, loss = 0.46877685\n",
      "Iteration 1299, loss = 0.46872715\n",
      "Iteration 1300, loss = 0.46868495\n",
      "Iteration 1301, loss = 0.46864304\n",
      "Iteration 1302, loss = 0.46859944\n",
      "Iteration 1303, loss = 0.46855795\n",
      "Iteration 1304, loss = 0.46851192\n",
      "Iteration 1305, loss = 0.46847531\n",
      "Iteration 1306, loss = 0.46842144\n",
      "Iteration 1307, loss = 0.46837567\n",
      "Iteration 1308, loss = 0.46832753\n",
      "Iteration 1309, loss = 0.46828204\n",
      "Iteration 1310, loss = 0.46823188\n",
      "Iteration 1311, loss = 0.46818378\n",
      "Iteration 1312, loss = 0.46813771\n",
      "Iteration 1313, loss = 0.46808514\n",
      "Iteration 1314, loss = 0.46803485\n",
      "Iteration 1315, loss = 0.46798726\n",
      "Iteration 1316, loss = 0.46793778\n",
      "Iteration 1317, loss = 0.46789649\n",
      "Iteration 1318, loss = 0.46784901\n",
      "Iteration 1319, loss = 0.46780451\n",
      "Iteration 1320, loss = 0.46776343\n",
      "Iteration 1321, loss = 0.46772142\n",
      "Iteration 1322, loss = 0.46767423\n",
      "Iteration 1323, loss = 0.46762748\n",
      "Iteration 1324, loss = 0.46758457\n",
      "Iteration 1325, loss = 0.46753886\n",
      "Iteration 1326, loss = 0.46749769\n",
      "Iteration 1327, loss = 0.46745051\n",
      "Iteration 1328, loss = 0.46740955\n",
      "Iteration 1329, loss = 0.46736346\n",
      "Iteration 1330, loss = 0.46732420\n",
      "Iteration 1331, loss = 0.46727891\n",
      "Iteration 1332, loss = 0.46723193\n",
      "Iteration 1333, loss = 0.46719368\n",
      "Iteration 1334, loss = 0.46714627\n",
      "Iteration 1335, loss = 0.46710330\n",
      "Iteration 1336, loss = 0.46706000\n",
      "Iteration 1337, loss = 0.46701819\n",
      "Iteration 1338, loss = 0.46698027\n",
      "Iteration 1339, loss = 0.46693353\n",
      "Iteration 1340, loss = 0.46689963\n",
      "Iteration 1341, loss = 0.46685511\n",
      "Iteration 1342, loss = 0.46681282\n",
      "Iteration 1343, loss = 0.46677152\n",
      "Iteration 1344, loss = 0.46672790\n",
      "Iteration 1345, loss = 0.46668712\n",
      "Iteration 1346, loss = 0.46664528\n",
      "Iteration 1347, loss = 0.46660636\n",
      "Iteration 1348, loss = 0.46656626\n",
      "Iteration 1349, loss = 0.46652949\n",
      "Iteration 1350, loss = 0.46649623\n",
      "Iteration 1351, loss = 0.46645270\n",
      "Iteration 1352, loss = 0.46641821\n",
      "Iteration 1353, loss = 0.46637832\n",
      "Iteration 1354, loss = 0.46633269\n",
      "Iteration 1355, loss = 0.46629029\n",
      "Iteration 1356, loss = 0.46625088\n",
      "Iteration 1357, loss = 0.46621017\n",
      "Iteration 1358, loss = 0.46617539\n",
      "Iteration 1359, loss = 0.46613479\n",
      "Iteration 1360, loss = 0.46609605\n",
      "Iteration 1361, loss = 0.46606106\n",
      "Iteration 1362, loss = 0.46601924\n",
      "Iteration 1363, loss = 0.46598100\n",
      "Iteration 1364, loss = 0.46593777\n",
      "Iteration 1365, loss = 0.46589597\n",
      "Iteration 1366, loss = 0.46585702\n",
      "Iteration 1367, loss = 0.46581915\n",
      "Iteration 1368, loss = 0.46577141\n",
      "Iteration 1369, loss = 0.46573437\n",
      "Iteration 1370, loss = 0.46569202\n",
      "Iteration 1371, loss = 0.46565515\n",
      "Iteration 1372, loss = 0.46561328\n",
      "Iteration 1373, loss = 0.46557113\n",
      "Iteration 1374, loss = 0.46553269\n",
      "Iteration 1375, loss = 0.46549327\n",
      "Iteration 1376, loss = 0.46545085\n",
      "Iteration 1377, loss = 0.46541573\n",
      "Iteration 1378, loss = 0.46537465\n",
      "Iteration 1379, loss = 0.46533260\n",
      "Iteration 1380, loss = 0.46529679\n",
      "Iteration 1381, loss = 0.46525624\n",
      "Iteration 1382, loss = 0.46521684\n",
      "Iteration 1383, loss = 0.46518055\n",
      "Iteration 1384, loss = 0.46513760\n",
      "Iteration 1385, loss = 0.46510198\n",
      "Iteration 1386, loss = 0.46506106\n",
      "Iteration 1387, loss = 0.46502299\n",
      "Iteration 1388, loss = 0.46498200\n",
      "Iteration 1389, loss = 0.46494375\n",
      "Iteration 1390, loss = 0.46490442\n",
      "Iteration 1391, loss = 0.46485860\n",
      "Iteration 1392, loss = 0.46482246\n",
      "Iteration 1393, loss = 0.46478443\n",
      "Iteration 1394, loss = 0.46474545\n",
      "Iteration 1395, loss = 0.46470652\n",
      "Iteration 1396, loss = 0.46466830\n",
      "Iteration 1397, loss = 0.46463662\n",
      "Iteration 1398, loss = 0.46459855\n",
      "Iteration 1399, loss = 0.46456442\n",
      "Iteration 1400, loss = 0.46452608\n",
      "Iteration 1401, loss = 0.46448677\n",
      "Iteration 1402, loss = 0.46445666\n",
      "Iteration 1403, loss = 0.46441870\n",
      "Iteration 1404, loss = 0.46438288\n",
      "Iteration 1405, loss = 0.46434602\n",
      "Iteration 1406, loss = 0.46431588\n",
      "Iteration 1407, loss = 0.46428156\n",
      "Iteration 1408, loss = 0.46424417\n",
      "Iteration 1409, loss = 0.46420438\n",
      "Iteration 1410, loss = 0.46416786\n",
      "Iteration 1411, loss = 0.46412953\n",
      "Iteration 1412, loss = 0.46409568\n",
      "Iteration 1413, loss = 0.46406103\n",
      "Iteration 1414, loss = 0.46402847\n",
      "Iteration 1415, loss = 0.46399724\n",
      "Iteration 1416, loss = 0.46395951\n",
      "Iteration 1417, loss = 0.46392480\n",
      "Iteration 1418, loss = 0.46388466\n",
      "Iteration 1419, loss = 0.46384552\n",
      "Iteration 1420, loss = 0.46380503\n",
      "Iteration 1421, loss = 0.46376801\n",
      "Iteration 1422, loss = 0.46373503\n",
      "Iteration 1423, loss = 0.46369550\n",
      "Iteration 1424, loss = 0.46366668\n",
      "Iteration 1425, loss = 0.46362981\n",
      "Iteration 1426, loss = 0.46359633\n",
      "Iteration 1427, loss = 0.46356108\n",
      "Iteration 1428, loss = 0.46352639\n",
      "Iteration 1429, loss = 0.46349528\n",
      "Iteration 1430, loss = 0.46346372\n",
      "Iteration 1431, loss = 0.46342623\n",
      "Iteration 1432, loss = 0.46339417\n",
      "Iteration 1433, loss = 0.46336369\n",
      "Iteration 1434, loss = 0.46332396\n",
      "Iteration 1435, loss = 0.46328773\n",
      "Iteration 1436, loss = 0.46325261\n",
      "Iteration 1437, loss = 0.46321592\n",
      "Iteration 1438, loss = 0.46318159\n",
      "Iteration 1439, loss = 0.46314895\n",
      "Iteration 1440, loss = 0.46311066\n",
      "Iteration 1441, loss = 0.46307575\n",
      "Iteration 1442, loss = 0.46303682\n",
      "Iteration 1443, loss = 0.46300099\n",
      "Iteration 1444, loss = 0.46296522\n",
      "Iteration 1445, loss = 0.46292890\n",
      "Iteration 1446, loss = 0.46289655\n",
      "Iteration 1447, loss = 0.46286102\n",
      "Iteration 1448, loss = 0.46282042\n",
      "Iteration 1449, loss = 0.46278625\n",
      "Iteration 1450, loss = 0.46274910\n",
      "Iteration 1451, loss = 0.46271595\n",
      "Iteration 1452, loss = 0.46268228\n",
      "Iteration 1453, loss = 0.46264516\n",
      "Iteration 1454, loss = 0.46261225\n",
      "Iteration 1455, loss = 0.46257379\n",
      "Iteration 1456, loss = 0.46254429\n",
      "Iteration 1457, loss = 0.46250969\n",
      "Iteration 1458, loss = 0.46247479\n",
      "Iteration 1459, loss = 0.46244274\n",
      "Iteration 1460, loss = 0.46241109\n",
      "Iteration 1461, loss = 0.46237739\n",
      "Iteration 1462, loss = 0.46234344\n",
      "Iteration 1463, loss = 0.46231128\n",
      "Iteration 1464, loss = 0.46227978\n",
      "Iteration 1465, loss = 0.46224779\n",
      "Iteration 1466, loss = 0.46221451\n",
      "Iteration 1467, loss = 0.46218471\n",
      "Iteration 1468, loss = 0.46215237\n",
      "Iteration 1469, loss = 0.46211896\n",
      "Iteration 1470, loss = 0.46208939\n",
      "Iteration 1471, loss = 0.46205578\n",
      "Iteration 1472, loss = 0.46202390\n",
      "Iteration 1473, loss = 0.46199387\n",
      "Iteration 1474, loss = 0.46196217\n",
      "Iteration 1475, loss = 0.46192878\n",
      "Iteration 1476, loss = 0.46189704\n",
      "Iteration 1477, loss = 0.46186399\n",
      "Iteration 1478, loss = 0.46182926\n",
      "Iteration 1479, loss = 0.46179945\n",
      "Iteration 1480, loss = 0.46176577\n",
      "Iteration 1481, loss = 0.46172891\n",
      "Iteration 1482, loss = 0.46169769\n",
      "Iteration 1483, loss = 0.46166666\n",
      "Iteration 1484, loss = 0.46163420\n",
      "Iteration 1485, loss = 0.46159853\n",
      "Iteration 1486, loss = 0.46156268\n",
      "Iteration 1487, loss = 0.46152820\n",
      "Iteration 1488, loss = 0.46149340\n",
      "Iteration 1489, loss = 0.46145927\n",
      "Iteration 1490, loss = 0.46143004\n",
      "Iteration 1491, loss = 0.46139711\n",
      "Iteration 1492, loss = 0.46136357\n",
      "Iteration 1493, loss = 0.46133477\n",
      "Iteration 1494, loss = 0.46130375\n",
      "Iteration 1495, loss = 0.46126873\n",
      "Iteration 1496, loss = 0.46123420\n",
      "Iteration 1497, loss = 0.46120164\n",
      "Iteration 1498, loss = 0.46116862\n",
      "Iteration 1499, loss = 0.46113639\n",
      "Iteration 1500, loss = 0.46110259\n",
      "Iteration 1501, loss = 0.46107108\n",
      "Iteration 1502, loss = 0.46104484\n",
      "Iteration 1503, loss = 0.46101961\n",
      "Iteration 1504, loss = 0.46098964\n",
      "Iteration 1505, loss = 0.46096605\n",
      "Iteration 1506, loss = 0.46093555\n",
      "Iteration 1507, loss = 0.46090487\n",
      "Iteration 1508, loss = 0.46087793\n",
      "Iteration 1509, loss = 0.46083851\n",
      "Iteration 1510, loss = 0.46080552\n",
      "Iteration 1511, loss = 0.46077407\n",
      "Iteration 1512, loss = 0.46074313\n",
      "Iteration 1513, loss = 0.46071396\n",
      "Iteration 1514, loss = 0.46068459\n",
      "Iteration 1515, loss = 0.46065291\n",
      "Iteration 1516, loss = 0.46062088\n",
      "Iteration 1517, loss = 0.46059342\n",
      "Iteration 1518, loss = 0.46055925\n",
      "Iteration 1519, loss = 0.46053206\n",
      "Iteration 1520, loss = 0.46050061\n",
      "Iteration 1521, loss = 0.46047396\n",
      "Iteration 1522, loss = 0.46044256\n",
      "Iteration 1523, loss = 0.46041336\n",
      "Iteration 1524, loss = 0.46038458\n",
      "Iteration 1525, loss = 0.46035588\n",
      "Iteration 1526, loss = 0.46032663\n",
      "Iteration 1527, loss = 0.46029625\n",
      "Iteration 1528, loss = 0.46027004\n",
      "Iteration 1529, loss = 0.46023825\n",
      "Iteration 1530, loss = 0.46020535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1531, loss = 0.46017793\n",
      "Iteration 1532, loss = 0.46014743\n",
      "Iteration 1533, loss = 0.46011788\n",
      "Iteration 1534, loss = 0.46009121\n",
      "Iteration 1535, loss = 0.46005834\n",
      "Iteration 1536, loss = 0.46002808\n",
      "Iteration 1537, loss = 0.46000522\n",
      "Iteration 1538, loss = 0.45997242\n",
      "Iteration 1539, loss = 0.45995105\n",
      "Iteration 1540, loss = 0.45992165\n",
      "Iteration 1541, loss = 0.45989319\n",
      "Iteration 1542, loss = 0.45986716\n",
      "Iteration 1543, loss = 0.45983665\n",
      "Iteration 1544, loss = 0.45980910\n",
      "Iteration 1545, loss = 0.45977586\n",
      "Iteration 1546, loss = 0.45974916\n",
      "Iteration 1547, loss = 0.45971852\n",
      "Iteration 1548, loss = 0.45969054\n",
      "Iteration 1549, loss = 0.45966414\n",
      "Iteration 1550, loss = 0.45963793\n",
      "Iteration 1551, loss = 0.45961065\n",
      "Iteration 1552, loss = 0.45958589\n",
      "Iteration 1553, loss = 0.45955735\n",
      "Iteration 1554, loss = 0.45952904\n",
      "Iteration 1555, loss = 0.45950221\n",
      "Iteration 1556, loss = 0.45947676\n",
      "Iteration 1557, loss = 0.45944953\n",
      "Iteration 1558, loss = 0.45942182\n",
      "Iteration 1559, loss = 0.45939861\n",
      "Iteration 1560, loss = 0.45937011\n",
      "Iteration 1561, loss = 0.45934215\n",
      "Iteration 1562, loss = 0.45931426\n",
      "Iteration 1563, loss = 0.45928911\n",
      "Iteration 1564, loss = 0.45925912\n",
      "Iteration 1565, loss = 0.45923149\n",
      "Iteration 1566, loss = 0.45920525\n",
      "Iteration 1567, loss = 0.45917677\n",
      "Iteration 1568, loss = 0.45914608\n",
      "Iteration 1569, loss = 0.45912059\n",
      "Iteration 1570, loss = 0.45908758\n",
      "Iteration 1571, loss = 0.45906443\n",
      "Iteration 1572, loss = 0.45903378\n",
      "Iteration 1573, loss = 0.45900677\n",
      "Iteration 1574, loss = 0.45898191\n",
      "Iteration 1575, loss = 0.45895912\n",
      "Iteration 1576, loss = 0.45892755\n",
      "Iteration 1577, loss = 0.45890084\n",
      "Iteration 1578, loss = 0.45887522\n",
      "Iteration 1579, loss = 0.45885170\n",
      "Iteration 1580, loss = 0.45882878\n",
      "Iteration 1581, loss = 0.45880086\n",
      "Iteration 1582, loss = 0.45877608\n",
      "Iteration 1583, loss = 0.45874865\n",
      "Iteration 1584, loss = 0.45872084\n",
      "Iteration 1585, loss = 0.45869822\n",
      "Iteration 1586, loss = 0.45867182\n",
      "Iteration 1587, loss = 0.45864596\n",
      "Iteration 1588, loss = 0.45862100\n",
      "Iteration 1589, loss = 0.45858934\n",
      "Iteration 1590, loss = 0.45856798\n",
      "Iteration 1591, loss = 0.45854237\n",
      "Iteration 1592, loss = 0.45851725\n",
      "Iteration 1593, loss = 0.45849085\n",
      "Iteration 1594, loss = 0.45846371\n",
      "Iteration 1595, loss = 0.45843633\n",
      "Iteration 1596, loss = 0.45840304\n",
      "Iteration 1597, loss = 0.45837461\n",
      "Iteration 1598, loss = 0.45834311\n",
      "Iteration 1599, loss = 0.45831734\n",
      "Iteration 1600, loss = 0.45828838\n",
      "Iteration 1601, loss = 0.45825542\n",
      "Iteration 1602, loss = 0.45823037\n",
      "Iteration 1603, loss = 0.45819956\n",
      "Iteration 1604, loss = 0.45817398\n",
      "Iteration 1605, loss = 0.45814615\n",
      "Iteration 1606, loss = 0.45811846\n",
      "Iteration 1607, loss = 0.45809002\n",
      "Iteration 1608, loss = 0.45806574\n",
      "Iteration 1609, loss = 0.45803775\n",
      "Iteration 1610, loss = 0.45801023\n",
      "Iteration 1611, loss = 0.45798258\n",
      "Iteration 1612, loss = 0.45795727\n",
      "Iteration 1613, loss = 0.45793481\n",
      "Iteration 1614, loss = 0.45790917\n",
      "Iteration 1615, loss = 0.45788205\n",
      "Iteration 1616, loss = 0.45785867\n",
      "Iteration 1617, loss = 0.45783394\n",
      "Iteration 1618, loss = 0.45781177\n",
      "Iteration 1619, loss = 0.45778285\n",
      "Iteration 1620, loss = 0.45776007\n",
      "Iteration 1621, loss = 0.45773423\n",
      "Iteration 1622, loss = 0.45770659\n",
      "Iteration 1623, loss = 0.45768011\n",
      "Iteration 1624, loss = 0.45765346\n",
      "Iteration 1625, loss = 0.45763082\n",
      "Iteration 1626, loss = 0.45760101\n",
      "Iteration 1627, loss = 0.45757678\n",
      "Iteration 1628, loss = 0.45754918\n",
      "Iteration 1629, loss = 0.45752332\n",
      "Iteration 1630, loss = 0.45749886\n",
      "Iteration 1631, loss = 0.45747701\n",
      "Iteration 1632, loss = 0.45745197\n",
      "Iteration 1633, loss = 0.45742540\n",
      "Iteration 1634, loss = 0.45740333\n",
      "Iteration 1635, loss = 0.45737955\n",
      "Iteration 1636, loss = 0.45735638\n",
      "Iteration 1637, loss = 0.45733480\n",
      "Iteration 1638, loss = 0.45730841\n",
      "Iteration 1639, loss = 0.45728271\n",
      "Iteration 1640, loss = 0.45726079\n",
      "Iteration 1641, loss = 0.45723223\n",
      "Iteration 1642, loss = 0.45720545\n",
      "Iteration 1643, loss = 0.45717771\n",
      "Iteration 1644, loss = 0.45715746\n",
      "Iteration 1645, loss = 0.45713060\n",
      "Iteration 1646, loss = 0.45710822\n",
      "Iteration 1647, loss = 0.45708094\n",
      "Iteration 1648, loss = 0.45705454\n",
      "Iteration 1649, loss = 0.45703198\n",
      "Iteration 1650, loss = 0.45700552\n",
      "Iteration 1651, loss = 0.45698477\n",
      "Iteration 1652, loss = 0.45695681\n",
      "Iteration 1653, loss = 0.45693652\n",
      "Iteration 1654, loss = 0.45691161\n",
      "Iteration 1655, loss = 0.45688418\n",
      "Iteration 1656, loss = 0.45685815\n",
      "Iteration 1657, loss = 0.45683393\n",
      "Iteration 1658, loss = 0.45680663\n",
      "Iteration 1659, loss = 0.45678133\n",
      "Iteration 1660, loss = 0.45676037\n",
      "Iteration 1661, loss = 0.45674145\n",
      "Iteration 1662, loss = 0.45671735\n",
      "Iteration 1663, loss = 0.45669776\n",
      "Iteration 1664, loss = 0.45667278\n",
      "Iteration 1665, loss = 0.45665223\n",
      "Iteration 1666, loss = 0.45662893\n",
      "Iteration 1667, loss = 0.45660749\n",
      "Iteration 1668, loss = 0.45658936\n",
      "Iteration 1669, loss = 0.45656248\n",
      "Iteration 1670, loss = 0.45654058\n",
      "Iteration 1671, loss = 0.45651481\n",
      "Iteration 1672, loss = 0.45648981\n",
      "Iteration 1673, loss = 0.45646706\n",
      "Iteration 1674, loss = 0.45643874\n",
      "Iteration 1675, loss = 0.45641623\n",
      "Iteration 1676, loss = 0.45639274\n",
      "Iteration 1677, loss = 0.45636659\n",
      "Iteration 1678, loss = 0.45634638\n",
      "Iteration 1679, loss = 0.45632480\n",
      "Iteration 1680, loss = 0.45630352\n",
      "Iteration 1681, loss = 0.45627847\n",
      "Iteration 1682, loss = 0.45625323\n",
      "Iteration 1683, loss = 0.45623312\n",
      "Iteration 1684, loss = 0.45620802\n",
      "Iteration 1685, loss = 0.45618180\n",
      "Iteration 1686, loss = 0.45616083\n",
      "Iteration 1687, loss = 0.45614176\n",
      "Iteration 1688, loss = 0.45611698\n",
      "Iteration 1689, loss = 0.45609535\n",
      "Iteration 1690, loss = 0.45607555\n",
      "Iteration 1691, loss = 0.45605016\n",
      "Iteration 1692, loss = 0.45602675\n",
      "Iteration 1693, loss = 0.45600155\n",
      "Iteration 1694, loss = 0.45598138\n",
      "Iteration 1695, loss = 0.45595793\n",
      "Iteration 1696, loss = 0.45593706\n",
      "Iteration 1697, loss = 0.45591302\n",
      "Iteration 1698, loss = 0.45589653\n",
      "Iteration 1699, loss = 0.45587506\n",
      "Iteration 1700, loss = 0.45585797\n",
      "Iteration 1701, loss = 0.45583505\n",
      "Iteration 1702, loss = 0.45581540\n",
      "Iteration 1703, loss = 0.45579100\n",
      "Iteration 1704, loss = 0.45577067\n",
      "Iteration 1705, loss = 0.45575289\n",
      "Iteration 1706, loss = 0.45572736\n",
      "Iteration 1707, loss = 0.45570483\n",
      "Iteration 1708, loss = 0.45567959\n",
      "Iteration 1709, loss = 0.45565639\n",
      "Iteration 1710, loss = 0.45563074\n",
      "Iteration 1711, loss = 0.45561028\n",
      "Iteration 1712, loss = 0.45558790\n",
      "Iteration 1713, loss = 0.45556537\n",
      "Iteration 1714, loss = 0.45554242\n",
      "Iteration 1715, loss = 0.45551835\n",
      "Iteration 1716, loss = 0.45549833\n",
      "Iteration 1717, loss = 0.45547690\n",
      "Iteration 1718, loss = 0.45545751\n",
      "Iteration 1719, loss = 0.45543668\n",
      "Iteration 1720, loss = 0.45541599\n",
      "Iteration 1721, loss = 0.45539364\n",
      "Iteration 1722, loss = 0.45537029\n",
      "Iteration 1723, loss = 0.45534941\n",
      "Iteration 1724, loss = 0.45532553\n",
      "Iteration 1725, loss = 0.45530557\n",
      "Iteration 1726, loss = 0.45528460\n",
      "Iteration 1727, loss = 0.45526069\n",
      "Iteration 1728, loss = 0.45524177\n",
      "Iteration 1729, loss = 0.45521679\n",
      "Iteration 1730, loss = 0.45519865\n",
      "Iteration 1731, loss = 0.45517291\n",
      "Iteration 1732, loss = 0.45515188\n",
      "Iteration 1733, loss = 0.45512946\n",
      "Iteration 1734, loss = 0.45510905\n",
      "Iteration 1735, loss = 0.45508542\n",
      "Iteration 1736, loss = 0.45506727\n",
      "Iteration 1737, loss = 0.45504481\n",
      "Iteration 1738, loss = 0.45502597\n",
      "Iteration 1739, loss = 0.45500878\n",
      "Iteration 1740, loss = 0.45498707\n",
      "Iteration 1741, loss = 0.45496430\n",
      "Iteration 1742, loss = 0.45494588\n",
      "Iteration 1743, loss = 0.45492549\n",
      "Iteration 1744, loss = 0.45490336\n",
      "Iteration 1745, loss = 0.45488286\n",
      "Iteration 1746, loss = 0.45486260\n",
      "Iteration 1747, loss = 0.45484444\n",
      "Iteration 1748, loss = 0.45482305\n",
      "Iteration 1749, loss = 0.45480304\n",
      "Iteration 1750, loss = 0.45478386\n",
      "Iteration 1751, loss = 0.45476376\n",
      "Iteration 1752, loss = 0.45474334\n",
      "Iteration 1753, loss = 0.45471959\n",
      "Iteration 1754, loss = 0.45470263\n",
      "Iteration 1755, loss = 0.45467875\n",
      "Iteration 1756, loss = 0.45465808\n",
      "Iteration 1757, loss = 0.45463806\n",
      "Iteration 1758, loss = 0.45461953\n",
      "Iteration 1759, loss = 0.45459909\n",
      "Iteration 1760, loss = 0.45458060\n",
      "Iteration 1761, loss = 0.45455966\n",
      "Iteration 1762, loss = 0.45453812\n",
      "Iteration 1763, loss = 0.45451973\n",
      "Iteration 1764, loss = 0.45449769\n",
      "Iteration 1765, loss = 0.45448088\n",
      "Iteration 1766, loss = 0.45445817\n",
      "Iteration 1767, loss = 0.45443938\n",
      "Iteration 1768, loss = 0.45442093\n",
      "Iteration 1769, loss = 0.45440180\n",
      "Iteration 1770, loss = 0.45437985\n",
      "Iteration 1771, loss = 0.45436024\n",
      "Iteration 1772, loss = 0.45434072\n",
      "Iteration 1773, loss = 0.45432261\n",
      "Iteration 1774, loss = 0.45430151\n",
      "Iteration 1775, loss = 0.45428056\n",
      "Iteration 1776, loss = 0.45426309\n",
      "Iteration 1777, loss = 0.45424065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1778, loss = 0.45422354\n",
      "Iteration 1779, loss = 0.45420140\n",
      "Iteration 1780, loss = 0.45418599\n",
      "Iteration 1781, loss = 0.45416284\n",
      "Iteration 1782, loss = 0.45414718\n",
      "Iteration 1783, loss = 0.45412502\n",
      "Iteration 1784, loss = 0.45410936\n",
      "Iteration 1785, loss = 0.45409042\n",
      "Iteration 1786, loss = 0.45407381\n",
      "Iteration 1787, loss = 0.45405427\n",
      "Iteration 1788, loss = 0.45403694\n",
      "Iteration 1789, loss = 0.45402173\n",
      "Iteration 1790, loss = 0.45400363\n",
      "Iteration 1791, loss = 0.45398851\n",
      "Iteration 1792, loss = 0.45396925\n",
      "Iteration 1793, loss = 0.45395250\n",
      "Iteration 1794, loss = 0.45393318\n",
      "Iteration 1795, loss = 0.45391751\n",
      "Iteration 1796, loss = 0.45389961\n",
      "Iteration 1797, loss = 0.45387880\n",
      "Iteration 1798, loss = 0.45385997\n",
      "Iteration 1799, loss = 0.45384192\n",
      "Iteration 1800, loss = 0.45382170\n",
      "Iteration 1801, loss = 0.45380351\n",
      "Iteration 1802, loss = 0.45378887\n",
      "Iteration 1803, loss = 0.45377309\n",
      "Iteration 1804, loss = 0.45375480\n",
      "Iteration 1805, loss = 0.45373806\n",
      "Iteration 1806, loss = 0.45372072\n",
      "Iteration 1807, loss = 0.45370433\n",
      "Iteration 1808, loss = 0.45368450\n",
      "Iteration 1809, loss = 0.45366430\n",
      "Iteration 1810, loss = 0.45364756\n",
      "Iteration 1811, loss = 0.45362847\n",
      "Iteration 1812, loss = 0.45361122\n",
      "Iteration 1813, loss = 0.45358897\n",
      "Iteration 1814, loss = 0.45357323\n",
      "Iteration 1815, loss = 0.45355292\n",
      "Iteration 1816, loss = 0.45353473\n",
      "Iteration 1817, loss = 0.45352073\n",
      "Iteration 1818, loss = 0.45350102\n",
      "Iteration 1819, loss = 0.45348137\n",
      "Iteration 1820, loss = 0.45346007\n",
      "Iteration 1821, loss = 0.45344249\n",
      "Iteration 1822, loss = 0.45341992\n",
      "Iteration 1823, loss = 0.45340173\n",
      "Iteration 1824, loss = 0.45338065\n",
      "Iteration 1825, loss = 0.45336544\n",
      "Iteration 1826, loss = 0.45334868\n",
      "Iteration 1827, loss = 0.45333157\n",
      "Iteration 1828, loss = 0.45331529\n",
      "Iteration 1829, loss = 0.45329920\n",
      "Iteration 1830, loss = 0.45328069\n",
      "Iteration 1831, loss = 0.45326455\n",
      "Iteration 1832, loss = 0.45324569\n",
      "Iteration 1833, loss = 0.45323042\n",
      "Iteration 1834, loss = 0.45321111\n",
      "Iteration 1835, loss = 0.45319366\n",
      "Iteration 1836, loss = 0.45317528\n",
      "Iteration 1837, loss = 0.45315985\n",
      "Iteration 1838, loss = 0.45314001\n",
      "Iteration 1839, loss = 0.45312441\n",
      "Iteration 1840, loss = 0.45310675\n",
      "Iteration 1841, loss = 0.45309013\n",
      "Iteration 1842, loss = 0.45307070\n",
      "Iteration 1843, loss = 0.45305144\n",
      "Iteration 1844, loss = 0.45303618\n",
      "Iteration 1845, loss = 0.45301446\n",
      "Iteration 1846, loss = 0.45299920\n",
      "Iteration 1847, loss = 0.45298152\n",
      "Iteration 1848, loss = 0.45296048\n",
      "Iteration 1849, loss = 0.45294447\n",
      "Iteration 1850, loss = 0.45292710\n",
      "Iteration 1851, loss = 0.45290558\n",
      "Iteration 1852, loss = 0.45289393\n",
      "Iteration 1853, loss = 0.45286860\n",
      "Iteration 1854, loss = 0.45285407\n",
      "Iteration 1855, loss = 0.45283726\n",
      "Iteration 1856, loss = 0.45281704\n",
      "Iteration 1857, loss = 0.45280503\n",
      "Iteration 1858, loss = 0.45278740\n",
      "Iteration 1859, loss = 0.45276737\n",
      "Iteration 1860, loss = 0.45275504\n",
      "Iteration 1861, loss = 0.45273982\n",
      "Iteration 1862, loss = 0.45272628\n",
      "Iteration 1863, loss = 0.45270773\n",
      "Iteration 1864, loss = 0.45268928\n",
      "Iteration 1865, loss = 0.45267645\n",
      "Iteration 1866, loss = 0.45265802\n",
      "Iteration 1867, loss = 0.45263971\n",
      "Iteration 1868, loss = 0.45262262\n",
      "Iteration 1869, loss = 0.45260634\n",
      "Iteration 1870, loss = 0.45259276\n",
      "Iteration 1871, loss = 0.45257632\n",
      "Iteration 1872, loss = 0.45256359\n",
      "Iteration 1873, loss = 0.45254415\n",
      "Iteration 1874, loss = 0.45253185\n",
      "Iteration 1875, loss = 0.45251470\n",
      "Iteration 1876, loss = 0.45250067\n",
      "Iteration 1877, loss = 0.45248591\n",
      "Iteration 1878, loss = 0.45247297\n",
      "Iteration 1879, loss = 0.45245708\n",
      "Iteration 1880, loss = 0.45244322\n",
      "Iteration 1881, loss = 0.45242451\n",
      "Iteration 1882, loss = 0.45240671\n",
      "Iteration 1883, loss = 0.45238754\n",
      "Iteration 1884, loss = 0.45237046\n",
      "Iteration 1885, loss = 0.45235228\n",
      "Iteration 1886, loss = 0.45233482\n",
      "Iteration 1887, loss = 0.45232356\n",
      "Iteration 1888, loss = 0.45230638\n",
      "Iteration 1889, loss = 0.45228946\n",
      "Iteration 1890, loss = 0.45227512\n",
      "Iteration 1891, loss = 0.45226098\n",
      "Iteration 1892, loss = 0.45224413\n",
      "Iteration 1893, loss = 0.45223227\n",
      "Iteration 1894, loss = 0.45221675\n",
      "Iteration 1895, loss = 0.45220094\n",
      "Iteration 1896, loss = 0.45218560\n",
      "Iteration 1897, loss = 0.45217422\n",
      "Iteration 1898, loss = 0.45216196\n",
      "Iteration 1899, loss = 0.45214932\n",
      "Iteration 1900, loss = 0.45213829\n",
      "Iteration 1901, loss = 0.45212540\n",
      "Iteration 1902, loss = 0.45211393\n",
      "Iteration 1903, loss = 0.45210299\n",
      "Iteration 1904, loss = 0.45208902\n",
      "Iteration 1905, loss = 0.45207570\n",
      "Iteration 1906, loss = 0.45206166\n",
      "Iteration 1907, loss = 0.45204415\n",
      "Iteration 1908, loss = 0.45202888\n",
      "Iteration 1909, loss = 0.45201453\n",
      "Iteration 1910, loss = 0.45199959\n",
      "Iteration 1911, loss = 0.45198343\n",
      "Iteration 1912, loss = 0.45197096\n",
      "Iteration 1913, loss = 0.45195910\n",
      "Iteration 1914, loss = 0.45194659\n",
      "Iteration 1915, loss = 0.45193242\n",
      "Iteration 1916, loss = 0.45191985\n",
      "Iteration 1917, loss = 0.45190598\n",
      "Iteration 1918, loss = 0.45189467\n",
      "Iteration 1919, loss = 0.45187827\n",
      "Iteration 1920, loss = 0.45186681\n",
      "Iteration 1921, loss = 0.45185039\n",
      "Iteration 1922, loss = 0.45183275\n",
      "Iteration 1923, loss = 0.45181722\n",
      "Iteration 1924, loss = 0.45180313\n",
      "Iteration 1925, loss = 0.45178825\n",
      "Iteration 1926, loss = 0.45177567\n",
      "Iteration 1927, loss = 0.45176654\n",
      "Iteration 1928, loss = 0.45175261\n",
      "Iteration 1929, loss = 0.45174163\n",
      "Iteration 1930, loss = 0.45172903\n",
      "Iteration 1931, loss = 0.45171549\n",
      "Iteration 1932, loss = 0.45170261\n",
      "Iteration 1933, loss = 0.45168921\n",
      "Iteration 1934, loss = 0.45167476\n",
      "Iteration 1935, loss = 0.45166177\n",
      "Iteration 1936, loss = 0.45165261\n",
      "Iteration 1937, loss = 0.45163458\n",
      "Iteration 1938, loss = 0.45162252\n",
      "Iteration 1939, loss = 0.45160803\n",
      "Iteration 1940, loss = 0.45159392\n",
      "Iteration 1941, loss = 0.45158023\n",
      "Iteration 1942, loss = 0.45156732\n",
      "Iteration 1943, loss = 0.45155371\n",
      "Iteration 1944, loss = 0.45154185\n",
      "Iteration 1945, loss = 0.45152812\n",
      "Iteration 1946, loss = 0.45151264\n",
      "Iteration 1947, loss = 0.45150155\n",
      "Iteration 1948, loss = 0.45148756\n",
      "Iteration 1949, loss = 0.45147353\n",
      "Iteration 1950, loss = 0.45145990\n",
      "Iteration 1951, loss = 0.45144919\n",
      "Iteration 1952, loss = 0.45143280\n",
      "Iteration 1953, loss = 0.45141939\n",
      "Iteration 1954, loss = 0.45140667\n",
      "Iteration 1955, loss = 0.45139148\n",
      "Iteration 1956, loss = 0.45137831\n",
      "Iteration 1957, loss = 0.45136357\n",
      "Iteration 1958, loss = 0.45135024\n",
      "Iteration 1959, loss = 0.45133340\n",
      "Iteration 1960, loss = 0.45132125\n",
      "Iteration 1961, loss = 0.45130958\n",
      "Iteration 1962, loss = 0.45129665\n",
      "Iteration 1963, loss = 0.45128229\n",
      "Iteration 1964, loss = 0.45126676\n",
      "Iteration 1965, loss = 0.45125210\n",
      "Iteration 1966, loss = 0.45123818\n",
      "Iteration 1967, loss = 0.45121854\n",
      "Iteration 1968, loss = 0.45120857\n",
      "Iteration 1969, loss = 0.45119201\n",
      "Iteration 1970, loss = 0.45117789\n",
      "Iteration 1971, loss = 0.45116141\n",
      "Iteration 1972, loss = 0.45114470\n",
      "Iteration 1973, loss = 0.45113322\n",
      "Iteration 1974, loss = 0.45111262\n",
      "Iteration 1975, loss = 0.45110281\n",
      "Iteration 1976, loss = 0.45108481\n",
      "Iteration 1977, loss = 0.45106862\n",
      "Iteration 1978, loss = 0.45105418\n",
      "Iteration 1979, loss = 0.45104255\n",
      "Iteration 1980, loss = 0.45102551\n",
      "Iteration 1981, loss = 0.45101040\n",
      "Iteration 1982, loss = 0.45099756\n",
      "Iteration 1983, loss = 0.45098320\n",
      "Iteration 1984, loss = 0.45097041\n",
      "Iteration 1985, loss = 0.45095469\n",
      "Iteration 1986, loss = 0.45094174\n",
      "Iteration 1987, loss = 0.45092672\n",
      "Iteration 1988, loss = 0.45091693\n",
      "Iteration 1989, loss = 0.45090228\n",
      "Iteration 1990, loss = 0.45089200\n",
      "Iteration 1991, loss = 0.45087970\n",
      "Iteration 1992, loss = 0.45086877\n",
      "Iteration 1993, loss = 0.45085691\n",
      "Iteration 1994, loss = 0.45084424\n",
      "Iteration 1995, loss = 0.45083232\n",
      "Iteration 1996, loss = 0.45082096\n",
      "Iteration 1997, loss = 0.45080652\n",
      "Iteration 1998, loss = 0.45079412\n",
      "Iteration 1999, loss = 0.45078063\n",
      "Iteration 2000, loss = 0.45076950\n",
      "Iteration 2001, loss = 0.45075519\n",
      "Iteration 2002, loss = 0.45074368\n",
      "Iteration 2003, loss = 0.45072525\n",
      "Iteration 2004, loss = 0.45071558\n",
      "Iteration 2005, loss = 0.45070361\n",
      "Iteration 2006, loss = 0.45069200\n",
      "Iteration 2007, loss = 0.45068144\n",
      "Iteration 2008, loss = 0.45066773\n",
      "Iteration 2009, loss = 0.45065834\n",
      "Iteration 2010, loss = 0.45064833\n",
      "Iteration 2011, loss = 0.45063586\n",
      "Iteration 2012, loss = 0.45062766\n",
      "Iteration 2013, loss = 0.45061640\n",
      "Iteration 2014, loss = 0.45060446\n",
      "Iteration 2015, loss = 0.45059340\n",
      "Iteration 2016, loss = 0.45058311\n",
      "Iteration 2017, loss = 0.45057177\n",
      "Iteration 2018, loss = 0.45056036\n",
      "Iteration 2019, loss = 0.45054751\n",
      "Iteration 2020, loss = 0.45053557\n",
      "Iteration 2021, loss = 0.45052476\n",
      "Iteration 2022, loss = 0.45051142\n",
      "Iteration 2023, loss = 0.45049848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2024, loss = 0.45048694\n",
      "Iteration 2025, loss = 0.45047551\n",
      "Iteration 2026, loss = 0.45046199\n",
      "Iteration 2027, loss = 0.45045184\n",
      "Iteration 2028, loss = 0.45044129\n",
      "Iteration 2029, loss = 0.45042882\n",
      "Iteration 2030, loss = 0.45041914\n",
      "Iteration 2031, loss = 0.45041117\n",
      "Iteration 2032, loss = 0.45040033\n",
      "Iteration 2033, loss = 0.45038965\n",
      "Iteration 2034, loss = 0.45037930\n",
      "Iteration 2035, loss = 0.45036515\n",
      "Iteration 2036, loss = 0.45035364\n",
      "Iteration 2037, loss = 0.45034111\n",
      "Iteration 2038, loss = 0.45032732\n",
      "Iteration 2039, loss = 0.45031641\n",
      "Iteration 2040, loss = 0.45030252\n",
      "Iteration 2041, loss = 0.45029446\n",
      "Iteration 2042, loss = 0.45028251\n",
      "Iteration 2043, loss = 0.45027432\n",
      "Iteration 2044, loss = 0.45026284\n",
      "Iteration 2045, loss = 0.45025297\n",
      "Iteration 2046, loss = 0.45024219\n",
      "Iteration 2047, loss = 0.45022948\n",
      "Iteration 2048, loss = 0.45022009\n",
      "Iteration 2049, loss = 0.45020901\n",
      "Iteration 2050, loss = 0.45019921\n",
      "Iteration 2051, loss = 0.45018613\n",
      "Iteration 2052, loss = 0.45017522\n",
      "Iteration 2053, loss = 0.45016256\n",
      "Iteration 2054, loss = 0.45015064\n",
      "Iteration 2055, loss = 0.45013852\n",
      "Iteration 2056, loss = 0.45012427\n",
      "Iteration 2057, loss = 0.45011598\n",
      "Iteration 2058, loss = 0.45009892\n",
      "Iteration 2059, loss = 0.45008734\n",
      "Iteration 2060, loss = 0.45007429\n",
      "Iteration 2061, loss = 0.45006249\n",
      "Iteration 2062, loss = 0.45005021\n",
      "Iteration 2063, loss = 0.45003578\n",
      "Iteration 2064, loss = 0.45002587\n",
      "Iteration 2065, loss = 0.45001171\n",
      "Iteration 2066, loss = 0.44999885\n",
      "Iteration 2067, loss = 0.44998646\n",
      "Iteration 2068, loss = 0.44997371\n",
      "Iteration 2069, loss = 0.44996445\n",
      "Iteration 2070, loss = 0.44995206\n",
      "Iteration 2071, loss = 0.44994039\n",
      "Iteration 2072, loss = 0.44992849\n",
      "Iteration 2073, loss = 0.44991876\n",
      "Iteration 2074, loss = 0.44990718\n",
      "Iteration 2075, loss = 0.44989719\n",
      "Iteration 2076, loss = 0.44988762\n",
      "Iteration 2077, loss = 0.44988023\n",
      "Iteration 2078, loss = 0.44987007\n",
      "Iteration 2079, loss = 0.44985992\n",
      "Iteration 2080, loss = 0.44984794\n",
      "Iteration 2081, loss = 0.44983847\n",
      "Iteration 2082, loss = 0.44982392\n",
      "Iteration 2083, loss = 0.44981152\n",
      "Iteration 2084, loss = 0.44979838\n",
      "Iteration 2085, loss = 0.44978366\n",
      "Iteration 2086, loss = 0.44977043\n",
      "Iteration 2087, loss = 0.44975673\n",
      "Iteration 2088, loss = 0.44974498\n",
      "Iteration 2089, loss = 0.44973333\n",
      "Iteration 2090, loss = 0.44972082\n",
      "Iteration 2091, loss = 0.44971130\n",
      "Iteration 2092, loss = 0.44969876\n",
      "Iteration 2093, loss = 0.44969096\n",
      "Iteration 2094, loss = 0.44968037\n",
      "Iteration 2095, loss = 0.44966915\n",
      "Iteration 2096, loss = 0.44966096\n",
      "Iteration 2097, loss = 0.44964972\n",
      "Iteration 2098, loss = 0.44963858\n",
      "Iteration 2099, loss = 0.44962830\n",
      "Iteration 2100, loss = 0.44961607\n",
      "Iteration 2101, loss = 0.44960295\n",
      "Iteration 2102, loss = 0.44959011\n",
      "Iteration 2103, loss = 0.44957932\n",
      "Iteration 2104, loss = 0.44956938\n",
      "Iteration 2105, loss = 0.44955946\n",
      "Iteration 2106, loss = 0.44954891\n",
      "Iteration 2107, loss = 0.44953996\n",
      "Iteration 2108, loss = 0.44953196\n",
      "Iteration 2109, loss = 0.44951985\n",
      "Iteration 2110, loss = 0.44951140\n",
      "Iteration 2111, loss = 0.44950197\n",
      "Iteration 2112, loss = 0.44949491\n",
      "Iteration 2113, loss = 0.44948201\n",
      "Iteration 2114, loss = 0.44947201\n",
      "Iteration 2115, loss = 0.44946382\n",
      "Iteration 2116, loss = 0.44945352\n",
      "Iteration 2117, loss = 0.44944412\n",
      "Iteration 2118, loss = 0.44943508\n",
      "Iteration 2119, loss = 0.44942280\n",
      "Iteration 2120, loss = 0.44941178\n",
      "Iteration 2121, loss = 0.44940172\n",
      "Iteration 2122, loss = 0.44939143\n",
      "Iteration 2123, loss = 0.44937992\n",
      "Iteration 2124, loss = 0.44937080\n",
      "Iteration 2125, loss = 0.44935697\n",
      "Iteration 2126, loss = 0.44934769\n",
      "Iteration 2127, loss = 0.44933161\n",
      "Iteration 2128, loss = 0.44932239\n",
      "Iteration 2129, loss = 0.44931333\n",
      "Iteration 2130, loss = 0.44930069\n",
      "Iteration 2131, loss = 0.44929218\n",
      "Iteration 2132, loss = 0.44928075\n",
      "Iteration 2133, loss = 0.44927237\n",
      "Iteration 2134, loss = 0.44926271\n",
      "Iteration 2135, loss = 0.44925338\n",
      "Iteration 2136, loss = 0.44924492\n",
      "Iteration 2137, loss = 0.44923334\n",
      "Iteration 2138, loss = 0.44922212\n",
      "Iteration 2139, loss = 0.44921364\n",
      "Iteration 2140, loss = 0.44920492\n",
      "Iteration 2141, loss = 0.44919237\n",
      "Iteration 2142, loss = 0.44918100\n",
      "Iteration 2143, loss = 0.44917187\n",
      "Iteration 2144, loss = 0.44915837\n",
      "Iteration 2145, loss = 0.44915055\n",
      "Iteration 2146, loss = 0.44913933\n",
      "Iteration 2147, loss = 0.44913133\n",
      "Iteration 2148, loss = 0.44912261\n",
      "Iteration 2149, loss = 0.44911577\n",
      "Iteration 2150, loss = 0.44910771\n",
      "Iteration 2151, loss = 0.44909822\n",
      "Iteration 2152, loss = 0.44908795\n",
      "Iteration 2153, loss = 0.44907783\n",
      "Iteration 2154, loss = 0.44907022\n",
      "Iteration 2155, loss = 0.44905803\n",
      "Iteration 2156, loss = 0.44904903\n",
      "Iteration 2157, loss = 0.44903835\n",
      "Iteration 2158, loss = 0.44902883\n",
      "Iteration 2159, loss = 0.44901709\n",
      "Iteration 2160, loss = 0.44900826\n",
      "Iteration 2161, loss = 0.44899623\n",
      "Iteration 2162, loss = 0.44898721\n",
      "Iteration 2163, loss = 0.44897299\n",
      "Iteration 2164, loss = 0.44896647\n",
      "Iteration 2165, loss = 0.44895464\n",
      "Iteration 2166, loss = 0.44894368\n",
      "Iteration 2167, loss = 0.44893492\n",
      "Iteration 2168, loss = 0.44892410\n",
      "Iteration 2169, loss = 0.44891593\n",
      "Iteration 2170, loss = 0.44890403\n",
      "Iteration 2171, loss = 0.44889372\n",
      "Iteration 2172, loss = 0.44888486\n",
      "Iteration 2173, loss = 0.44887318\n",
      "Iteration 2174, loss = 0.44886424\n",
      "Iteration 2175, loss = 0.44885267\n",
      "Iteration 2176, loss = 0.44884178\n",
      "Iteration 2177, loss = 0.44883132\n",
      "Iteration 2178, loss = 0.44882080\n",
      "Iteration 2179, loss = 0.44881053\n",
      "Iteration 2180, loss = 0.44879793\n",
      "Iteration 2181, loss = 0.44878889\n",
      "Iteration 2182, loss = 0.44877663\n",
      "Iteration 2183, loss = 0.44876656\n",
      "Iteration 2184, loss = 0.44875743\n",
      "Iteration 2185, loss = 0.44874679\n",
      "Iteration 2186, loss = 0.44873632\n",
      "Iteration 2187, loss = 0.44872292\n",
      "Iteration 2188, loss = 0.44871356\n",
      "Iteration 2189, loss = 0.44870008\n",
      "Iteration 2190, loss = 0.44868900\n",
      "Iteration 2191, loss = 0.44868148\n",
      "Iteration 2192, loss = 0.44866866\n",
      "Iteration 2193, loss = 0.44865778\n",
      "Iteration 2194, loss = 0.44865326\n",
      "Iteration 2195, loss = 0.44864302\n",
      "Iteration 2196, loss = 0.44863220\n",
      "Iteration 2197, loss = 0.44862576\n",
      "Iteration 2198, loss = 0.44861374\n",
      "Iteration 2199, loss = 0.44860704\n",
      "Iteration 2200, loss = 0.44859713\n",
      "Iteration 2201, loss = 0.44858818\n",
      "Iteration 2202, loss = 0.44857958\n",
      "Iteration 2203, loss = 0.44857082\n",
      "Iteration 2204, loss = 0.44856216\n",
      "Iteration 2205, loss = 0.44855280\n",
      "Iteration 2206, loss = 0.44854293\n",
      "Iteration 2207, loss = 0.44853439\n",
      "Iteration 2208, loss = 0.44852742\n",
      "Iteration 2209, loss = 0.44852168\n",
      "Iteration 2210, loss = 0.44851293\n",
      "Iteration 2211, loss = 0.44850550\n",
      "Iteration 2212, loss = 0.44849637\n",
      "Iteration 2213, loss = 0.44849122\n",
      "Iteration 2214, loss = 0.44848054\n",
      "Iteration 2215, loss = 0.44847418\n",
      "Iteration 2216, loss = 0.44846757\n",
      "Iteration 2217, loss = 0.44845774\n",
      "Iteration 2218, loss = 0.44845317\n",
      "Iteration 2219, loss = 0.44844357\n",
      "Iteration 2220, loss = 0.44843741\n",
      "Iteration 2221, loss = 0.44842777\n",
      "Iteration 2222, loss = 0.44841667\n",
      "Iteration 2223, loss = 0.44840579\n",
      "Iteration 2224, loss = 0.44839646\n",
      "Iteration 2225, loss = 0.44838645\n",
      "Iteration 2226, loss = 0.44837308\n",
      "Iteration 2227, loss = 0.44836148\n",
      "Iteration 2228, loss = 0.44835319\n",
      "Iteration 2229, loss = 0.44834239\n",
      "Iteration 2230, loss = 0.44833639\n",
      "Iteration 2231, loss = 0.44832455\n",
      "Iteration 2232, loss = 0.44831587\n",
      "Iteration 2233, loss = 0.44830631\n",
      "Iteration 2234, loss = 0.44829897\n",
      "Iteration 2235, loss = 0.44828516\n",
      "Iteration 2236, loss = 0.44827606\n",
      "Iteration 2237, loss = 0.44826657\n",
      "Iteration 2238, loss = 0.44825886\n",
      "Iteration 2239, loss = 0.44824807\n",
      "Iteration 2240, loss = 0.44824143\n",
      "Iteration 2241, loss = 0.44823209\n",
      "Iteration 2242, loss = 0.44822440\n",
      "Iteration 2243, loss = 0.44821629\n",
      "Iteration 2244, loss = 0.44820814\n",
      "Iteration 2245, loss = 0.44819688\n",
      "Iteration 2246, loss = 0.44818842\n",
      "Iteration 2247, loss = 0.44817970\n",
      "Iteration 2248, loss = 0.44817108\n",
      "Iteration 2249, loss = 0.44816423\n",
      "Iteration 2250, loss = 0.44815171\n",
      "Iteration 2251, loss = 0.44814389\n",
      "Iteration 2252, loss = 0.44813594\n",
      "Iteration 2253, loss = 0.44813153\n",
      "Iteration 2254, loss = 0.44812229\n",
      "Iteration 2255, loss = 0.44811637\n",
      "Iteration 2256, loss = 0.44810799\n",
      "Iteration 2257, loss = 0.44810282\n",
      "Iteration 2258, loss = 0.44809491\n",
      "Iteration 2259, loss = 0.44808658\n",
      "Iteration 2260, loss = 0.44807971\n",
      "Iteration 2261, loss = 0.44807548\n",
      "Iteration 2262, loss = 0.44806596\n",
      "Iteration 2263, loss = 0.44805919\n",
      "Iteration 2264, loss = 0.44805320\n",
      "Iteration 2265, loss = 0.44804360\n",
      "Iteration 2266, loss = 0.44803745\n",
      "Iteration 2267, loss = 0.44802885\n",
      "Iteration 2268, loss = 0.44802312\n",
      "Iteration 2269, loss = 0.44801892\n",
      "Iteration 2270, loss = 0.44801226\n",
      "Iteration 2271, loss = 0.44800406\n",
      "Iteration 2272, loss = 0.44799686\n",
      "Iteration 2273, loss = 0.44799041\n",
      "Iteration 2274, loss = 0.44798226\n",
      "Iteration 2275, loss = 0.44797563\n",
      "Iteration 2276, loss = 0.44796957\n",
      "Iteration 2277, loss = 0.44796249\n",
      "Iteration 2278, loss = 0.44795634\n",
      "Iteration 2279, loss = 0.44795008\n",
      "Iteration 2280, loss = 0.44794467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2281, loss = 0.44793670\n",
      "Iteration 2282, loss = 0.44793056\n",
      "Iteration 2283, loss = 0.44792303\n",
      "Iteration 2284, loss = 0.44791207\n",
      "Iteration 2285, loss = 0.44790544\n",
      "Iteration 2286, loss = 0.44789789\n",
      "Iteration 2287, loss = 0.44788797\n",
      "Iteration 2288, loss = 0.44787911\n",
      "Iteration 2289, loss = 0.44786821\n",
      "Iteration 2290, loss = 0.44785799\n",
      "Iteration 2291, loss = 0.44784870\n",
      "Iteration 2292, loss = 0.44783767\n",
      "Iteration 2293, loss = 0.44782747\n",
      "Iteration 2294, loss = 0.44781842\n",
      "Iteration 2295, loss = 0.44780909\n",
      "Iteration 2296, loss = 0.44779840\n",
      "Iteration 2297, loss = 0.44779170\n",
      "Iteration 2298, loss = 0.44778294\n",
      "Iteration 2299, loss = 0.44777755\n",
      "Iteration 2300, loss = 0.44777082\n",
      "Iteration 2301, loss = 0.44776614\n",
      "Iteration 2302, loss = 0.44775924\n",
      "Iteration 2303, loss = 0.44775369\n",
      "Iteration 2304, loss = 0.44774617\n",
      "Iteration 2305, loss = 0.44773997\n",
      "Iteration 2306, loss = 0.44773326\n",
      "Iteration 2307, loss = 0.44772675\n",
      "Iteration 2308, loss = 0.44771887\n",
      "Iteration 2309, loss = 0.44771233\n",
      "Iteration 2310, loss = 0.44770575\n",
      "Iteration 2311, loss = 0.44769821\n",
      "Iteration 2312, loss = 0.44769230\n",
      "Iteration 2313, loss = 0.44768776\n",
      "Iteration 2314, loss = 0.44767820\n",
      "Iteration 2315, loss = 0.44767224\n",
      "Iteration 2316, loss = 0.44766409\n",
      "Iteration 2317, loss = 0.44765970\n",
      "Iteration 2318, loss = 0.44765132\n",
      "Iteration 2319, loss = 0.44764389\n",
      "Iteration 2320, loss = 0.44763738\n",
      "Iteration 2321, loss = 0.44762992\n",
      "Iteration 2322, loss = 0.44762358\n",
      "Iteration 2323, loss = 0.44761538\n",
      "Iteration 2324, loss = 0.44760824\n",
      "Iteration 2325, loss = 0.44760105\n",
      "Iteration 2326, loss = 0.44759332\n",
      "Iteration 2327, loss = 0.44758721\n",
      "Iteration 2328, loss = 0.44757845\n",
      "Iteration 2329, loss = 0.44757043\n",
      "Iteration 2330, loss = 0.44756239\n",
      "Iteration 2331, loss = 0.44755537\n",
      "Iteration 2332, loss = 0.44754776\n",
      "Iteration 2333, loss = 0.44753966\n",
      "Iteration 2334, loss = 0.44753048\n",
      "Iteration 2335, loss = 0.44752331\n",
      "Iteration 2336, loss = 0.44751641\n",
      "Iteration 2337, loss = 0.44750986\n",
      "Iteration 2338, loss = 0.44750479\n",
      "Iteration 2339, loss = 0.44749749\n",
      "Iteration 2340, loss = 0.44748956\n",
      "Iteration 2341, loss = 0.44748245\n",
      "Iteration 2342, loss = 0.44747498\n",
      "Iteration 2343, loss = 0.44746872\n",
      "Iteration 2344, loss = 0.44746237\n",
      "Iteration 2345, loss = 0.44745488\n",
      "Iteration 2346, loss = 0.44744791\n",
      "Iteration 2347, loss = 0.44743828\n",
      "Iteration 2348, loss = 0.44743055\n",
      "Iteration 2349, loss = 0.44742389\n",
      "Iteration 2350, loss = 0.44741845\n",
      "Iteration 2351, loss = 0.44741392\n",
      "Iteration 2352, loss = 0.44740358\n",
      "Iteration 2353, loss = 0.44739692\n",
      "Iteration 2354, loss = 0.44739098\n",
      "Iteration 2355, loss = 0.44738659\n",
      "Iteration 2356, loss = 0.44737845\n",
      "Iteration 2357, loss = 0.44737153\n",
      "Iteration 2358, loss = 0.44736423\n",
      "Iteration 2359, loss = 0.44735640\n",
      "Iteration 2360, loss = 0.44734836\n",
      "Iteration 2361, loss = 0.44734019\n",
      "Iteration 2362, loss = 0.44733294\n",
      "Iteration 2363, loss = 0.44732717\n",
      "Iteration 2364, loss = 0.44732079\n",
      "Iteration 2365, loss = 0.44731259\n",
      "Iteration 2366, loss = 0.44730621\n",
      "Iteration 2367, loss = 0.44729998\n",
      "Iteration 2368, loss = 0.44729237\n",
      "Iteration 2369, loss = 0.44728347\n",
      "Iteration 2370, loss = 0.44727545\n",
      "Iteration 2371, loss = 0.44726900\n",
      "Iteration 2372, loss = 0.44726070\n",
      "Iteration 2373, loss = 0.44725437\n",
      "Iteration 2374, loss = 0.44724344\n",
      "Iteration 2375, loss = 0.44723754\n",
      "Iteration 2376, loss = 0.44722881\n",
      "Iteration 2377, loss = 0.44722285\n",
      "Iteration 2378, loss = 0.44721621\n",
      "Iteration 2379, loss = 0.44720984\n",
      "Iteration 2380, loss = 0.44720175\n",
      "Iteration 2381, loss = 0.44719645\n",
      "Iteration 2382, loss = 0.44719030\n",
      "Iteration 2383, loss = 0.44718445\n",
      "Iteration 2384, loss = 0.44717781\n",
      "Iteration 2385, loss = 0.44717200\n",
      "Iteration 2386, loss = 0.44716638\n",
      "Iteration 2387, loss = 0.44716172\n",
      "Iteration 2388, loss = 0.44715279\n",
      "Iteration 2389, loss = 0.44714582\n",
      "Iteration 2390, loss = 0.44713884\n",
      "Iteration 2391, loss = 0.44713081\n",
      "Iteration 2392, loss = 0.44712434\n",
      "Iteration 2393, loss = 0.44711708\n",
      "Iteration 2394, loss = 0.44710730\n",
      "Iteration 2395, loss = 0.44709990\n",
      "Iteration 2396, loss = 0.44709370\n",
      "Iteration 2397, loss = 0.44708399\n",
      "Iteration 2398, loss = 0.44707881\n",
      "Iteration 2399, loss = 0.44707152\n",
      "Iteration 2400, loss = 0.44706403\n",
      "Iteration 2401, loss = 0.44705659\n",
      "Iteration 2402, loss = 0.44704917\n",
      "Iteration 2403, loss = 0.44703756\n",
      "Iteration 2404, loss = 0.44702994\n",
      "Iteration 2405, loss = 0.44701957\n",
      "Iteration 2406, loss = 0.44700974\n",
      "Iteration 2407, loss = 0.44700429\n",
      "Iteration 2408, loss = 0.44699542\n",
      "Iteration 2409, loss = 0.44698894\n",
      "Iteration 2410, loss = 0.44698108\n",
      "Iteration 2411, loss = 0.44697546\n",
      "Iteration 2412, loss = 0.44696769\n",
      "Iteration 2413, loss = 0.44696077\n",
      "Iteration 2414, loss = 0.44695280\n",
      "Iteration 2415, loss = 0.44694723\n",
      "Iteration 2416, loss = 0.44693987\n",
      "Iteration 2417, loss = 0.44693016\n",
      "Iteration 2418, loss = 0.44692408\n",
      "Iteration 2419, loss = 0.44691666\n",
      "Iteration 2420, loss = 0.44690894\n",
      "Iteration 2421, loss = 0.44689980\n",
      "Iteration 2422, loss = 0.44689218\n",
      "Iteration 2423, loss = 0.44688446\n",
      "Iteration 2424, loss = 0.44687828\n",
      "Iteration 2425, loss = 0.44687168\n",
      "Iteration 2426, loss = 0.44686342\n",
      "Iteration 2427, loss = 0.44685713\n",
      "Iteration 2428, loss = 0.44684747\n",
      "Iteration 2429, loss = 0.44684558\n",
      "Iteration 2430, loss = 0.44683373\n",
      "Iteration 2431, loss = 0.44682895\n",
      "Iteration 2432, loss = 0.44682191\n",
      "Iteration 2433, loss = 0.44681384\n",
      "Iteration 2434, loss = 0.44680875\n",
      "Iteration 2435, loss = 0.44679881\n",
      "Iteration 2436, loss = 0.44679293\n",
      "Iteration 2437, loss = 0.44678655\n",
      "Iteration 2438, loss = 0.44678121\n",
      "Iteration 2439, loss = 0.44677548\n",
      "Iteration 2440, loss = 0.44676797\n",
      "Iteration 2441, loss = 0.44676277\n",
      "Iteration 2442, loss = 0.44675861\n",
      "Iteration 2443, loss = 0.44675158\n",
      "Iteration 2444, loss = 0.44674596\n",
      "Iteration 2445, loss = 0.44674001\n",
      "Iteration 2446, loss = 0.44673667\n",
      "Iteration 2447, loss = 0.44672789\n",
      "Iteration 2448, loss = 0.44672012\n",
      "Iteration 2449, loss = 0.44671602\n",
      "Iteration 2450, loss = 0.44670933\n",
      "Iteration 2451, loss = 0.44670439\n",
      "Iteration 2452, loss = 0.44669978\n",
      "Iteration 2453, loss = 0.44669135\n",
      "Iteration 2454, loss = 0.44668662\n",
      "Iteration 2455, loss = 0.44667943\n",
      "Iteration 2456, loss = 0.44667179\n",
      "Iteration 2457, loss = 0.44666574\n",
      "Iteration 2458, loss = 0.44665949\n",
      "Iteration 2459, loss = 0.44665386\n",
      "Iteration 2460, loss = 0.44664439\n",
      "Iteration 2461, loss = 0.44663943\n",
      "Iteration 2462, loss = 0.44663121\n",
      "Iteration 2463, loss = 0.44662826\n",
      "Iteration 2464, loss = 0.44661945\n",
      "Iteration 2465, loss = 0.44661472\n",
      "Iteration 2466, loss = 0.44660775\n",
      "Iteration 2467, loss = 0.44660209\n",
      "Iteration 2468, loss = 0.44659705\n",
      "Iteration 2469, loss = 0.44658990\n",
      "Iteration 2470, loss = 0.44658376\n",
      "Iteration 2471, loss = 0.44657848\n",
      "Iteration 2472, loss = 0.44657141\n",
      "Iteration 2473, loss = 0.44656595\n",
      "Iteration 2474, loss = 0.44656061\n",
      "Iteration 2475, loss = 0.44655321\n",
      "Iteration 2476, loss = 0.44654652\n",
      "Iteration 2477, loss = 0.44654152\n",
      "Iteration 2478, loss = 0.44653398\n",
      "Iteration 2479, loss = 0.44653008\n",
      "Iteration 2480, loss = 0.44652293\n",
      "Iteration 2481, loss = 0.44651803\n",
      "Iteration 2482, loss = 0.44651406\n",
      "Iteration 2483, loss = 0.44650854\n",
      "Iteration 2484, loss = 0.44650492\n",
      "Iteration 2485, loss = 0.44650042\n",
      "Iteration 2486, loss = 0.44649620\n",
      "Iteration 2487, loss = 0.44649091\n",
      "Iteration 2488, loss = 0.44648450\n",
      "Iteration 2489, loss = 0.44648128\n",
      "Iteration 2490, loss = 0.44647321\n",
      "Iteration 2491, loss = 0.44646544\n",
      "Iteration 2492, loss = 0.44645813\n",
      "Iteration 2493, loss = 0.44645442\n",
      "Iteration 2494, loss = 0.44644420\n",
      "Iteration 2495, loss = 0.44643758\n",
      "Iteration 2496, loss = 0.44642860\n",
      "Iteration 2497, loss = 0.44642296\n",
      "Iteration 2498, loss = 0.44641472\n",
      "Iteration 2499, loss = 0.44640687\n",
      "Iteration 2500, loss = 0.44640019\n",
      "Iteration 2501, loss = 0.44639466\n",
      "Iteration 2502, loss = 0.44638520\n",
      "Iteration 2503, loss = 0.44638092\n",
      "Iteration 2504, loss = 0.44637314\n",
      "Iteration 2505, loss = 0.44636942\n",
      "Iteration 2506, loss = 0.44636241\n",
      "Iteration 2507, loss = 0.44635840\n",
      "Iteration 2508, loss = 0.44635196\n",
      "Iteration 2509, loss = 0.44634693\n",
      "Iteration 2510, loss = 0.44634245\n",
      "Iteration 2511, loss = 0.44633503\n",
      "Iteration 2512, loss = 0.44632915\n",
      "Iteration 2513, loss = 0.44632418\n",
      "Iteration 2514, loss = 0.44631911\n",
      "Iteration 2515, loss = 0.44631254\n",
      "Iteration 2516, loss = 0.44630790\n",
      "Iteration 2517, loss = 0.44630008\n",
      "Iteration 2518, loss = 0.44629436\n",
      "Iteration 2519, loss = 0.44629082\n",
      "Iteration 2520, loss = 0.44628404\n",
      "Iteration 2521, loss = 0.44627977\n",
      "Iteration 2522, loss = 0.44627353\n",
      "Iteration 2523, loss = 0.44626804\n",
      "Iteration 2524, loss = 0.44626047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2525, loss = 0.44625463\n",
      "Iteration 2526, loss = 0.44624914\n",
      "Iteration 2527, loss = 0.44624106\n",
      "Iteration 2528, loss = 0.44623460\n",
      "Iteration 2529, loss = 0.44622891\n",
      "Iteration 2530, loss = 0.44622379\n",
      "Iteration 2531, loss = 0.44621863\n",
      "Iteration 2532, loss = 0.44621375\n",
      "Iteration 2533, loss = 0.44620880\n",
      "Iteration 2534, loss = 0.44620416\n",
      "Iteration 2535, loss = 0.44620127\n",
      "Iteration 2536, loss = 0.44619448\n",
      "Iteration 2537, loss = 0.44618855\n",
      "Iteration 2538, loss = 0.44618380\n",
      "Iteration 2539, loss = 0.44617821\n",
      "Iteration 2540, loss = 0.44617279\n",
      "Iteration 2541, loss = 0.44616743\n",
      "Iteration 2542, loss = 0.44616340\n",
      "Iteration 2543, loss = 0.44615754\n",
      "Iteration 2544, loss = 0.44615166\n",
      "Iteration 2545, loss = 0.44614710\n",
      "Iteration 2546, loss = 0.44614076\n",
      "Iteration 2547, loss = 0.44613305\n",
      "Iteration 2548, loss = 0.44612806\n",
      "Iteration 2549, loss = 0.44612276\n",
      "Iteration 2550, loss = 0.44611544\n",
      "Iteration 2551, loss = 0.44611051\n",
      "Iteration 2552, loss = 0.44610370\n",
      "Iteration 2553, loss = 0.44609771\n",
      "Iteration 2554, loss = 0.44609282\n",
      "Iteration 2555, loss = 0.44608500\n",
      "Iteration 2556, loss = 0.44607859\n",
      "Iteration 2557, loss = 0.44607286\n",
      "Iteration 2558, loss = 0.44606735\n",
      "Iteration 2559, loss = 0.44606150\n",
      "Iteration 2560, loss = 0.44605570\n",
      "Iteration 2561, loss = 0.44604838\n",
      "Iteration 2562, loss = 0.44604251\n",
      "Iteration 2563, loss = 0.44603643\n",
      "Iteration 2564, loss = 0.44602703\n",
      "Iteration 2565, loss = 0.44602110\n",
      "Iteration 2566, loss = 0.44601502\n",
      "Iteration 2567, loss = 0.44600781\n",
      "Iteration 2568, loss = 0.44600320\n",
      "Iteration 2569, loss = 0.44599811\n",
      "Iteration 2570, loss = 0.44599269\n",
      "Iteration 2571, loss = 0.44598750\n",
      "Iteration 2572, loss = 0.44598351\n",
      "Iteration 2573, loss = 0.44597716\n",
      "Iteration 2574, loss = 0.44597254\n",
      "Iteration 2575, loss = 0.44596790\n",
      "Iteration 2576, loss = 0.44596398\n",
      "Iteration 2577, loss = 0.44595877\n",
      "Iteration 2578, loss = 0.44595281\n",
      "Iteration 2579, loss = 0.44594968\n",
      "Iteration 2580, loss = 0.44594433\n",
      "Iteration 2581, loss = 0.44594063\n",
      "Iteration 2582, loss = 0.44593557\n",
      "Iteration 2583, loss = 0.44593060\n",
      "Iteration 2584, loss = 0.44592665\n",
      "Iteration 2585, loss = 0.44592296\n",
      "Iteration 2586, loss = 0.44592032\n",
      "Iteration 2587, loss = 0.44591294\n",
      "Iteration 2588, loss = 0.44590575\n",
      "Iteration 2589, loss = 0.44590118\n",
      "Iteration 2590, loss = 0.44589385\n",
      "Iteration 2591, loss = 0.44589009\n",
      "Iteration 2592, loss = 0.44588434\n",
      "Iteration 2593, loss = 0.44588065\n",
      "Iteration 2594, loss = 0.44587474\n",
      "Iteration 2595, loss = 0.44586925\n",
      "Iteration 2596, loss = 0.44586574\n",
      "Iteration 2597, loss = 0.44585896\n",
      "Iteration 2598, loss = 0.44585123\n",
      "Iteration 2599, loss = 0.44584819\n",
      "Iteration 2600, loss = 0.44584119\n",
      "Iteration 2601, loss = 0.44583561\n",
      "Iteration 2602, loss = 0.44583070\n",
      "Iteration 2603, loss = 0.44582753\n",
      "Iteration 2604, loss = 0.44582120\n",
      "Iteration 2605, loss = 0.44581762\n",
      "Iteration 2606, loss = 0.44581027\n",
      "Iteration 2607, loss = 0.44580513\n",
      "Iteration 2608, loss = 0.44580069\n",
      "Iteration 2609, loss = 0.44579437\n",
      "Iteration 2610, loss = 0.44578924\n",
      "Iteration 2611, loss = 0.44578682\n",
      "Iteration 2612, loss = 0.44577996\n",
      "Iteration 2613, loss = 0.44577545\n",
      "Iteration 2614, loss = 0.44577245\n",
      "Iteration 2615, loss = 0.44576759\n",
      "Iteration 2616, loss = 0.44576496\n",
      "Iteration 2617, loss = 0.44575962\n",
      "Iteration 2618, loss = 0.44575429\n",
      "Iteration 2619, loss = 0.44575084\n",
      "Iteration 2620, loss = 0.44574709\n",
      "Iteration 2621, loss = 0.44574259\n",
      "Iteration 2622, loss = 0.44573965\n",
      "Iteration 2623, loss = 0.44573551\n",
      "Iteration 2624, loss = 0.44573174\n",
      "Iteration 2625, loss = 0.44572720\n",
      "Iteration 2626, loss = 0.44572288\n",
      "Iteration 2627, loss = 0.44571897\n",
      "Iteration 2628, loss = 0.44571438\n",
      "Iteration 2629, loss = 0.44570887\n",
      "Iteration 2630, loss = 0.44570403\n",
      "Iteration 2631, loss = 0.44569782\n",
      "Iteration 2632, loss = 0.44569263\n",
      "Iteration 2633, loss = 0.44568760\n",
      "Iteration 2634, loss = 0.44568308\n",
      "Iteration 2635, loss = 0.44567850\n",
      "Iteration 2636, loss = 0.44567562\n",
      "Iteration 2637, loss = 0.44567067\n",
      "Iteration 2638, loss = 0.44566743\n",
      "Iteration 2639, loss = 0.44566294\n",
      "Iteration 2640, loss = 0.44565926\n",
      "Iteration 2641, loss = 0.44565443\n",
      "Iteration 2642, loss = 0.44564930\n",
      "Iteration 2643, loss = 0.44564550\n",
      "Iteration 2644, loss = 0.44564092\n",
      "Iteration 2645, loss = 0.44563527\n",
      "Iteration 2646, loss = 0.44563182\n",
      "Iteration 2647, loss = 0.44562719\n",
      "Iteration 2648, loss = 0.44562030\n",
      "Iteration 2649, loss = 0.44561757\n",
      "Iteration 2650, loss = 0.44560951\n",
      "Iteration 2651, loss = 0.44560503\n",
      "Iteration 2652, loss = 0.44560009\n",
      "Iteration 2653, loss = 0.44559492\n",
      "Iteration 2654, loss = 0.44559115\n",
      "Iteration 2655, loss = 0.44558673\n",
      "Iteration 2656, loss = 0.44558255\n",
      "Iteration 2657, loss = 0.44557839\n",
      "Iteration 2658, loss = 0.44557257\n",
      "Iteration 2659, loss = 0.44556903\n",
      "Iteration 2660, loss = 0.44556669\n",
      "Iteration 2661, loss = 0.44556275\n",
      "Iteration 2662, loss = 0.44555798\n",
      "Iteration 2663, loss = 0.44555299\n",
      "Iteration 2664, loss = 0.44554875\n",
      "Iteration 2665, loss = 0.44554332\n",
      "Iteration 2666, loss = 0.44554000\n",
      "Iteration 2667, loss = 0.44553647\n",
      "Iteration 2668, loss = 0.44553192\n",
      "Iteration 2669, loss = 0.44552851\n",
      "Iteration 2670, loss = 0.44552419\n",
      "Iteration 2671, loss = 0.44552110\n",
      "Iteration 2672, loss = 0.44551497\n",
      "Iteration 2673, loss = 0.44551086\n",
      "Iteration 2674, loss = 0.44550571\n",
      "Iteration 2675, loss = 0.44550007\n",
      "Iteration 2676, loss = 0.44549545\n",
      "Iteration 2677, loss = 0.44549168\n",
      "Iteration 2678, loss = 0.44548733\n",
      "Iteration 2679, loss = 0.44548358\n",
      "Iteration 2680, loss = 0.44547900\n",
      "Iteration 2681, loss = 0.44547446\n",
      "Iteration 2682, loss = 0.44546880\n",
      "Iteration 2683, loss = 0.44546142\n",
      "Iteration 2684, loss = 0.44545687\n",
      "Iteration 2685, loss = 0.44545062\n",
      "Iteration 2686, loss = 0.44544711\n",
      "Iteration 2687, loss = 0.44544135\n",
      "Iteration 2688, loss = 0.44543535\n",
      "Iteration 2689, loss = 0.44543062\n",
      "Iteration 2690, loss = 0.44542413\n",
      "Iteration 2691, loss = 0.44542124\n",
      "Iteration 2692, loss = 0.44541555\n",
      "Iteration 2693, loss = 0.44541168\n",
      "Iteration 2694, loss = 0.44540725\n",
      "Iteration 2695, loss = 0.44540303\n",
      "Iteration 2696, loss = 0.44540025\n",
      "Iteration 2697, loss = 0.44539683\n",
      "Iteration 2698, loss = 0.44539014\n",
      "Iteration 2699, loss = 0.44538702\n",
      "Iteration 2700, loss = 0.44538246\n",
      "Iteration 2701, loss = 0.44537814\n",
      "Iteration 2702, loss = 0.44537436\n",
      "Iteration 2703, loss = 0.44537113\n",
      "Iteration 2704, loss = 0.44536711\n",
      "Iteration 2705, loss = 0.44536255\n",
      "Iteration 2706, loss = 0.44535780\n",
      "Iteration 2707, loss = 0.44535316\n",
      "Iteration 2708, loss = 0.44535171\n",
      "Iteration 2709, loss = 0.44534333\n",
      "Iteration 2710, loss = 0.44533976\n",
      "Iteration 2711, loss = 0.44533507\n",
      "Iteration 2712, loss = 0.44533028\n",
      "Iteration 2713, loss = 0.44532662\n",
      "Iteration 2714, loss = 0.44532191\n",
      "Iteration 2715, loss = 0.44531661\n",
      "Iteration 2716, loss = 0.44531251\n",
      "Iteration 2717, loss = 0.44530833\n",
      "Iteration 2718, loss = 0.44530465\n",
      "Iteration 2719, loss = 0.44530068\n",
      "Iteration 2720, loss = 0.44529708\n",
      "Iteration 2721, loss = 0.44529342\n",
      "Iteration 2722, loss = 0.44528668\n",
      "Iteration 2723, loss = 0.44528275\n",
      "Iteration 2724, loss = 0.44527627\n",
      "Iteration 2725, loss = 0.44527181\n",
      "Iteration 2726, loss = 0.44526973\n",
      "Iteration 2727, loss = 0.44526507\n",
      "Iteration 2728, loss = 0.44526264\n",
      "Iteration 2729, loss = 0.44526084\n",
      "Iteration 2730, loss = 0.44525686\n",
      "Iteration 2731, loss = 0.44525282\n",
      "Iteration 2732, loss = 0.44524853\n",
      "Iteration 2733, loss = 0.44524413\n",
      "Iteration 2734, loss = 0.44524221\n",
      "Iteration 2735, loss = 0.44523647\n",
      "Iteration 2736, loss = 0.44523371\n",
      "Iteration 2737, loss = 0.44523004\n",
      "Iteration 2738, loss = 0.44522790\n",
      "Iteration 2739, loss = 0.44522504\n",
      "Iteration 2740, loss = 0.44522237\n",
      "Iteration 2741, loss = 0.44521954\n",
      "Iteration 2742, loss = 0.44521731\n",
      "Iteration 2743, loss = 0.44521433\n",
      "Iteration 2744, loss = 0.44521079\n",
      "Iteration 2745, loss = 0.44520771\n",
      "Iteration 2746, loss = 0.44520406\n",
      "Iteration 2747, loss = 0.44519957\n",
      "Iteration 2748, loss = 0.44519630\n",
      "Iteration 2749, loss = 0.44519244\n",
      "Iteration 2750, loss = 0.44518882\n",
      "Iteration 2751, loss = 0.44518442\n",
      "Iteration 2752, loss = 0.44518100\n",
      "Iteration 2753, loss = 0.44517804\n",
      "Iteration 2754, loss = 0.44517260\n",
      "Iteration 2755, loss = 0.44516983\n",
      "Iteration 2756, loss = 0.44516579\n",
      "Iteration 2757, loss = 0.44516362\n",
      "Iteration 2758, loss = 0.44515970\n",
      "Iteration 2759, loss = 0.44515680\n",
      "Iteration 2760, loss = 0.44515309\n",
      "Iteration 2761, loss = 0.44514802\n",
      "Iteration 2762, loss = 0.44514429\n",
      "Iteration 2763, loss = 0.44514117\n",
      "Iteration 2764, loss = 0.44513834\n",
      "Iteration 2765, loss = 0.44513316\n",
      "Iteration 2766, loss = 0.44512951\n",
      "Iteration 2767, loss = 0.44512654\n",
      "Iteration 2768, loss = 0.44512239\n",
      "Iteration 2769, loss = 0.44511961\n",
      "Iteration 2770, loss = 0.44511581\n",
      "Iteration 2771, loss = 0.44511378\n",
      "Iteration 2772, loss = 0.44511057\n",
      "Iteration 2773, loss = 0.44510654\n",
      "Iteration 2774, loss = 0.44510637\n",
      "Iteration 2775, loss = 0.44510050\n",
      "Iteration 2776, loss = 0.44509771\n",
      "Iteration 2777, loss = 0.44509429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2778, loss = 0.44508956\n",
      "Iteration 2779, loss = 0.44508620\n",
      "Iteration 2780, loss = 0.44508168\n",
      "Iteration 2781, loss = 0.44507796\n",
      "Iteration 2782, loss = 0.44507446\n",
      "Iteration 2783, loss = 0.44506988\n",
      "Iteration 2784, loss = 0.44506644\n",
      "Iteration 2785, loss = 0.44506267\n",
      "Iteration 2786, loss = 0.44505920\n",
      "Iteration 2787, loss = 0.44505615\n",
      "Iteration 2788, loss = 0.44505142\n",
      "Iteration 2789, loss = 0.44504749\n",
      "Iteration 2790, loss = 0.44504385\n",
      "Iteration 2791, loss = 0.44504065\n",
      "Iteration 2792, loss = 0.44503747\n",
      "Iteration 2793, loss = 0.44503297\n",
      "Iteration 2794, loss = 0.44502969\n",
      "Iteration 2795, loss = 0.44502630\n",
      "Iteration 2796, loss = 0.44502403\n",
      "Iteration 2797, loss = 0.44502062\n",
      "Iteration 2798, loss = 0.44501695\n",
      "Iteration 2799, loss = 0.44501414\n",
      "Iteration 2800, loss = 0.44500872\n",
      "Iteration 2801, loss = 0.44500731\n",
      "Iteration 2802, loss = 0.44500258\n",
      "Iteration 2803, loss = 0.44499910\n",
      "Iteration 2804, loss = 0.44499637\n",
      "Iteration 2805, loss = 0.44499260\n",
      "Iteration 2806, loss = 0.44498949\n",
      "Iteration 2807, loss = 0.44498495\n",
      "Iteration 2808, loss = 0.44498228\n",
      "Iteration 2809, loss = 0.44497682\n",
      "Iteration 2810, loss = 0.44497167\n",
      "Iteration 2811, loss = 0.44496821\n",
      "Iteration 2812, loss = 0.44496464\n",
      "Iteration 2813, loss = 0.44495992\n",
      "Iteration 2814, loss = 0.44495637\n",
      "Iteration 2815, loss = 0.44495184\n",
      "Iteration 2816, loss = 0.44494973\n",
      "Iteration 2817, loss = 0.44494620\n",
      "Iteration 2818, loss = 0.44494298\n",
      "Iteration 2819, loss = 0.44493865\n",
      "Iteration 2820, loss = 0.44493552\n",
      "Iteration 2821, loss = 0.44493187\n",
      "Iteration 2822, loss = 0.44492768\n",
      "Iteration 2823, loss = 0.44492421\n",
      "Iteration 2824, loss = 0.44491977\n",
      "Iteration 2825, loss = 0.44491499\n",
      "Iteration 2826, loss = 0.44491091\n",
      "Iteration 2827, loss = 0.44490593\n",
      "Iteration 2828, loss = 0.44490227\n",
      "Iteration 2829, loss = 0.44489746\n",
      "Iteration 2830, loss = 0.44489381\n",
      "Iteration 2831, loss = 0.44489106\n",
      "Iteration 2832, loss = 0.44488832\n",
      "Iteration 2833, loss = 0.44488469\n",
      "Iteration 2834, loss = 0.44488021\n",
      "Iteration 2835, loss = 0.44487636\n",
      "Iteration 2836, loss = 0.44487262\n",
      "Iteration 2837, loss = 0.44486968\n",
      "Iteration 2838, loss = 0.44486536\n",
      "Iteration 2839, loss = 0.44486209\n",
      "Iteration 2840, loss = 0.44485780\n",
      "Iteration 2841, loss = 0.44485564\n",
      "Iteration 2842, loss = 0.44485290\n",
      "Iteration 2843, loss = 0.44485032\n",
      "Iteration 2844, loss = 0.44484719\n",
      "Iteration 2845, loss = 0.44484370\n",
      "Iteration 2846, loss = 0.44484013\n",
      "Iteration 2847, loss = 0.44483650\n",
      "Iteration 2848, loss = 0.44483393\n",
      "Iteration 2849, loss = 0.44482998\n",
      "Iteration 2850, loss = 0.44482714\n",
      "Iteration 2851, loss = 0.44482350\n",
      "Iteration 2852, loss = 0.44482007\n",
      "Iteration 2853, loss = 0.44481688\n",
      "Iteration 2854, loss = 0.44481212\n",
      "Iteration 2855, loss = 0.44480904\n",
      "Iteration 2856, loss = 0.44480628\n",
      "Iteration 2857, loss = 0.44480144\n",
      "Iteration 2858, loss = 0.44479870\n",
      "Iteration 2859, loss = 0.44479654\n",
      "Iteration 2860, loss = 0.44479239\n",
      "Iteration 2861, loss = 0.44478886\n",
      "Iteration 2862, loss = 0.44478600\n",
      "Iteration 2863, loss = 0.44478250\n",
      "Iteration 2864, loss = 0.44477872\n",
      "Iteration 2865, loss = 0.44477656\n",
      "Iteration 2866, loss = 0.44477290\n",
      "Iteration 2867, loss = 0.44477019\n",
      "Iteration 2868, loss = 0.44476773\n",
      "Iteration 2869, loss = 0.44476511\n",
      "Iteration 2870, loss = 0.44476170\n",
      "Iteration 2871, loss = 0.44475860\n",
      "Iteration 2872, loss = 0.44475608\n",
      "Iteration 2873, loss = 0.44475131\n",
      "Iteration 2874, loss = 0.44474848\n",
      "Iteration 2875, loss = 0.44474528\n",
      "Iteration 2876, loss = 0.44474222\n",
      "Iteration 2877, loss = 0.44474221\n",
      "Iteration 2878, loss = 0.44473646\n",
      "Iteration 2879, loss = 0.44473423\n",
      "Iteration 2880, loss = 0.44473105\n",
      "Iteration 2881, loss = 0.44472775\n",
      "Iteration 2882, loss = 0.44472358\n",
      "Iteration 2883, loss = 0.44472127\n",
      "Iteration 2884, loss = 0.44471566\n",
      "Iteration 2885, loss = 0.44471192\n",
      "Iteration 2886, loss = 0.44470834\n",
      "Iteration 2887, loss = 0.44470521\n",
      "Iteration 2888, loss = 0.44470120\n",
      "Iteration 2889, loss = 0.44469661\n",
      "Iteration 2890, loss = 0.44469407\n",
      "Iteration 2891, loss = 0.44469051\n",
      "Iteration 2892, loss = 0.44468852\n",
      "Iteration 2893, loss = 0.44468500\n",
      "Iteration 2894, loss = 0.44468225\n",
      "Iteration 2895, loss = 0.44467943\n",
      "Iteration 2896, loss = 0.44467614\n",
      "Iteration 2897, loss = 0.44467554\n",
      "Iteration 2898, loss = 0.44466918\n",
      "Iteration 2899, loss = 0.44466651\n",
      "Iteration 2900, loss = 0.44466334\n",
      "Iteration 2901, loss = 0.44466038\n",
      "Iteration 2902, loss = 0.44465518\n",
      "Iteration 2903, loss = 0.44465204\n",
      "Iteration 2904, loss = 0.44465017\n",
      "Iteration 2905, loss = 0.44464766\n",
      "Iteration 2906, loss = 0.44464561\n",
      "Iteration 2907, loss = 0.44464090\n",
      "Iteration 2908, loss = 0.44463717\n",
      "Iteration 2909, loss = 0.44463548\n",
      "Iteration 2910, loss = 0.44463002\n",
      "Iteration 2911, loss = 0.44462721\n",
      "Iteration 2912, loss = 0.44462353\n",
      "Iteration 2913, loss = 0.44461984\n",
      "Iteration 2914, loss = 0.44461873\n",
      "Iteration 2915, loss = 0.44461460\n",
      "Iteration 2916, loss = 0.44461189\n",
      "Iteration 2917, loss = 0.44461028\n",
      "Iteration 2918, loss = 0.44460561\n",
      "Iteration 2919, loss = 0.44460263\n",
      "Iteration 2920, loss = 0.44459972\n",
      "Iteration 2921, loss = 0.44459632\n",
      "Iteration 2922, loss = 0.44459522\n",
      "Iteration 2923, loss = 0.44459056\n",
      "Iteration 2924, loss = 0.44458827\n",
      "Iteration 2925, loss = 0.44458552\n",
      "Iteration 2926, loss = 0.44458257\n",
      "Iteration 2927, loss = 0.44457855\n",
      "Iteration 2928, loss = 0.44457578\n",
      "Iteration 2929, loss = 0.44457173\n",
      "Iteration 2930, loss = 0.44456750\n",
      "Iteration 2931, loss = 0.44456569\n",
      "Iteration 2932, loss = 0.44456206\n",
      "Iteration 2933, loss = 0.44455945\n",
      "Iteration 2934, loss = 0.44455613\n",
      "Iteration 2935, loss = 0.44455272\n",
      "Iteration 2936, loss = 0.44454914\n",
      "Iteration 2937, loss = 0.44454663\n",
      "Iteration 2938, loss = 0.44454222\n",
      "Iteration 2939, loss = 0.44453828\n",
      "Iteration 2940, loss = 0.44453481\n",
      "Iteration 2941, loss = 0.44453134\n",
      "Iteration 2942, loss = 0.44452859\n",
      "Iteration 2943, loss = 0.44452496\n",
      "Iteration 2944, loss = 0.44452173\n",
      "Iteration 2945, loss = 0.44451898\n",
      "Iteration 2946, loss = 0.44451774\n",
      "Iteration 2947, loss = 0.44451482\n",
      "Iteration 2948, loss = 0.44451181\n",
      "Iteration 2949, loss = 0.44450854\n",
      "Iteration 2950, loss = 0.44450585\n",
      "Iteration 2951, loss = 0.44450185\n",
      "Iteration 2952, loss = 0.44449798\n",
      "Iteration 2953, loss = 0.44449506\n",
      "Iteration 2954, loss = 0.44449210\n",
      "Iteration 2955, loss = 0.44448890\n",
      "Iteration 2956, loss = 0.44448637\n",
      "Iteration 2957, loss = 0.44448326\n",
      "Iteration 2958, loss = 0.44448124\n",
      "Iteration 2959, loss = 0.44447867\n",
      "Iteration 2960, loss = 0.44447555\n",
      "Iteration 2961, loss = 0.44447255\n",
      "Iteration 2962, loss = 0.44446913\n",
      "Iteration 2963, loss = 0.44446650\n",
      "Iteration 2964, loss = 0.44446424\n",
      "Iteration 2965, loss = 0.44446226\n",
      "Iteration 2966, loss = 0.44445770\n",
      "Iteration 2967, loss = 0.44445606\n",
      "Iteration 2968, loss = 0.44445119\n",
      "Iteration 2969, loss = 0.44444908\n",
      "Iteration 2970, loss = 0.44444632\n",
      "Iteration 2971, loss = 0.44444642\n",
      "Iteration 2972, loss = 0.44444022\n",
      "Iteration 2973, loss = 0.44443796\n",
      "Iteration 2974, loss = 0.44443502\n",
      "Iteration 2975, loss = 0.44443249\n",
      "Iteration 2976, loss = 0.44442991\n",
      "Iteration 2977, loss = 0.44442765\n",
      "Iteration 2978, loss = 0.44442554\n",
      "Iteration 2979, loss = 0.44442368\n",
      "Iteration 2980, loss = 0.44442042\n",
      "Iteration 2981, loss = 0.44441716\n",
      "Iteration 2982, loss = 0.44441420\n",
      "Iteration 2983, loss = 0.44441129\n",
      "Iteration 2984, loss = 0.44440983\n",
      "Iteration 2985, loss = 0.44440535\n",
      "Iteration 2986, loss = 0.44440261\n",
      "Iteration 2987, loss = 0.44439831\n",
      "Iteration 2988, loss = 0.44439597\n",
      "Iteration 2989, loss = 0.44439342\n",
      "Iteration 2990, loss = 0.44439087\n",
      "Iteration 2991, loss = 0.44438913\n",
      "Iteration 2992, loss = 0.44438581\n",
      "Iteration 2993, loss = 0.44438365\n",
      "Iteration 2994, loss = 0.44437969\n",
      "Iteration 2995, loss = 0.44437650\n",
      "Iteration 2996, loss = 0.44437365\n",
      "Iteration 2997, loss = 0.44436980\n",
      "Iteration 2998, loss = 0.44436771\n",
      "Iteration 2999, loss = 0.44436499\n",
      "Iteration 3000, loss = 0.44436282\n",
      "Iteration 3001, loss = 0.44436160\n",
      "Iteration 3002, loss = 0.44436027\n",
      "Iteration 3003, loss = 0.44435615\n",
      "Iteration 3004, loss = 0.44435420\n",
      "Iteration 3005, loss = 0.44435165\n",
      "Iteration 3006, loss = 0.44434999\n",
      "Iteration 3007, loss = 0.44434872\n",
      "Iteration 3008, loss = 0.44434632\n",
      "Iteration 3009, loss = 0.44434464\n",
      "Iteration 3010, loss = 0.44434383\n",
      "Iteration 3011, loss = 0.44433956\n",
      "Iteration 3012, loss = 0.44433901\n",
      "Iteration 3013, loss = 0.44433521\n",
      "Iteration 3014, loss = 0.44433316\n",
      "Iteration 3015, loss = 0.44433115\n",
      "Iteration 3016, loss = 0.44432906\n",
      "Iteration 3017, loss = 0.44432598\n",
      "Iteration 3018, loss = 0.44432235\n",
      "Iteration 3019, loss = 0.44431977\n",
      "Iteration 3020, loss = 0.44431730\n",
      "Iteration 3021, loss = 0.44431451\n",
      "Iteration 3022, loss = 0.44431196\n",
      "Iteration 3023, loss = 0.44430824\n",
      "Iteration 3024, loss = 0.44430477\n",
      "Iteration 3025, loss = 0.44430088\n",
      "Iteration 3026, loss = 0.44429778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3027, loss = 0.44429568\n",
      "Iteration 3028, loss = 0.44429256\n",
      "Iteration 3029, loss = 0.44428978\n",
      "Iteration 3030, loss = 0.44428722\n",
      "Iteration 3031, loss = 0.44428454\n",
      "Iteration 3032, loss = 0.44428277\n",
      "Iteration 3033, loss = 0.44428058\n",
      "Iteration 3034, loss = 0.44427909\n",
      "Iteration 3035, loss = 0.44427699\n",
      "Iteration 3036, loss = 0.44427615\n",
      "Iteration 3037, loss = 0.44427311\n",
      "Iteration 3038, loss = 0.44427162\n",
      "Iteration 3039, loss = 0.44426994\n",
      "Iteration 3040, loss = 0.44426843\n",
      "Iteration 3041, loss = 0.44426602\n",
      "Iteration 3042, loss = 0.44426489\n",
      "Iteration 3043, loss = 0.44426044\n",
      "Iteration 3044, loss = 0.44425674\n",
      "Iteration 3045, loss = 0.44425315\n",
      "Iteration 3046, loss = 0.44425116\n",
      "Iteration 3047, loss = 0.44424777\n",
      "Iteration 3048, loss = 0.44424503\n",
      "Iteration 3049, loss = 0.44424345\n",
      "Iteration 3050, loss = 0.44424113\n",
      "Iteration 3051, loss = 0.44424039\n",
      "Iteration 3052, loss = 0.44423709\n",
      "Iteration 3053, loss = 0.44423557\n",
      "Iteration 3054, loss = 0.44423357\n",
      "Iteration 3055, loss = 0.44423299\n",
      "Iteration 3056, loss = 0.44422858\n",
      "Iteration 3057, loss = 0.44422778\n",
      "Iteration 3058, loss = 0.44422339\n",
      "Iteration 3059, loss = 0.44422139\n",
      "Iteration 3060, loss = 0.44421948\n",
      "Iteration 3061, loss = 0.44421562\n",
      "Iteration 3062, loss = 0.44421281\n",
      "Iteration 3063, loss = 0.44421041\n",
      "Iteration 3064, loss = 0.44420853\n",
      "Iteration 3065, loss = 0.44420516\n",
      "Iteration 3066, loss = 0.44420359\n",
      "Iteration 3067, loss = 0.44420041\n",
      "Iteration 3068, loss = 0.44419796\n",
      "Iteration 3069, loss = 0.44419572\n",
      "Iteration 3070, loss = 0.44419289\n",
      "Iteration 3071, loss = 0.44418937\n",
      "Iteration 3072, loss = 0.44418618\n",
      "Iteration 3073, loss = 0.44418388\n",
      "Iteration 3074, loss = 0.44418144\n",
      "Iteration 3075, loss = 0.44417826\n",
      "Iteration 3076, loss = 0.44417618\n",
      "Iteration 3077, loss = 0.44417341\n",
      "Iteration 3078, loss = 0.44417113\n",
      "Iteration 3079, loss = 0.44416709\n",
      "Iteration 3080, loss = 0.44416344\n",
      "Iteration 3081, loss = 0.44416086\n",
      "Iteration 3082, loss = 0.44415814\n",
      "Iteration 3083, loss = 0.44415399\n",
      "Iteration 3084, loss = 0.44415116\n",
      "Iteration 3085, loss = 0.44414850\n",
      "Iteration 3086, loss = 0.44414599\n",
      "Iteration 3087, loss = 0.44414372\n",
      "Iteration 3088, loss = 0.44414059\n",
      "Iteration 3089, loss = 0.44413816\n",
      "Iteration 3090, loss = 0.44413635\n",
      "Iteration 3091, loss = 0.44413189\n",
      "Iteration 3092, loss = 0.44412993\n",
      "Iteration 3093, loss = 0.44412743\n",
      "Iteration 3094, loss = 0.44412587\n",
      "Iteration 3095, loss = 0.44412239\n",
      "Iteration 3096, loss = 0.44412013\n",
      "Iteration 3097, loss = 0.44411836\n",
      "Iteration 3098, loss = 0.44411677\n",
      "Iteration 3099, loss = 0.44411349\n",
      "Iteration 3100, loss = 0.44411253\n",
      "Iteration 3101, loss = 0.44410848\n",
      "Iteration 3102, loss = 0.44410537\n",
      "Iteration 3103, loss = 0.44410361\n",
      "Iteration 3104, loss = 0.44410118\n",
      "Iteration 3105, loss = 0.44409833\n",
      "Iteration 3106, loss = 0.44409594\n",
      "Iteration 3107, loss = 0.44409397\n",
      "Iteration 3108, loss = 0.44409222\n",
      "Iteration 3109, loss = 0.44409020\n",
      "Iteration 3110, loss = 0.44408639\n",
      "Iteration 3111, loss = 0.44408421\n",
      "Iteration 3112, loss = 0.44408190\n",
      "Iteration 3113, loss = 0.44407953\n",
      "Iteration 3114, loss = 0.44407785\n",
      "Iteration 3115, loss = 0.44407479\n",
      "Iteration 3116, loss = 0.44407176\n",
      "Iteration 3117, loss = 0.44407017\n",
      "Iteration 3118, loss = 0.44406944\n",
      "Iteration 3119, loss = 0.44406562\n",
      "Iteration 3120, loss = 0.44406198\n",
      "Iteration 3121, loss = 0.44406014\n",
      "Iteration 3122, loss = 0.44405595\n",
      "Iteration 3123, loss = 0.44405193\n",
      "Iteration 3124, loss = 0.44404977\n",
      "Iteration 3125, loss = 0.44404488\n",
      "Iteration 3126, loss = 0.44404339\n",
      "Iteration 3127, loss = 0.44404021\n",
      "Iteration 3128, loss = 0.44403718\n",
      "Iteration 3129, loss = 0.44403430\n",
      "Iteration 3130, loss = 0.44403194\n",
      "Iteration 3131, loss = 0.44403034\n",
      "Iteration 3132, loss = 0.44402929\n",
      "Iteration 3133, loss = 0.44402704\n",
      "Iteration 3134, loss = 0.44402593\n",
      "Iteration 3135, loss = 0.44402490\n",
      "Iteration 3136, loss = 0.44402152\n",
      "Iteration 3137, loss = 0.44401980\n",
      "Iteration 3138, loss = 0.44401776\n",
      "Iteration 3139, loss = 0.44401537\n",
      "Iteration 3140, loss = 0.44401405\n",
      "Iteration 3141, loss = 0.44401073\n",
      "Iteration 3142, loss = 0.44400957\n",
      "Iteration 3143, loss = 0.44400630\n",
      "Iteration 3144, loss = 0.44400352\n",
      "Iteration 3145, loss = 0.44400225\n",
      "Iteration 3146, loss = 0.44399972\n",
      "Iteration 3147, loss = 0.44399491\n",
      "Iteration 3148, loss = 0.44399485\n",
      "Iteration 3149, loss = 0.44398988\n",
      "Iteration 3150, loss = 0.44398720\n",
      "Iteration 3151, loss = 0.44398601\n",
      "Iteration 3152, loss = 0.44398360\n",
      "Iteration 3153, loss = 0.44398175\n",
      "Iteration 3154, loss = 0.44398002\n",
      "Iteration 3155, loss = 0.44397792\n",
      "Iteration 3156, loss = 0.44397630\n",
      "Iteration 3157, loss = 0.44397358\n",
      "Iteration 3158, loss = 0.44397259\n",
      "Iteration 3159, loss = 0.44396906\n",
      "Iteration 3160, loss = 0.44396725\n",
      "Iteration 3161, loss = 0.44396593\n",
      "Iteration 3162, loss = 0.44396413\n",
      "Iteration 3163, loss = 0.44396272\n",
      "Iteration 3164, loss = 0.44396049\n",
      "Iteration 3165, loss = 0.44395815\n",
      "Iteration 3166, loss = 0.44395604\n",
      "Iteration 3167, loss = 0.44395413\n",
      "Iteration 3168, loss = 0.44395121\n",
      "Iteration 3169, loss = 0.44394770\n",
      "Iteration 3170, loss = 0.44394596\n",
      "Iteration 3171, loss = 0.44394269\n",
      "Iteration 3172, loss = 0.44394047\n",
      "Iteration 3173, loss = 0.44393839\n",
      "Iteration 3174, loss = 0.44393744\n",
      "Iteration 3175, loss = 0.44393389\n",
      "Iteration 3176, loss = 0.44393236\n",
      "Iteration 3177, loss = 0.44393151\n",
      "Iteration 3178, loss = 0.44392751\n",
      "Iteration 3179, loss = 0.44392635\n",
      "Iteration 3180, loss = 0.44392438\n",
      "Iteration 3181, loss = 0.44392090\n",
      "Iteration 3182, loss = 0.44391900\n",
      "Iteration 3183, loss = 0.44391527\n",
      "Iteration 3184, loss = 0.44391333\n",
      "Iteration 3185, loss = 0.44390936\n",
      "Iteration 3186, loss = 0.44390707\n",
      "Iteration 3187, loss = 0.44390484\n",
      "Iteration 3188, loss = 0.44390031\n",
      "Iteration 3189, loss = 0.44389786\n",
      "Iteration 3190, loss = 0.44389442\n",
      "Iteration 3191, loss = 0.44389274\n",
      "Iteration 3192, loss = 0.44389027\n",
      "Iteration 3193, loss = 0.44388857\n",
      "Iteration 3194, loss = 0.44388647\n",
      "Iteration 3195, loss = 0.44388450\n",
      "Iteration 3196, loss = 0.44388279\n",
      "Iteration 3197, loss = 0.44387936\n",
      "Iteration 3198, loss = 0.44387671\n",
      "Iteration 3199, loss = 0.44387309\n",
      "Iteration 3200, loss = 0.44387099\n",
      "Iteration 3201, loss = 0.44386816\n",
      "Iteration 3202, loss = 0.44386651\n",
      "Iteration 3203, loss = 0.44386249\n",
      "Iteration 3204, loss = 0.44385856\n",
      "Iteration 3205, loss = 0.44385629\n",
      "Iteration 3206, loss = 0.44385283\n",
      "Iteration 3207, loss = 0.44385276\n",
      "Iteration 3208, loss = 0.44384696\n",
      "Iteration 3209, loss = 0.44384512\n",
      "Iteration 3210, loss = 0.44384332\n",
      "Iteration 3211, loss = 0.44384025\n",
      "Iteration 3212, loss = 0.44383822\n",
      "Iteration 3213, loss = 0.44383769\n",
      "Iteration 3214, loss = 0.44383636\n",
      "Iteration 3215, loss = 0.44383596\n",
      "Iteration 3216, loss = 0.44383478\n",
      "Iteration 3217, loss = 0.44383382\n",
      "Iteration 3218, loss = 0.44383209\n",
      "Iteration 3219, loss = 0.44383125\n",
      "Iteration 3220, loss = 0.44382953\n",
      "Iteration 3221, loss = 0.44382948\n",
      "Iteration 3222, loss = 0.44382874\n",
      "Iteration 3223, loss = 0.44382911\n",
      "Iteration 3224, loss = 0.44382834\n",
      "Iteration 3225, loss = 0.44382705\n",
      "Iteration 3226, loss = 0.44382586\n",
      "Iteration 3227, loss = 0.44382479\n",
      "Iteration 3228, loss = 0.44382347\n",
      "Iteration 3229, loss = 0.44382232\n",
      "Iteration 3230, loss = 0.44382068\n",
      "Iteration 3231, loss = 0.44381941\n",
      "Iteration 3232, loss = 0.44381810\n",
      "Iteration 3233, loss = 0.44381718\n",
      "Iteration 3234, loss = 0.44381425\n",
      "Iteration 3235, loss = 0.44381290\n",
      "Iteration 3236, loss = 0.44381105\n",
      "Iteration 3237, loss = 0.44380940\n",
      "Iteration 3238, loss = 0.44380768\n",
      "Iteration 3239, loss = 0.44380516\n",
      "Iteration 3240, loss = 0.44380267\n",
      "Iteration 3241, loss = 0.44380068\n",
      "Iteration 3242, loss = 0.44379982\n",
      "Iteration 3243, loss = 0.44379780\n",
      "Iteration 3244, loss = 0.44379632\n",
      "Iteration 3245, loss = 0.44379467\n",
      "Iteration 3246, loss = 0.44379315\n",
      "Iteration 3247, loss = 0.44379056\n",
      "Iteration 3248, loss = 0.44378900\n",
      "Iteration 3249, loss = 0.44378689\n",
      "Iteration 3250, loss = 0.44378489\n",
      "Iteration 3251, loss = 0.44378346\n",
      "Iteration 3252, loss = 0.44378123\n",
      "Iteration 3253, loss = 0.44377941\n",
      "Iteration 3254, loss = 0.44377800\n",
      "Iteration 3255, loss = 0.44377675\n",
      "Iteration 3256, loss = 0.44377686\n",
      "Iteration 3257, loss = 0.44377310\n",
      "Iteration 3258, loss = 0.44377152\n",
      "Iteration 3259, loss = 0.44376960\n",
      "Iteration 3260, loss = 0.44376730\n",
      "Iteration 3261, loss = 0.44376567\n",
      "Iteration 3262, loss = 0.44376411\n",
      "Iteration 3263, loss = 0.44376164\n",
      "Iteration 3264, loss = 0.44376088\n",
      "Iteration 3265, loss = 0.44375852\n",
      "Iteration 3266, loss = 0.44375559\n",
      "Iteration 3267, loss = 0.44375346\n",
      "Iteration 3268, loss = 0.44375141\n",
      "Iteration 3269, loss = 0.44375028\n",
      "Iteration 3270, loss = 0.44374915\n",
      "Iteration 3271, loss = 0.44374552\n",
      "Iteration 3272, loss = 0.44374299\n",
      "Iteration 3273, loss = 0.44374140\n",
      "Iteration 3274, loss = 0.44374028\n",
      "Iteration 3275, loss = 0.44373705\n",
      "Iteration 3276, loss = 0.44373580\n",
      "Iteration 3277, loss = 0.44373376\n",
      "Iteration 3278, loss = 0.44373203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3279, loss = 0.44373090\n",
      "Iteration 3280, loss = 0.44372933\n",
      "Iteration 3281, loss = 0.44372743\n",
      "Iteration 3282, loss = 0.44372593\n",
      "Iteration 3283, loss = 0.44372284\n",
      "Iteration 3284, loss = 0.44372167\n",
      "Iteration 3285, loss = 0.44372015\n",
      "Iteration 3286, loss = 0.44371748\n",
      "Iteration 3287, loss = 0.44371609\n",
      "Iteration 3288, loss = 0.44371402\n",
      "Iteration 3289, loss = 0.44371284\n",
      "Iteration 3290, loss = 0.44371035\n",
      "Iteration 3291, loss = 0.44370848\n",
      "Iteration 3292, loss = 0.44370651\n",
      "Iteration 3293, loss = 0.44370553\n",
      "Iteration 3294, loss = 0.44370201\n",
      "Iteration 3295, loss = 0.44370028\n",
      "Iteration 3296, loss = 0.44369872\n",
      "Iteration 3297, loss = 0.44369726\n",
      "Iteration 3298, loss = 0.44369489\n",
      "Iteration 3299, loss = 0.44369319\n",
      "Iteration 3300, loss = 0.44369038\n",
      "Iteration 3301, loss = 0.44368989\n",
      "Iteration 3302, loss = 0.44368774\n",
      "Iteration 3303, loss = 0.44368571\n",
      "Iteration 3304, loss = 0.44368468\n",
      "Iteration 3305, loss = 0.44368285\n",
      "Iteration 3306, loss = 0.44368142\n",
      "Iteration 3307, loss = 0.44368023\n",
      "Iteration 3308, loss = 0.44367801\n",
      "Iteration 3309, loss = 0.44367687\n",
      "Iteration 3310, loss = 0.44367486\n",
      "Iteration 3311, loss = 0.44367329\n",
      "Iteration 3312, loss = 0.44367214\n",
      "Iteration 3313, loss = 0.44366939\n",
      "Iteration 3314, loss = 0.44366915\n",
      "Iteration 3315, loss = 0.44366570\n",
      "Iteration 3316, loss = 0.44366393\n",
      "Iteration 3317, loss = 0.44366388\n",
      "Iteration 3318, loss = 0.44366101\n",
      "Iteration 3319, loss = 0.44365997\n",
      "Iteration 3320, loss = 0.44365916\n",
      "Iteration 3321, loss = 0.44365802\n",
      "Iteration 3322, loss = 0.44365593\n",
      "Iteration 3323, loss = 0.44365466\n",
      "Iteration 3324, loss = 0.44365264\n",
      "Iteration 3325, loss = 0.44365076\n",
      "Iteration 3326, loss = 0.44364941\n",
      "Iteration 3327, loss = 0.44364708\n",
      "Iteration 3328, loss = 0.44364493\n",
      "Iteration 3329, loss = 0.44364331\n",
      "Iteration 3330, loss = 0.44364012\n",
      "Iteration 3331, loss = 0.44363777\n",
      "Iteration 3332, loss = 0.44363768\n",
      "Iteration 3333, loss = 0.44363468\n",
      "Iteration 3334, loss = 0.44363315\n",
      "Iteration 3335, loss = 0.44363192\n",
      "Iteration 3336, loss = 0.44362922\n",
      "Iteration 3337, loss = 0.44362778\n",
      "Iteration 3338, loss = 0.44362625\n",
      "Iteration 3339, loss = 0.44362485\n",
      "Iteration 3340, loss = 0.44362316\n",
      "Iteration 3341, loss = 0.44362171\n",
      "Iteration 3342, loss = 0.44361924\n",
      "Iteration 3343, loss = 0.44361779\n",
      "Iteration 3344, loss = 0.44361552\n",
      "Iteration 3345, loss = 0.44361499\n",
      "Iteration 3346, loss = 0.44361114\n",
      "Iteration 3347, loss = 0.44360985\n",
      "Iteration 3348, loss = 0.44360782\n",
      "Iteration 3349, loss = 0.44360565\n",
      "Iteration 3350, loss = 0.44360283\n",
      "Iteration 3351, loss = 0.44360164\n",
      "Iteration 3352, loss = 0.44359922\n",
      "Iteration 3353, loss = 0.44359642\n",
      "Iteration 3354, loss = 0.44359459\n",
      "Iteration 3355, loss = 0.44359196\n",
      "Iteration 3356, loss = 0.44358954\n",
      "Iteration 3357, loss = 0.44358824\n",
      "Iteration 3358, loss = 0.44358689\n",
      "Iteration 3359, loss = 0.44358409\n",
      "Iteration 3360, loss = 0.44358278\n",
      "Iteration 3361, loss = 0.44358165\n",
      "Iteration 3362, loss = 0.44358000\n",
      "Iteration 3363, loss = 0.44357915\n",
      "Iteration 3364, loss = 0.44357828\n",
      "Iteration 3365, loss = 0.44357579\n",
      "Iteration 3366, loss = 0.44357407\n",
      "Iteration 3367, loss = 0.44357103\n",
      "Iteration 3368, loss = 0.44356949\n",
      "Iteration 3369, loss = 0.44356697\n",
      "Iteration 3370, loss = 0.44356617\n",
      "Iteration 3371, loss = 0.44356220\n",
      "Iteration 3372, loss = 0.44356063\n",
      "Iteration 3373, loss = 0.44355871\n",
      "Iteration 3374, loss = 0.44355772\n",
      "Iteration 3375, loss = 0.44355484\n",
      "Iteration 3376, loss = 0.44355276\n",
      "Iteration 3377, loss = 0.44355189\n",
      "Iteration 3378, loss = 0.44354872\n",
      "Iteration 3379, loss = 0.44354668\n",
      "Iteration 3380, loss = 0.44354620\n",
      "Iteration 3381, loss = 0.44354365\n",
      "Iteration 3382, loss = 0.44354110\n",
      "Iteration 3383, loss = 0.44353934\n",
      "Iteration 3384, loss = 0.44353796\n",
      "Iteration 3385, loss = 0.44353547\n",
      "Iteration 3386, loss = 0.44353410\n",
      "Iteration 3387, loss = 0.44353306\n",
      "Iteration 3388, loss = 0.44353139\n",
      "Iteration 3389, loss = 0.44352934\n",
      "Iteration 3390, loss = 0.44352701\n",
      "Iteration 3391, loss = 0.44352542\n",
      "Iteration 3392, loss = 0.44352216\n",
      "Iteration 3393, loss = 0.44352058\n",
      "Iteration 3394, loss = 0.44351923\n",
      "Iteration 3395, loss = 0.44351697\n",
      "Iteration 3396, loss = 0.44351526\n",
      "Iteration 3397, loss = 0.44351306\n",
      "Iteration 3398, loss = 0.44351150\n",
      "Iteration 3399, loss = 0.44351019\n",
      "Iteration 3400, loss = 0.44350842\n",
      "Iteration 3401, loss = 0.44350727\n",
      "Iteration 3402, loss = 0.44350461\n",
      "Iteration 3403, loss = 0.44350375\n",
      "Iteration 3404, loss = 0.44350219\n",
      "Iteration 3405, loss = 0.44350089\n",
      "Iteration 3406, loss = 0.44349949\n",
      "Iteration 3407, loss = 0.44349879\n",
      "Iteration 3408, loss = 0.44349712\n",
      "Iteration 3409, loss = 0.44349651\n",
      "Iteration 3410, loss = 0.44349461\n",
      "Iteration 3411, loss = 0.44349371\n",
      "Iteration 3412, loss = 0.44349251\n",
      "Iteration 3413, loss = 0.44349042\n",
      "Iteration 3414, loss = 0.44349010\n",
      "Iteration 3415, loss = 0.44348714\n",
      "Iteration 3416, loss = 0.44348554\n",
      "Iteration 3417, loss = 0.44348474\n",
      "Iteration 3418, loss = 0.44348270\n",
      "Iteration 3419, loss = 0.44348151\n",
      "Iteration 3420, loss = 0.44347972\n",
      "Iteration 3421, loss = 0.44347907\n",
      "Iteration 3422, loss = 0.44347645\n",
      "Iteration 3423, loss = 0.44347410\n",
      "Iteration 3424, loss = 0.44347368\n",
      "Iteration 3425, loss = 0.44347028\n",
      "Iteration 3426, loss = 0.44346853\n",
      "Iteration 3427, loss = 0.44346685\n",
      "Iteration 3428, loss = 0.44346593\n",
      "Iteration 3429, loss = 0.44346340\n",
      "Iteration 3430, loss = 0.44346226\n",
      "Iteration 3431, loss = 0.44345861\n",
      "Iteration 3432, loss = 0.44345659\n",
      "Iteration 3433, loss = 0.44345692\n",
      "Iteration 3434, loss = 0.44345369\n",
      "Iteration 3435, loss = 0.44345336\n",
      "Iteration 3436, loss = 0.44345230\n",
      "Iteration 3437, loss = 0.44345129\n",
      "Iteration 3438, loss = 0.44345033\n",
      "Iteration 3439, loss = 0.44344890\n",
      "Iteration 3440, loss = 0.44344821\n",
      "Iteration 3441, loss = 0.44344683\n",
      "Iteration 3442, loss = 0.44344584\n",
      "Iteration 3443, loss = 0.44344493\n",
      "Iteration 3444, loss = 0.44344483\n",
      "Iteration 3445, loss = 0.44344323\n",
      "Iteration 3446, loss = 0.44344259\n",
      "Iteration 3447, loss = 0.44344167\n",
      "Iteration 3448, loss = 0.44344072\n",
      "Iteration 3449, loss = 0.44343887\n",
      "Iteration 3450, loss = 0.44343680\n",
      "Iteration 3451, loss = 0.44343563\n",
      "Iteration 3452, loss = 0.44343475\n",
      "Iteration 3453, loss = 0.44343349\n",
      "Iteration 3454, loss = 0.44343233\n",
      "Iteration 3455, loss = 0.44343098\n",
      "Iteration 3456, loss = 0.44343047\n",
      "Iteration 3457, loss = 0.44342923\n",
      "Iteration 3458, loss = 0.44342822\n",
      "Iteration 3459, loss = 0.44342618\n",
      "Iteration 3460, loss = 0.44342477\n",
      "Iteration 3461, loss = 0.44342334\n",
      "Iteration 3462, loss = 0.44342260\n",
      "Iteration 3463, loss = 0.44342044\n",
      "Iteration 3464, loss = 0.44342115\n",
      "Iteration 3465, loss = 0.44341658\n",
      "Iteration 3466, loss = 0.44341535\n",
      "Iteration 3467, loss = 0.44341341\n",
      "Iteration 3468, loss = 0.44341165\n",
      "Iteration 3469, loss = 0.44341008\n",
      "Iteration 3470, loss = 0.44340788\n",
      "Iteration 3471, loss = 0.44340590\n",
      "Iteration 3472, loss = 0.44340423\n",
      "Iteration 3473, loss = 0.44340334\n",
      "Iteration 3474, loss = 0.44340096\n",
      "Iteration 3475, loss = 0.44339891\n",
      "Iteration 3476, loss = 0.44339817\n",
      "Iteration 3477, loss = 0.44339775\n",
      "Iteration 3478, loss = 0.44339528\n",
      "Iteration 3479, loss = 0.44339349\n",
      "Iteration 3480, loss = 0.44339255\n",
      "Iteration 3481, loss = 0.44339025\n",
      "Iteration 3482, loss = 0.44338907\n",
      "Iteration 3483, loss = 0.44338860\n",
      "Iteration 3484, loss = 0.44338628\n",
      "Iteration 3485, loss = 0.44338538\n",
      "Iteration 3486, loss = 0.44338506\n",
      "Iteration 3487, loss = 0.44338437\n",
      "Iteration 3488, loss = 0.44338350\n",
      "Iteration 3489, loss = 0.44338137\n",
      "Iteration 3490, loss = 0.44337983\n",
      "Iteration 3491, loss = 0.44337874\n",
      "Iteration 3492, loss = 0.44337719\n",
      "Iteration 3493, loss = 0.44337503\n",
      "Iteration 3494, loss = 0.44337346\n",
      "Iteration 3495, loss = 0.44337174\n",
      "Iteration 3496, loss = 0.44337161\n",
      "Iteration 3497, loss = 0.44336878\n",
      "Iteration 3498, loss = 0.44336657\n",
      "Iteration 3499, loss = 0.44336434\n",
      "Iteration 3500, loss = 0.44336193\n",
      "Iteration 3501, loss = 0.44336083\n",
      "Iteration 3502, loss = 0.44335933\n",
      "Iteration 3503, loss = 0.44335880\n",
      "Iteration 3504, loss = 0.44335668\n",
      "Iteration 3505, loss = 0.44335649\n",
      "Iteration 3506, loss = 0.44335501\n",
      "Iteration 3507, loss = 0.44335308\n",
      "Iteration 3508, loss = 0.44335286\n",
      "Iteration 3509, loss = 0.44335107\n",
      "Iteration 3510, loss = 0.44334975\n",
      "Iteration 3511, loss = 0.44334795\n",
      "Iteration 3512, loss = 0.44334690\n",
      "Iteration 3513, loss = 0.44334564\n",
      "Iteration 3514, loss = 0.44334359\n",
      "Iteration 3515, loss = 0.44334293\n",
      "Iteration 3516, loss = 0.44334032\n",
      "Iteration 3517, loss = 0.44333874\n",
      "Iteration 3518, loss = 0.44333726\n",
      "Iteration 3519, loss = 0.44333586\n",
      "Iteration 3520, loss = 0.44333303\n",
      "Iteration 3521, loss = 0.44333138\n",
      "Iteration 3522, loss = 0.44332980\n",
      "Iteration 3523, loss = 0.44332830\n",
      "Iteration 3524, loss = 0.44332589\n",
      "Iteration 3525, loss = 0.44332580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3526, loss = 0.44332411\n",
      "Iteration 3527, loss = 0.44332369\n",
      "Iteration 3528, loss = 0.44332310\n",
      "Iteration 3529, loss = 0.44332249\n",
      "Iteration 3530, loss = 0.44332162\n",
      "Iteration 3531, loss = 0.44331880\n",
      "Iteration 3532, loss = 0.44331779\n",
      "Iteration 3533, loss = 0.44331523\n",
      "Iteration 3534, loss = 0.44331689\n",
      "Iteration 3535, loss = 0.44331339\n",
      "Iteration 3536, loss = 0.44331309\n",
      "Iteration 3537, loss = 0.44331218\n",
      "Iteration 3538, loss = 0.44331130\n",
      "Iteration 3539, loss = 0.44331039\n",
      "Iteration 3540, loss = 0.44330988\n",
      "Iteration 3541, loss = 0.44330926\n",
      "Iteration 3542, loss = 0.44330816\n",
      "Iteration 3543, loss = 0.44330727\n",
      "Iteration 3544, loss = 0.44330626\n",
      "Iteration 3545, loss = 0.44330479\n",
      "Iteration 3546, loss = 0.44330489\n",
      "Iteration 3547, loss = 0.44330226\n",
      "Iteration 3548, loss = 0.44330096\n",
      "Iteration 3549, loss = 0.44329964\n",
      "Iteration 3550, loss = 0.44329842\n",
      "Iteration 3551, loss = 0.44329588\n",
      "Iteration 3552, loss = 0.44329456\n",
      "Iteration 3553, loss = 0.44329309\n",
      "Iteration 3554, loss = 0.44329167\n",
      "Iteration 3555, loss = 0.44329020\n",
      "Iteration 3556, loss = 0.44328787\n",
      "Iteration 3557, loss = 0.44328690\n",
      "Iteration 3558, loss = 0.44328570\n",
      "Iteration 3559, loss = 0.44328481\n",
      "Iteration 3560, loss = 0.44328360\n",
      "Iteration 3561, loss = 0.44328272\n",
      "Iteration 3562, loss = 0.44328167\n",
      "Iteration 3563, loss = 0.44327971\n",
      "Iteration 3564, loss = 0.44327923\n",
      "Iteration 3565, loss = 0.44327801\n",
      "Iteration 3566, loss = 0.44327775\n",
      "Iteration 3567, loss = 0.44327772\n",
      "Iteration 3568, loss = 0.44327680\n",
      "Iteration 3569, loss = 0.44327616\n",
      "Iteration 3570, loss = 0.44327506\n",
      "Iteration 3571, loss = 0.44327518\n",
      "Iteration 3572, loss = 0.44327365\n",
      "Iteration 3573, loss = 0.44327284\n",
      "Iteration 3574, loss = 0.44327187\n",
      "Iteration 3575, loss = 0.44327114\n",
      "Iteration 3576, loss = 0.44326941\n",
      "Iteration 3577, loss = 0.44326763\n",
      "Iteration 3578, loss = 0.44326681\n",
      "Iteration 3579, loss = 0.44326601\n",
      "Iteration 3580, loss = 0.44326437\n",
      "Iteration 3581, loss = 0.44326384\n",
      "Iteration 3582, loss = 0.44326399\n",
      "Iteration 3583, loss = 0.44326288\n",
      "Iteration 3584, loss = 0.44326223\n",
      "Iteration 3585, loss = 0.44326153\n",
      "Iteration 3586, loss = 0.44326036\n",
      "Iteration 3587, loss = 0.44325857\n",
      "Iteration 3588, loss = 0.44325829\n",
      "Iteration 3589, loss = 0.44325601\n",
      "Iteration 3590, loss = 0.44325429\n",
      "Iteration 3591, loss = 0.44325394\n",
      "Iteration 3592, loss = 0.44325168\n",
      "Iteration 3593, loss = 0.44325201\n",
      "Iteration 3594, loss = 0.44325105\n",
      "Iteration 3595, loss = 0.44324995\n",
      "Iteration 3596, loss = 0.44325008\n",
      "Iteration 3597, loss = 0.44324962\n",
      "Iteration 3598, loss = 0.44324921\n",
      "Iteration 3599, loss = 0.44324866\n",
      "Iteration 3600, loss = 0.44324794\n",
      "Iteration 3601, loss = 0.44324780\n",
      "Iteration 3602, loss = 0.44324619\n",
      "Iteration 3603, loss = 0.44324495\n",
      "Iteration 3604, loss = 0.44324463\n",
      "Iteration 3605, loss = 0.44324277\n",
      "Iteration 3606, loss = 0.44324283\n",
      "Iteration 3607, loss = 0.44324173\n",
      "Iteration 3608, loss = 0.44324150\n",
      "Iteration 3609, loss = 0.44324048\n",
      "Iteration 3610, loss = 0.44324030\n",
      "Iteration 3611, loss = 0.44323813\n",
      "Iteration 3612, loss = 0.44323612\n",
      "Iteration 3613, loss = 0.44323416\n",
      "Iteration 3614, loss = 0.44323337\n",
      "Iteration 3615, loss = 0.44323158\n",
      "Iteration 3616, loss = 0.44323020\n",
      "Iteration 3617, loss = 0.44322934\n",
      "Iteration 3618, loss = 0.44322786\n",
      "Iteration 3619, loss = 0.44322623\n",
      "Iteration 3620, loss = 0.44322509\n",
      "Iteration 3621, loss = 0.44322415\n",
      "Iteration 3622, loss = 0.44322193\n",
      "Iteration 3623, loss = 0.44322103\n",
      "Iteration 3624, loss = 0.44321916\n",
      "Iteration 3625, loss = 0.44321681\n",
      "Iteration 3626, loss = 0.44321571\n",
      "Iteration 3627, loss = 0.44321476\n",
      "Iteration 3628, loss = 0.44321339\n",
      "Iteration 3629, loss = 0.44321216\n",
      "Iteration 3630, loss = 0.44321003\n",
      "Iteration 3631, loss = 0.44320895\n",
      "Iteration 3632, loss = 0.44320694\n",
      "Iteration 3633, loss = 0.44320595\n",
      "Iteration 3634, loss = 0.44320470\n",
      "Iteration 3635, loss = 0.44320410\n",
      "Iteration 3636, loss = 0.44320262\n",
      "Iteration 3637, loss = 0.44320082\n",
      "Iteration 3638, loss = 0.44319903\n",
      "Iteration 3639, loss = 0.44319829\n",
      "Iteration 3640, loss = 0.44319689\n",
      "Iteration 3641, loss = 0.44319701\n",
      "Iteration 3642, loss = 0.44319509\n",
      "Iteration 3643, loss = 0.44319208\n",
      "Iteration 3644, loss = 0.44319161\n",
      "Iteration 3645, loss = 0.44319036\n",
      "Iteration 3646, loss = 0.44318909\n",
      "Iteration 3647, loss = 0.44318967\n",
      "Iteration 3648, loss = 0.44318752\n",
      "Iteration 3649, loss = 0.44318642\n",
      "Iteration 3650, loss = 0.44318546\n",
      "Iteration 3651, loss = 0.44318464\n",
      "Iteration 3652, loss = 0.44318349\n",
      "Iteration 3653, loss = 0.44318194\n",
      "Iteration 3654, loss = 0.44318181\n",
      "Iteration 3655, loss = 0.44317962\n",
      "Iteration 3656, loss = 0.44317831\n",
      "Iteration 3657, loss = 0.44317777\n",
      "Iteration 3658, loss = 0.44317554\n",
      "Iteration 3659, loss = 0.44317620\n",
      "Iteration 3660, loss = 0.44317368\n",
      "Iteration 3661, loss = 0.44317305\n",
      "Iteration 3662, loss = 0.44317202\n",
      "Iteration 3663, loss = 0.44317148\n",
      "Iteration 3664, loss = 0.44317015\n",
      "Iteration 3665, loss = 0.44316884\n",
      "Iteration 3666, loss = 0.44316851\n",
      "Iteration 3667, loss = 0.44316689\n",
      "Iteration 3668, loss = 0.44316590\n",
      "Iteration 3669, loss = 0.44316585\n",
      "Iteration 3670, loss = 0.44316324\n",
      "Iteration 3671, loss = 0.44316163\n",
      "Iteration 3672, loss = 0.44316067\n",
      "Iteration 3673, loss = 0.44315898\n",
      "Iteration 3674, loss = 0.44315831\n",
      "Iteration 3675, loss = 0.44315567\n",
      "Iteration 3676, loss = 0.44315503\n",
      "Iteration 3677, loss = 0.44315461\n",
      "Iteration 3678, loss = 0.44315396\n",
      "Iteration 3679, loss = 0.44315341\n",
      "Iteration 3680, loss = 0.44315319\n",
      "Iteration 3681, loss = 0.44315252\n",
      "Iteration 3682, loss = 0.44315158\n",
      "Iteration 3683, loss = 0.44315085\n",
      "Iteration 3684, loss = 0.44314911\n",
      "Iteration 3685, loss = 0.44314917\n",
      "Iteration 3686, loss = 0.44314656\n",
      "Iteration 3687, loss = 0.44314531\n",
      "Iteration 3688, loss = 0.44314415\n",
      "Iteration 3689, loss = 0.44314364\n",
      "Iteration 3690, loss = 0.44314254\n",
      "Iteration 3691, loss = 0.44314156\n",
      "Iteration 3692, loss = 0.44314038\n",
      "Iteration 3693, loss = 0.44313998\n",
      "Iteration 3694, loss = 0.44313890\n",
      "Iteration 3695, loss = 0.44313821\n",
      "Iteration 3696, loss = 0.44313770\n",
      "Iteration 3697, loss = 0.44313623\n",
      "Iteration 3698, loss = 0.44313559\n",
      "Iteration 3699, loss = 0.44313516\n",
      "Iteration 3700, loss = 0.44313615\n",
      "Iteration 3701, loss = 0.44313371\n",
      "Iteration 3702, loss = 0.44313444\n",
      "Iteration 3703, loss = 0.44313241\n",
      "Iteration 3704, loss = 0.44313195\n",
      "Iteration 3705, loss = 0.44313026\n",
      "Iteration 3706, loss = 0.44312902\n",
      "Iteration 3707, loss = 0.44312866\n",
      "Iteration 3708, loss = 0.44312726\n",
      "Iteration 3709, loss = 0.44312614\n",
      "Iteration 3710, loss = 0.44312521\n",
      "Iteration 3711, loss = 0.44312463\n",
      "Iteration 3712, loss = 0.44312287\n",
      "Iteration 3713, loss = 0.44312150\n",
      "Iteration 3714, loss = 0.44312261\n",
      "Iteration 3715, loss = 0.44311995\n",
      "Iteration 3716, loss = 0.44311873\n",
      "Iteration 3717, loss = 0.44311802\n",
      "Iteration 3718, loss = 0.44311664\n",
      "Iteration 3719, loss = 0.44311711\n",
      "Iteration 3720, loss = 0.44311473\n",
      "Iteration 3721, loss = 0.44311198\n",
      "Iteration 3722, loss = 0.44311107\n",
      "Iteration 3723, loss = 0.44311007\n",
      "Iteration 3724, loss = 0.44310757\n",
      "Iteration 3725, loss = 0.44310674\n",
      "Iteration 3726, loss = 0.44310569\n",
      "Iteration 3727, loss = 0.44310368\n",
      "Iteration 3728, loss = 0.44310250\n",
      "Iteration 3729, loss = 0.44310103\n",
      "Iteration 3730, loss = 0.44310011\n",
      "Iteration 3731, loss = 0.44309916\n",
      "Iteration 3732, loss = 0.44309791\n",
      "Iteration 3733, loss = 0.44309727\n",
      "Iteration 3734, loss = 0.44309649\n",
      "Iteration 3735, loss = 0.44309561\n",
      "Iteration 3736, loss = 0.44309448\n",
      "Iteration 3737, loss = 0.44309440\n",
      "Iteration 3738, loss = 0.44309270\n",
      "Iteration 3739, loss = 0.44309258\n",
      "Iteration 3740, loss = 0.44309200\n",
      "Iteration 3741, loss = 0.44309114\n",
      "Iteration 3742, loss = 0.44309012\n",
      "Iteration 3743, loss = 0.44309001\n",
      "Iteration 3744, loss = 0.44308893\n",
      "Iteration 3745, loss = 0.44308775\n",
      "Iteration 3746, loss = 0.44308627\n",
      "Iteration 3747, loss = 0.44308600\n",
      "Iteration 3748, loss = 0.44308472\n",
      "Iteration 3749, loss = 0.44308437\n",
      "Iteration 3750, loss = 0.44308394\n",
      "Iteration 3751, loss = 0.44308201\n",
      "Iteration 3752, loss = 0.44308125\n",
      "Iteration 3753, loss = 0.44307959\n",
      "Iteration 3754, loss = 0.44307902\n",
      "Iteration 3755, loss = 0.44307699\n",
      "Iteration 3756, loss = 0.44307650\n",
      "Iteration 3757, loss = 0.44307501\n",
      "Iteration 3758, loss = 0.44307414\n",
      "Iteration 3759, loss = 0.44307288\n",
      "Iteration 3760, loss = 0.44307199\n",
      "Iteration 3761, loss = 0.44307178\n",
      "Iteration 3762, loss = 0.44306879\n",
      "Iteration 3763, loss = 0.44306831\n",
      "Iteration 3764, loss = 0.44306729\n",
      "Iteration 3765, loss = 0.44306606\n",
      "Iteration 3766, loss = 0.44306516\n",
      "Iteration 3767, loss = 0.44306433\n",
      "Iteration 3768, loss = 0.44306447\n",
      "Iteration 3769, loss = 0.44306220\n",
      "Iteration 3770, loss = 0.44306148\n",
      "Iteration 3771, loss = 0.44306027\n",
      "Iteration 3772, loss = 0.44305871\n",
      "Iteration 3773, loss = 0.44305814\n",
      "Iteration 3774, loss = 0.44305746\n",
      "Iteration 3775, loss = 0.44305657\n",
      "Iteration 3776, loss = 0.44305564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3777, loss = 0.44305460\n",
      "Iteration 3778, loss = 0.44305438\n",
      "Iteration 3779, loss = 0.44305360\n",
      "Iteration 3780, loss = 0.44305303\n",
      "Iteration 3781, loss = 0.44305228\n",
      "Iteration 3782, loss = 0.44305115\n",
      "Iteration 3783, loss = 0.44305037\n",
      "Iteration 3784, loss = 0.44304942\n",
      "Iteration 3785, loss = 0.44304871\n",
      "Iteration 3786, loss = 0.44304804\n",
      "Iteration 3787, loss = 0.44304743\n",
      "Iteration 3788, loss = 0.44304756\n",
      "Iteration 3789, loss = 0.44304732\n",
      "Iteration 3790, loss = 0.44304584\n",
      "Iteration 3791, loss = 0.44304503\n",
      "Iteration 3792, loss = 0.44304363\n",
      "Iteration 3793, loss = 0.44304299\n",
      "Iteration 3794, loss = 0.44304233\n",
      "Iteration 3795, loss = 0.44304120\n",
      "Iteration 3796, loss = 0.44304078\n",
      "Iteration 3797, loss = 0.44303908\n",
      "Iteration 3798, loss = 0.44303782\n",
      "Iteration 3799, loss = 0.44303688\n",
      "Iteration 3800, loss = 0.44303636\n",
      "Iteration 3801, loss = 0.44303563\n",
      "Iteration 3802, loss = 0.44303497\n",
      "Iteration 3803, loss = 0.44303413\n",
      "Iteration 3804, loss = 0.44303430\n",
      "Iteration 3805, loss = 0.44303222\n",
      "Iteration 3806, loss = 0.44303109\n",
      "Iteration 3807, loss = 0.44303020\n",
      "Iteration 3808, loss = 0.44303011\n",
      "Iteration 3809, loss = 0.44302779\n",
      "Iteration 3810, loss = 0.44302746\n",
      "Iteration 3811, loss = 0.44302738\n",
      "Iteration 3812, loss = 0.44302599\n",
      "Iteration 3813, loss = 0.44302591\n",
      "Iteration 3814, loss = 0.44302570\n",
      "Iteration 3815, loss = 0.44302494\n",
      "Iteration 3816, loss = 0.44302383\n",
      "Iteration 3817, loss = 0.44302319\n",
      "Iteration 3818, loss = 0.44302224\n",
      "Iteration 3819, loss = 0.44302102\n",
      "Iteration 3820, loss = 0.44301987\n",
      "Iteration 3821, loss = 0.44301840\n",
      "Iteration 3822, loss = 0.44301737\n",
      "Iteration 3823, loss = 0.44301651\n",
      "Iteration 3824, loss = 0.44301646\n",
      "Iteration 3825, loss = 0.44301451\n",
      "Iteration 3826, loss = 0.44301356\n",
      "Iteration 3827, loss = 0.44301313\n",
      "Iteration 3828, loss = 0.44301211\n",
      "Iteration 3829, loss = 0.44301119\n",
      "Iteration 3830, loss = 0.44301042\n",
      "Iteration 3831, loss = 0.44300950\n",
      "Iteration 3832, loss = 0.44300907\n",
      "Iteration 3833, loss = 0.44300742\n",
      "Iteration 3834, loss = 0.44300652\n",
      "Iteration 3835, loss = 0.44300605\n",
      "Iteration 3836, loss = 0.44300519\n",
      "Iteration 3837, loss = 0.44300468\n",
      "Iteration 3838, loss = 0.44300364\n",
      "Iteration 3839, loss = 0.44300318\n",
      "Iteration 3840, loss = 0.44300207\n",
      "Iteration 3841, loss = 0.44300210\n",
      "Iteration 3842, loss = 0.44300082\n",
      "Iteration 3843, loss = 0.44300029\n",
      "Iteration 3844, loss = 0.44299978\n",
      "Iteration 3845, loss = 0.44299949\n",
      "Iteration 3846, loss = 0.44299930\n",
      "Iteration 3847, loss = 0.44299827\n",
      "Iteration 3848, loss = 0.44299885\n",
      "Iteration 3849, loss = 0.44299706\n",
      "Iteration 3850, loss = 0.44299650\n",
      "Iteration 3851, loss = 0.44299589\n",
      "Iteration 3852, loss = 0.44299468\n",
      "Iteration 3853, loss = 0.44299480\n",
      "Iteration 3854, loss = 0.44299365\n",
      "Iteration 3855, loss = 0.44299311\n",
      "Iteration 3856, loss = 0.44299245\n",
      "Iteration 3857, loss = 0.44299174\n",
      "Iteration 3858, loss = 0.44299066\n",
      "Iteration 3859, loss = 0.44298948\n",
      "Iteration 3860, loss = 0.44298912\n",
      "Iteration 3861, loss = 0.44298848\n",
      "Iteration 3862, loss = 0.44298752\n",
      "Iteration 3863, loss = 0.44298656\n",
      "Iteration 3864, loss = 0.44298556\n",
      "Iteration 3865, loss = 0.44298527\n",
      "Iteration 3866, loss = 0.44298479\n",
      "Iteration 3867, loss = 0.44298390\n",
      "Iteration 3868, loss = 0.44298326\n",
      "Iteration 3869, loss = 0.44298257\n",
      "Iteration 3870, loss = 0.44298104\n",
      "Iteration 3871, loss = 0.44298049\n",
      "Iteration 3872, loss = 0.44297998\n",
      "Iteration 3873, loss = 0.44297923\n",
      "Iteration 3874, loss = 0.44297843\n",
      "Iteration 3875, loss = 0.44297816\n",
      "Iteration 3876, loss = 0.44297782\n",
      "Iteration 3877, loss = 0.44297737\n",
      "Iteration 3878, loss = 0.44297639\n",
      "Iteration 3879, loss = 0.44297642\n",
      "Iteration 3880, loss = 0.44297517\n",
      "Iteration 3881, loss = 0.44297312\n",
      "Iteration 3882, loss = 0.44297310\n",
      "Iteration 3883, loss = 0.44297073\n",
      "Iteration 3884, loss = 0.44297012\n",
      "Iteration 3885, loss = 0.44296906\n",
      "Iteration 3886, loss = 0.44296813\n",
      "Iteration 3887, loss = 0.44296833\n",
      "Iteration 3888, loss = 0.44296663\n",
      "Iteration 3889, loss = 0.44296888\n",
      "Iteration 3890, loss = 0.44296539\n",
      "Iteration 3891, loss = 0.44296559\n",
      "Iteration 3892, loss = 0.44296520\n",
      "Iteration 3893, loss = 0.44296479\n",
      "Iteration 3894, loss = 0.44296400\n",
      "Iteration 3895, loss = 0.44296352\n",
      "Iteration 3896, loss = 0.44296225\n",
      "Iteration 3897, loss = 0.44296119\n",
      "Iteration 3898, loss = 0.44296224\n",
      "Iteration 3899, loss = 0.44295851\n",
      "Iteration 3900, loss = 0.44295779\n",
      "Iteration 3901, loss = 0.44295741\n",
      "Iteration 3902, loss = 0.44295570\n",
      "Iteration 3903, loss = 0.44295432\n",
      "Iteration 3904, loss = 0.44295454\n",
      "Iteration 3905, loss = 0.44295313\n",
      "Iteration 3906, loss = 0.44295287\n",
      "Iteration 3907, loss = 0.44295227\n",
      "Iteration 3908, loss = 0.44295222\n",
      "Iteration 3909, loss = 0.44295105\n",
      "Iteration 3910, loss = 0.44295011\n",
      "Iteration 3911, loss = 0.44294991\n",
      "Iteration 3912, loss = 0.44294859\n",
      "Iteration 3913, loss = 0.44294819\n",
      "Iteration 3914, loss = 0.44294686\n",
      "Iteration 3915, loss = 0.44294730\n",
      "Iteration 3916, loss = 0.44294544\n",
      "Iteration 3917, loss = 0.44294481\n",
      "Iteration 3918, loss = 0.44294430\n",
      "Iteration 3919, loss = 0.44294416\n",
      "Iteration 3920, loss = 0.44294380\n",
      "Iteration 3921, loss = 0.44294364\n",
      "Iteration 3922, loss = 0.44294288\n",
      "Iteration 3923, loss = 0.44294220\n",
      "Iteration 3924, loss = 0.44294170\n",
      "Iteration 3925, loss = 0.44294331\n",
      "Iteration 3926, loss = 0.44294052\n",
      "Iteration 3927, loss = 0.44294069\n",
      "Iteration 3928, loss = 0.44294064\n",
      "Iteration 3929, loss = 0.44293964\n",
      "Iteration 3930, loss = 0.44293975\n",
      "Iteration 3931, loss = 0.44293792\n",
      "Iteration 3932, loss = 0.44293669\n",
      "Iteration 3933, loss = 0.44293578\n",
      "Iteration 3934, loss = 0.44293548\n",
      "Iteration 3935, loss = 0.44293378\n",
      "Iteration 3936, loss = 0.44293451\n",
      "Iteration 3937, loss = 0.44293252\n",
      "Iteration 3938, loss = 0.44293137\n",
      "Iteration 3939, loss = 0.44293039\n",
      "Iteration 3940, loss = 0.44292954\n",
      "Iteration 3941, loss = 0.44292884\n",
      "Iteration 3942, loss = 0.44292822\n",
      "Iteration 3943, loss = 0.44292914\n",
      "Iteration 3944, loss = 0.44292650\n",
      "Iteration 3945, loss = 0.44292639\n",
      "Iteration 3946, loss = 0.44292569\n",
      "Iteration 3947, loss = 0.44292526\n",
      "Iteration 3948, loss = 0.44292487\n",
      "Iteration 3949, loss = 0.44292450\n",
      "Iteration 3950, loss = 0.44292404\n",
      "Iteration 3951, loss = 0.44292370\n",
      "Iteration 3952, loss = 0.44292374\n",
      "Iteration 3953, loss = 0.44292274\n",
      "Iteration 3954, loss = 0.44292210\n",
      "Iteration 3955, loss = 0.44292145\n",
      "Iteration 3956, loss = 0.44292195\n",
      "Iteration 3957, loss = 0.44291991\n",
      "Iteration 3958, loss = 0.44291949\n",
      "Iteration 3959, loss = 0.44291893\n",
      "Iteration 3960, loss = 0.44291786\n",
      "Iteration 3961, loss = 0.44291745\n",
      "Iteration 3962, loss = 0.44291735\n",
      "Iteration 3963, loss = 0.44291627\n",
      "Iteration 3964, loss = 0.44291507\n",
      "Iteration 3965, loss = 0.44291464\n",
      "Iteration 3966, loss = 0.44291366\n",
      "Iteration 3967, loss = 0.44291326\n",
      "Iteration 3968, loss = 0.44291248\n",
      "Iteration 3969, loss = 0.44291242\n",
      "Iteration 3970, loss = 0.44291178\n",
      "Iteration 3971, loss = 0.44291133\n",
      "Iteration 3972, loss = 0.44291106\n",
      "Iteration 3973, loss = 0.44291015\n",
      "Iteration 3974, loss = 0.44290978\n",
      "Iteration 3975, loss = 0.44290933\n",
      "Iteration 3976, loss = 0.44290872\n",
      "Iteration 3977, loss = 0.44290825\n",
      "Iteration 3978, loss = 0.44290828\n",
      "Iteration 3979, loss = 0.44290687\n",
      "Iteration 3980, loss = 0.44290660\n",
      "Iteration 3981, loss = 0.44290662\n",
      "Iteration 3982, loss = 0.44290635\n",
      "Iteration 3983, loss = 0.44290605\n",
      "Iteration 3984, loss = 0.44290562\n",
      "Iteration 3985, loss = 0.44290441\n",
      "Iteration 3986, loss = 0.44290397\n",
      "Iteration 3987, loss = 0.44290278\n",
      "Iteration 3988, loss = 0.44290397\n",
      "Iteration 3989, loss = 0.44290205\n",
      "Iteration 3990, loss = 0.44290228\n",
      "Iteration 3991, loss = 0.44290265\n",
      "Iteration 3992, loss = 0.44290265\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=50, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(600, 100, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=5000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=21, shuffle=True,\n",
       "       solver='sgd', tol=1e-09, validation_fraction=0.1, verbose=10,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363636363636363"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46,  0],\n",
       "       [ 9,  0]])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "clf1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363636363636363"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46,  0],\n",
       "       [ 9,  0]])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
